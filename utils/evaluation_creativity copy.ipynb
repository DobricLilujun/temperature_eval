{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation OpenAI as an annotator to replace the expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujun/anaconda3/envs/causalLLM/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n",
      "2024-06-07 21:12:22.080349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/lujun/anaconda3/envs/causalLLM/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from nltk.translate import meteor_score\n",
    "from transformers import pipeline, BartTokenizer, BartForConditionalGeneration\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "file_name = \"exp_result_Mistral-7B-Instruct-v0.2_20240531193523_952844.csv\"\n",
    "foler_path = \"Paper Experiment Results/New_filtered/creativity\"\n",
    "\n",
    "output_file_name = file_name.replace(\".csv\", \"_evaluated.csv\")\n",
    "input_path = os.path.join(foler_path, file_name)\n",
    "output_path = os.path.join(foler_path, output_file_name)\n",
    "_data = pd.read_csv(input_path)\n",
    "eval_model = \"gpt-3.5-turbo-0125\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"Paper Experiment Results/New_filtered/creativity/creativity_eval-main/creativity_eval-main/tests/ttcw_all_tests.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    tests = json.load(f)\n",
    "\n",
    "\n",
    "# Based on the create_eval code\n",
    "def full_prompt2context(full_prompt):\n",
    "    lines = full_prompt.strip().split(\"\\n\")\n",
    "    kept1 = \"\\n\".join(lines[:-1]).strip().split(\"\\n\")\n",
    "    kept2 = kept1[:-1]\n",
    "    return \"\\n\".join(kept2).strip()\n",
    "\n",
    "\n",
    "for test in tests:\n",
    "    test[\"expanded_context\"] = full_prompt2context(test[\"full_prompt\"])\n",
    "\n",
    "\n",
    "def match_prompt_column(model):\n",
    "    if \"Llama-2\" in model:\n",
    "        return \"llama2_chat_initial_prompt\"\n",
    "    elif \"Mixtral\" in model or \"Mistral\" in model:\n",
    "        return \"mixtral_instruct_initial_prompt\"\n",
    "    elif \"Llama-3\" in model:\n",
    "        return \"llama3_chat_initial_prompt\"\n",
    "    else:\n",
    "        print(\"The model name didn't match anything, please check!!!!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_pure_response(row):\n",
    "    model = row[\"model\"]\n",
    "    prompt_column = match_prompt_column(model)\n",
    "    response = row[\"generated_response\"]\n",
    "    prompt = row[prompt_column]\n",
    "    return extract_final_response(response=response, prompt=prompt)\n",
    "\n",
    "\n",
    "def extract_final_response(response, prompt):\n",
    "    return response[len(prompt) :]\n",
    "\n",
    "\n",
    "evaluation_df = pd.DataFrame(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prompt = \"\"\"You are given a creative short-story. Read it carefully. You are then given some background about specific aspects of creative writing, as well as a binary (Yes/No) question. Your objective is to use the background information to answer the question about the story. Start your answer with Yes or No. You can optionally then provide a short explanation for your answer.\n",
    "\n",
    "==========\n",
    "Story:\n",
    "{story}\n",
    "==========\n",
    "Background:\n",
    "{background}\n",
    "\n",
    "==========\n",
    "Question: {question}\n",
    "\n",
    "Remember to start your answer with Yes or No. You can optionally then provide a short explanation for your answer.\n",
    "\"\"\"\n",
    "\n",
    "evaluate_prompt_template = PromptTemplate.from_template(evaluate_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected new version results\n",
      "cartesian_product rows: 1176\n",
      "cartesian_product columns: Index(['ttcw_idx', 'torrance_dimension', 'category', 'question', 'full_prompt',\n",
      "       'expanded_context', 'Unnamed: 0', 'Temperature', 'story_idx',\n",
      "       'story_name', 'plot', 'content', 'word_count', 'initial_prompt',\n",
      "       'llama2_chat_initial_prompt', 'mixtral_instruct_initial_prompt',\n",
      "       'llama3_chat_initial_prompt', 'llama2_chat_initial_prompt_recurrent',\n",
      "       'mixtral_instruct_initial_prompt_recurrent',\n",
      "       'llama3_chat_initial_prompt_recurrent', 'generated_response',\n",
      "       'story_generated', 'timestamp', 'elapsed_time', 'temperature', 'model',\n",
      "       'repetition_times', 'pure_response'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stories_generated_df = pd.read_csv(input_path)\n",
    "\n",
    "if \"story_generated\" in stories_generated_df.columns:\n",
    "    print(\"Detected new version results\")\n",
    "    stories_generated_df[\"pure_response\"] = stories_generated_df[\"story_generated\"]\n",
    "else:\n",
    "    print(\"Detected old version results\")\n",
    "    stories_generated_df[\"pure_response\"] = stories_generated_df[\"generated_response\"]\n",
    "\n",
    "cartesian_product_df = pd.merge(\n",
    "    evaluation_df.assign(key=0),\n",
    "    stories_generated_df.assign(key=0),\n",
    "    how=\"outer\",\n",
    "    on=\"key\",\n",
    ").drop(\"key\", axis=1)\n",
    "\n",
    "print(\"cartesian_product rows:\", cartesian_product_df.shape[0])\n",
    "print(\"cartesian_product columns:\", cartesian_product_df.columns)\n",
    "\n",
    "cartesian_product_df[\"formatted_prompt\"] = cartesian_product_df.apply(\n",
    "    lambda row: evaluate_prompt_template.format(\n",
    "        story=row[\"pure_response\"],\n",
    "        background=row[\"expanded_context\"],\n",
    "        question=row[\"question\"],\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1176/1176 [00:00<00:00, 69070.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    data_evaluated = pd.read_csv(output_path)\n",
    "    last_idx = data_evaluated.shape[0]\n",
    "else:\n",
    "    last_idx = 0\n",
    "\n",
    "generated_responses = []\n",
    "client = OpenAI(api_key=api_key)\n",
    "with tqdm(total=len(cartesian_product_df)) as pbar:\n",
    "    for index, row in cartesian_product_df.iterrows():\n",
    "        if index < last_idx:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        response = (\n",
    "            client.chat.completions.create(\n",
    "                model=\"gpt-4-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": row[\"formatted_prompt\"]},\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message\n",
    "        ).content\n",
    "        generated_responses.append(response)\n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"OpenAI_response\"] = response\n",
    "        updated_dataframe = pd.DataFrame([updated_row])\n",
    "        pbar.update(1)\n",
    "        if not os.path.exists(output_path):\n",
    "            updated_dataframe.to_csv(output_path, index=False, mode=\"w\", header=True)\n",
    "        else:\n",
    "            updated_dataframe.to_csv(output_path, index=False, mode=\"a\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_function(x):\n",
    "    if str(x).startswith(\"Yes\"):\n",
    "        return \"yes\"\n",
    "    elif str(x).startswith(\"No\"):\n",
    "        return \"no\"\n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "\n",
    "data_evaluated[\"label\"] = data_evaluated[\"OpenAI_response\"].apply(label_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
