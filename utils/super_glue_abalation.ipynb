{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "tasks = [\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\"]\n",
    "\n",
    "\n",
    "# Preprocessing function: Format inputs based on task\n",
    "def preprocess_example(example, task):\n",
    "    # Format input text based on task\n",
    "    if task == \"boolq\":  # BoolQ: Yes/No question answering\n",
    "        context = example[\"passage\"]\n",
    "        question = example[\"question\"]\n",
    "        input_text = (\n",
    "            f\"Question: {question} Context: {context} Is this correct? (yes/no)\"\n",
    "        )\n",
    "\n",
    "    elif task == \"cb\":  # CommitmentBank: Textual entailment\n",
    "        premise = example[\"premise\"]\n",
    "        hypothesis = example[\"hypothesis\"]\n",
    "        input_text = f\"Premise: {premise} Hypothesis: {hypothesis} Does this follow? (yes/no/unknown)\"\n",
    "\n",
    "    elif task == \"copa\":  # COPA: Causal reasoning\n",
    "        premise = example[\"premise\"]\n",
    "        choice1 = example[\"choice1\"]\n",
    "        choice2 = example[\"choice2\"]\n",
    "        input_text = f\"Premise: {premise} Question: {example['question']} Option 1: {choice1} Option 2: {choice2} Which is more plausible?\"\n",
    "\n",
    "    elif task == \"multirc\":  # MultiRC: Multi-choice QA\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"paragraph\"]\n",
    "        answer = example[\"answer\"]\n",
    "        input_text = f'Context: {context} Question: {question} Is the answer \"{answer}\" correct? (yes/no)'\n",
    "\n",
    "    elif task == \"record\":  # ReCoRD: Cloze-style QA\n",
    "        context = example[\"passage\"]\n",
    "        query = example[\"query\"]\n",
    "        input_text = (\n",
    "            f\"Context: {context} Question: {query} What should fill in the blank?\"\n",
    "        )\n",
    "\n",
    "    elif task == \"rte\":  # Recognizing Textual Entailment\n",
    "        premise = example[\"premise\"]\n",
    "        hypothesis = example[\"hypothesis\"]\n",
    "        input_text = (\n",
    "            f\"Premise: {premise} Hypothesis: {hypothesis} Is this true? (yes/no)\"\n",
    "        )\n",
    "\n",
    "    elif task == \"wic\":  # WiC: Word-in-Context disambiguation\n",
    "        word = example[\"word\"]\n",
    "        sentence1 = example[\"sentence1\"]\n",
    "        sentence2 = example[\"sentence2\"]\n",
    "        input_text = f\"Word: {word} Sentence 1: {sentence1} Sentence 2: {sentence2} Does the word have the same meaning in both sentences? (yes/no)\"\n",
    "\n",
    "    elif task == \"wsc\":  # Winograd Schema Challenge\n",
    "        context = example[\"text\"]\n",
    "        input_text = (\n",
    "            f\"Sentence: {context} Does the pronoun refer to the correct entity?\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {task}\")\n",
    "\n",
    "    return (input_text,)\n",
    "\n",
    "\n",
    "def evaluate_task(task):\n",
    "    print(f\"Loading task data: {task}\")\n",
    "    dataset = load_dataset(\"super_glue\", task)\n",
    "    metric = (\n",
    "        load(\"accuracy\") if task != \"multirc\" else load(\"f1\")\n",
    "    )  # Use accuracy or F1 score\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset[\"validation\"]:  # Use validation set\n",
    "        # Preprocess data\n",
    "        inputs = preprocess_example(example, task, tokenizer)\n",
    "\n",
    "        # Generate predictions\n",
    "        prediction = predict(inputs, model, tokenizer, task)\n",
    "\n",
    "        # Store predictions and references\n",
    "        if task in [\"boolq\", \"cb\", \"rte\", \"wic\", \"wsc\"]:\n",
    "            predictions.append(prediction)\n",
    "            references.append(str(example[\"label\"]))  # Convert labels to strings\n",
    "\n",
    "        elif task == \"copa\":\n",
    "            predictions.append(prediction)\n",
    "            references.append(example[\"label\"])  # COPA labels are 0 or 1\n",
    "\n",
    "        elif task == \"record\":\n",
    "            predictions.append(prediction)\n",
    "            references.append(example[\"answers\"])  # ReCoRD labels are answer lists\n",
    "\n",
    "        elif task == \"multirc\":\n",
    "            predictions.append(prediction)\n",
    "            references.append(str(example[\"label\"]))\n",
    "\n",
    "    # Compute evaluation scores\n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"Results for task {task}: {results}\")\n",
    "\n",
    "\n",
    "# Main function: Evaluate all tasks\n",
    "if __name__ == \"__main__\":\n",
    "    for task in tasks:\n",
    "        try:\n",
    "            evaluate_task(task, model, tokenizer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating task {task}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.8M/51.8M [00:00<00:00, 92.4MB/s]\n",
      "Generating train split: 100%|██████████| 100730/100730 [00:16<00:00, 5940.57 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:01<00:00, 5735.06 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:01<00:00, 6466.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "tasks = [\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\"]\n",
    "\n",
    "task = \"record\"\n",
    "dataset = load_dataset(\"super_glue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passage': \"The harrowing stories of women and children locked up for so-called 'moral crimes' in Afghanistan's notorious female prison have been revealed after cameras were allowed inside. Mariam has been in Badam Bagh prison for three months after she shot a man who just raped her at gunpoint and then turned the weapon on herself - but she has yet to been charged. Nuria has eight months left to serve of her sentence for trying to divorce her husband. She gave birth in prison to her son and they share a cell together. Scroll down for video Nuria was jailed for trying to divorce her husband. Her son is one of 62 children living at Badam Bagh prison\\n@highlight\\nMost of the 202 Badam Bagh inmates are jailed for so-called 'moral crimes'\\n@highlight\\nCrimes include leaving their husbands or refusing an arrange marriage\\n@highlight\\n62 children live there and share cells with their mothers and five others\",\n",
       " 'query': 'The baby she gave birth to is her husbands and he has even offered to have the courts set her free if she returns, but @placeholder has refused.',\n",
       " 'entities': ['Badam Bagh', 'Nuria', 'Mariam', 'Afghanistan'],\n",
       " 'entity_spans': {'text': ['Afghanistan',\n",
       "   'Mariam',\n",
       "   'Badam Bagh',\n",
       "   'Nuria',\n",
       "   'Nuria',\n",
       "   'Badam Bagh',\n",
       "   'Badam Bagh'],\n",
       "  'start': [86, 178, 197, 357, 535, 627, 672],\n",
       "  'end': [97, 184, 207, 362, 540, 637, 682]},\n",
       " 'answers': ['Nuria'],\n",
       " 'idx': {'passage': 0, 'query': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
