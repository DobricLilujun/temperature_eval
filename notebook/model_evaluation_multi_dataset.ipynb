{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Data Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'api_key.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# df = pd.read_json(file_path, lines=True)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m EVALUATOR(server_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi_key.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     21\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_response_CR\u001b[39m(generation):\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'api_key.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "from utils.evaluators import EVALUATOR\n",
    "from langchain import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from bert_score import BERTScorer\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "LIST_DATASET_CSV = [\"CR\", \"CT\", \"ICL\", \"IF\", \"MT\", \"SUMM\"]  # Dataset list\n",
    "file_path = \"your_file.jsonl\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "evaluator = EVALUATOR(server_url=None, model_name=None)\n",
    "with open(\"/home/lujun_li/projects/temperature_eval/.vscode/api_key.txt\", \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "\n",
    "def evaluate_response_CR(generation):\n",
    "    generation_lower = generation.lower()\n",
    "    if generation_lower.startswith(\"yes\"):\n",
    "        return True, generation\n",
    "    elif generation_lower.startswith(\"no\"):\n",
    "        return False, generation\n",
    "    else:\n",
    "        print(f\"Unexpected response: {generation}\")\n",
    "        return None, generation\n",
    "\n",
    "\n",
    "def evaluate_response_CT(generation):\n",
    "    if str(generation).lower().startswith(\"Yes\"):\n",
    "        return True, generation\n",
    "    elif str(generation).lower().startswith(\"No\"):\n",
    "        return False, generation\n",
    "    else:\n",
    "        return None, generation\n",
    "\n",
    "\n",
    "def evaluate_response_IF(generation):\n",
    "    # check if generation is yes or no\n",
    "    if generation.lower().startswith(\"yes\") or generation.lower().startswith(\"no\"):\n",
    "        if generation.lower().startswith(\"yes\"):\n",
    "            return True, generation\n",
    "        else:\n",
    "            return False, generation\n",
    "    else:\n",
    "        if \"YES\" in generation and \"NO\" not in generation:\n",
    "            return True, generation\n",
    "        elif \"YES\" not in generation and \"NO\" in generation:\n",
    "            return False, generation\n",
    "        else:\n",
    "            # print(\"NO YES or NO answer!\" + generation)\n",
    "            return None, generation\n",
    "\n",
    "\n",
    "evaluate_prompt_CT = \"\"\"You are given a creative short-story. Read it carefully. You are then given some background about specific aspects of creative writing, as well as a binary (Yes/No) question. Your objective is to use the background information to answer the question about the story. Start your answer with \"Yes\" or \"No\". You can optionally then provide a short explanation for your answer.\n",
    "\n",
    "==========\n",
    "Story:\n",
    "{story}\n",
    "==========\n",
    "Background:\n",
    "{background}\n",
    "\n",
    "==========\n",
    "Question: {question}\n",
    "\n",
    "Remember to start your answer with Yes or No. You can optionally then provide a short explanation for your answer.\n",
    "\"\"\"\n",
    "\n",
    "with open(\n",
    "    \"/home/lujun_li/projects/temperature_eval/data/ttcw_all_tests.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    tests = json.load(f)\n",
    "\n",
    "\n",
    "def full_prompt2context(full_prompt):\n",
    "    lines = full_prompt.strip().split(\"\\n\")\n",
    "    kept1 = \"\\n\".join(lines[:-1]).strip().split(\"\\n\")\n",
    "    kept2 = kept1[:-1]\n",
    "    return \"\\n\".join(kept2).strip()\n",
    "\n",
    "\n",
    "for test in tests:\n",
    "    test[\"expanded_context\"] = full_prompt2context(test[\"full_prompt\"])\n",
    "\n",
    "evaluate_prompt_template_CT = PromptTemplate.from_template(evaluate_prompt_CT)\n",
    "\n",
    "\n",
    "def classification_score(prediction, ground_truth, all_classes):\n",
    "    em_match_list = []\n",
    "    for class_name in all_classes:\n",
    "        if class_name in prediction:\n",
    "            em_match_list.append(class_name)\n",
    "    for match_term in em_match_list:\n",
    "        if match_term in ground_truth and match_term != ground_truth:\n",
    "            em_match_list.remove(match_term)\n",
    "    if ground_truth in em_match_list:\n",
    "        score = 1.0 / len(em_match_list)\n",
    "    else:\n",
    "        score = 0.0\n",
    "    return score\n",
    "\n",
    "\n",
    "categories = [\n",
    "    \"Food\",\n",
    "    \"Date\",\n",
    "    \"Order, rank\",\n",
    "    \"Speed\",\n",
    "    \"Disease and medicine\",\n",
    "    \"Word with a special property\",\n",
    "    \"Abbreviation\",\n",
    "    \"Language\",\n",
    "    \"Letter like a-z\",\n",
    "    \"Other entity\",\n",
    "    \"Animal\",\n",
    "    \"Expression abbreviated\",\n",
    "    \"Price\",\n",
    "    \"Techniques and method\",\n",
    "    \"Musical instrument\",\n",
    "    \"Mountain\",\n",
    "    \"Currency name\",\n",
    "    \"Event\",\n",
    "    \"Product\",\n",
    "    \"State\",\n",
    "    \"Individual\",\n",
    "    \"Organ of body\",\n",
    "    \"Reason\",\n",
    "    \"Manner of an action\",\n",
    "    \"City\",\n",
    "    \"Religion\",\n",
    "    \"Invention, book and other creative piece\",\n",
    "    \"Distance, linear measure\",\n",
    "    \"Temperature\",\n",
    "    \"Postcode or other code\",\n",
    "    \"Size, area and volume\",\n",
    "    \"Sport\",\n",
    "    \"Country\",\n",
    "    \"Other location\",\n",
    "    \"Lasting time of somethin\",\n",
    "    \"Equivalent term\",\n",
    "    \"Description of something\",\n",
    "    \"Weight\",\n",
    "    \"Vehicle\",\n",
    "    \"Color\",\n",
    "    \"Other number\",\n",
    "    \"Definition of something\",\n",
    "    \"Element and substance\",\n",
    "    \"Description of a person\",\n",
    "    \"Symbols and sign\",\n",
    "    \"Number of something\",\n",
    "    \"Plant\",\n",
    "    \"Percent, fraction\",\n",
    "    \"Group or organization of person\",\n",
    "    \"Title of a person\",\n",
    "]\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    common = Counter(prediction) & Counter(ground_truth)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction)\n",
    "    recall = 1.0 * num_same / len(ground_truth)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def process_string(input_string):\n",
    "    processed_string = input_string.strip(\"[]\").replace(\"\\\\\", \"\")\n",
    "    questions = processed_string.split(\"\\n\")\n",
    "    questions = [q.strip(\"'\") for q in questions]\n",
    "    return questions\n",
    "\n",
    "\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "scorer = BERTScorer(model_type=\"bert-large-uncased\")\n",
    "bleu = BLEU(tokenize=\"flores101\", effective_order=True)\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(generation, reference):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"])\n",
    "    scores = scorer.score(generation, reference)\n",
    "    precision = scores[\"rougeL\"][0]\n",
    "    recall = scores[\"rougeL\"][1]\n",
    "    fmeasure = scores[\"rougeL\"][2]\n",
    "    return {\"precision\": precision, \"recall\": recall, \"fmeasure\": fmeasure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_bleu_score(row):\n",
    "    generated_response = row[\"generated_response\"]\n",
    "    reference_answer = row[\"target\"]\n",
    "    # Calculate BLEU score\n",
    "    score = bleu.sentence_score(\n",
    "        hypothesis=generated_response, references=[reference_answer]\n",
    "    ).score\n",
    "    return score\n",
    "\n",
    "\n",
    "def compute_rouge_score(row):\n",
    "    generated_response = row[\"generated_response\"]\n",
    "    reference_answer = row[\"target\"]\n",
    "    # Calculate ROUGE scores\n",
    "    answer_dict = calculate_rouge_scores(generated_response, reference_answer)\n",
    "    return answer_dict[\"fmeasure\"]  # Return the f-measure score\n",
    "\n",
    "\n",
    "def compute_icl_score(row):\n",
    "    generated_response = row[\"generated_response\"]\n",
    "    reference_answer = row[\"target\"].strip(\"[]' \")  # Clean up target string\n",
    "    # Assume classification_score is a defined function\n",
    "    score = classification_score(generated_response, reference_answer, categories)\n",
    "    return score\n",
    "\n",
    "\n",
    "output_folder = \"\"\n",
    "output_prefix = \"\"\n",
    "\n",
    "\n",
    "tqdm.pandas(\"Processing MT Dataset\")\n",
    "MT_df = df[df[\"dataset\"] == \"MT\"]\n",
    "MT_df[\"MT_accuracy\"] = MT_df.progress_apply(compute_bleu_score, axis=1)\n",
    "MT_df.to_json(f\"{output_folder}/{output_prefix}_MT.jsonl\", lines=True, orient=\"records\")\n",
    "\n",
    "tqdm.pandas(\"Processing SUMM Dataset\")\n",
    "SUMM_df = df[df[\"dataset\"] == \"SUMM\"]\n",
    "SUMM_df[\"SUMM_accuracy\"] = SUMM_df.progress_apply(compute_rouge_score, axis=1)\n",
    "SUMM_df.to_json(\n",
    "    f\"{output_folder}/{output_prefix}_SUMM.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "\n",
    "tqdm.pandas(\"Processing ICL Dataset\")\n",
    "ICL_df = df[df[\"dataset\"] == \"ICL\"]\n",
    "ICL_df[\"ICL_accuracy\"] = ICL_df.progress_apply(compute_icl_score, axis=1)\n",
    "ICL_df.to_json(\n",
    "    f\"{output_folder}/{output_prefix}_ICL.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "\n",
    "\n",
    "# CR Dataset Processing\n",
    "for i, row in tqdm(\n",
    "    df[df[\"dataset\"] == \"CR\"].iterrows(),\n",
    "    total=df[df[\"dataset\"] == \"CR\"].shape[0],\n",
    "    desc=\"Processing CR Rows\",\n",
    "    unit=\"Row\",\n",
    "):\n",
    "    update_row = row.copy()\n",
    "    # Get Basic Information\n",
    "    SYS_MSG = \"\"\"Evaluate the provided answer (if available) and the generated answer, and respond to the following question with either 'Yes' or 'No'. Choose 'Yes' if both answers convey the same meaning. Choose 'No' if the meanings of the two answers differ.\"\"\"\n",
    "    generated_response = row[\"generated_response\"]\n",
    "    reference_answer = row[\"target\"]\n",
    "    message_content = (\n",
    "        f'{SYS_MSG}\\nAnswer 1:\\n\"{generated_response}\"\\nAnswer 2:\\n{reference_answer}\\n'\n",
    "    )\n",
    "    ealuation_method = evaluate_response_CR\n",
    "\n",
    "    # Do the Evaluation\n",
    "    evaluator.evaluation_method = ealuation_method\n",
    "    label, generated_response = evaluator.evaluate(question_content=message_content)\n",
    "    update_row[\"CR_label\"] = label\n",
    "    update_row[\"CR_evaluation_response\"] = generated_response\n",
    "    update_row[\"CR_accuracy\"] = int(label)\n",
    "    with open(f\"{output_folder}/{output_prefix}_CR.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(update_row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# CT Dataset Processing\n",
    "for i, row in tqdm(\n",
    "    df[df[\"dataset\"] == \"CT\"].iterrows(),\n",
    "    total=df[df[\"dataset\"] == \"CT\"].shape[0],\n",
    "    desc=\"Processing CT Rows\",\n",
    "    unit=\"Row\",\n",
    "):\n",
    "    update_row = row.copy()\n",
    "    generated_response = row[\"generated_response\"]\n",
    "    plot = row[\"ref\"]\n",
    "    evaluation_method = evaluate_response_CT\n",
    "    labels = []\n",
    "    gen_eval_responses = []\n",
    "\n",
    "    for question in tests:\n",
    "        prompt = evaluate_prompt_template_CT.format(\n",
    "            story=generated_response,\n",
    "            background=plot,\n",
    "            question=question,\n",
    "        )\n",
    "        # evaluator.openai_api_key = api_key\n",
    "        label, gen_eval_response = evaluator.evaluate(question_content=message_content)\n",
    "        labels.append(label)\n",
    "        gen_eval_responses.append(gen_eval_response)\n",
    "\n",
    "    for i, question in enumerate(tests):\n",
    "        update_row[f\"CT_Q{i}\"] = gen_eval_responses[i]\n",
    "        update_row[f\"CT_Label_{i}\"] = labels[i]\n",
    "\n",
    "    true_count = labels.count(True)\n",
    "    accuracy = true_count / len(labels) if labels else 0\n",
    "    update_row[\"CT_accuracy\"] = accuracy\n",
    "    with open(f\"{output_folder}/{output_prefix}_CT.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(update_row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "    # evaluator.openai_api_key = None\n",
    "\n",
    "# IF Dataset Processing\n",
    "for i, row in tqdm(\n",
    "    df[df[\"dataset\"] == \"IF\"].iterrows(),\n",
    "    total=df[df[\"dataset\"] == \"IF\"].shape[0],\n",
    "    desc=\"Processing IF Rows\",\n",
    "    unit=\"Row\",\n",
    "):\n",
    "    update_row = row.copy()\n",
    "    SYS_MSG = \"Based on the provided Input (if any) and Generated Text, answer the ensuing Questions with either a YES or NO choice. Your selection should be based on your judgment as well as the following rules:\\n\\n- YES: Select 'YES' if the generated text entirely fulfills the condition specified in the question. However, note that even minor inaccuracies exclude the text from receiving a 'YES' rating. As an illustration. consider a question that asks. \\\"Does each sentence in the generated text use a second person?” If even one sentence does not use the second person, the answer should NOT be 'YES'. To qualify for a 'YES' rating, the generated text must be entirely accurate and relevant to the question\\n\\n- NO: Opt for 'NO' if the generated text fails to meet the question's requirements or provides no information that could be utilized to answer the question. For instance, if the question asks. \\\"Is the second sentence in the generated text a compound sentence?\\\" and the generated text only has one sentence. it offers no relevant information to answer the question. Consequently, the answer should be 'NO'.\"\n",
    "    generated_response = row[\"generated_response\"]\n",
    "    reference_answer = row[\"target\"]\n",
    "    prompt_input = row[\"input\"]\n",
    "    gen_eval_responses = []\n",
    "    labels = []\n",
    "\n",
    "    for question in process_string(reference_answer):\n",
    "        content = f'{SYS_MSG}\\n\\nInput:\\n\"{prompt_input}\"\\n\\nGenerated Text:\\n\"{generated_response}\"\\n\\nQuestion:\\n{question}\\n'\n",
    "        evaluator.evaluation_method = evaluate_response_IF\n",
    "        label, gen_eval_response = evaluator.evaluate(question_content=content)\n",
    "        labels.append(label)\n",
    "        gen_eval_responses.append(gen_eval_response)\n",
    "\n",
    "    true_count = labels.count(True)\n",
    "    accuracy = true_count / len(labels) if labels else 0\n",
    "    update_row[\"IF_accuracy\"] = accuracy\n",
    "    update_row[\"IF_label\"] = labels\n",
    "    update_row[\"IF_evaluation_response\"] = gen_eval_responses\n",
    "    with open(f\"{output_folder}/{output_prefix}_IF.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(update_row.to_dict(), ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env_lujun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
