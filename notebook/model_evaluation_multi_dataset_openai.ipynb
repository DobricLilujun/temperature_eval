{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/vllm_env_lujun/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "from utils.evaluators import EVALUATOR\n",
    "from langchain import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from bert_score import BERTScorer\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu.metrics import BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:03<00:00, 1097.55it/s]\n",
      "/tmp/ipykernel_1197216/3907256857.py:264: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MT_df[\"MT_accuracy\"] = MT_df.progress_apply(compute_bleu_score, axis=1)\n",
      "100%|██████████| 4228/4228 [00:04<00:00, 946.36it/s] \n",
      "/tmp/ipykernel_1197216/3907256857.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SUMM_df[\"SUMM_accuracy\"] = SUMM_df.progress_apply(compute_rouge_score, axis=1)\n",
      "100%|██████████| 2800/2800 [00:00<00:00, 89957.57it/s]\n",
      "/tmp/ipykernel_1197216/3907256857.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ICL_df[\"ICL_accuracy\"] = ICL_df.progress_apply(compute_icl_score, axis=1)\n",
      "Processing CT Rows: 100%|██████████| 168/168 [42:25<00:00, 15.15s/Row]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/.vscode/api_key.txt\",\n",
    "    \"r\",\n",
    ") as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "LIST_DATASET_CSV = [\"CR\", \"CT\", \"ICL\", \"IF\", \"MT\", \"SUMM\"]  # Dataset list\n",
    "\n",
    "\n",
    "input_file_folder = \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Additional_Results/model_complementary_4bits\"\n",
    "file_path = f\"{input_file_folder}/vllm_exp_dataset_csv_Meta-Llama-3-8B-Instruct-awq__20250113_201944.jsonl\"\n",
    "model_name = \"Meta-Llama-3-8B-Instruct-awq\"\n",
    "evaluator_model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "model_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "evaluator = EVALUATOR(\n",
    "    server_url=None,\n",
    "    model_name=None,\n",
    ")\n",
    "\n",
    "with open(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/ttcw_all_tests.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    tests = json.load(f)\n",
    "\n",
    "\n",
    "output_folder = f\"{input_file_folder}/evaluation/\"\n",
    "output_prefix = f\"evaluated_{evaluator_model_name}_{model_name}\"\n",
    "\n",
    "\n",
    "def evaluate_response_CR(generation):\n",
    "    # check if generation is yes or no\n",
    "    if generation.lower().startswith(\"yes\") or generation.lower().startswith(\"no\"):\n",
    "        if generation.lower().startswith(\"yes\"):\n",
    "            return True, generation\n",
    "        else:\n",
    "            return False, generation\n",
    "    else:\n",
    "        if \"yes\" in generation.lower() and \"no\" not in generation.lower():\n",
    "            return True, generation\n",
    "        elif \"yes\" not in generation.lower() and \"np\" in generation.lower():\n",
    "            return False, generation\n",
    "        else:\n",
    "            # print(\"NO YES or NO answer!\" + generation)\n",
    "            return None, generation\n",
    "\n",
    "\n",
    "def evaluate_response_CT(generation):\n",
    "    # check if generation is yes or no\n",
    "    if generation.lower().startswith(\"yes\") or generation.lower().startswith(\"no\"):\n",
    "        if generation.lower().startswith(\"yes\"):\n",
    "            return True, generation\n",
    "        else:\n",
    "            return False, generation\n",
    "    else:\n",
    "        if \"yes\" in generation.lower() and \"no\" not in generation.lower():\n",
    "            return True, generation\n",
    "        elif \"yes\" not in generation.lower() and \"np\" in generation.lower():\n",
    "            return False, generation\n",
    "        else:\n",
    "            # print(\"NO YES or NO answer!\" + generation)\n",
    "            return None, generation\n",
    "\n",
    "\n",
    "def evaluate_response_IF(generation):\n",
    "    # check if generation is yes or no\n",
    "    if generation.lower().startswith(\"yes\") or generation.lower().startswith(\"no\"):\n",
    "        if generation.lower().startswith(\"yes\"):\n",
    "            return True, generation\n",
    "        else:\n",
    "            return False, generation\n",
    "    else:\n",
    "        if \"yes\" in generation.lower() and \"no\" not in generation.lower():\n",
    "            return True, generation\n",
    "        elif \"yes\" not in generation.lower() and \"np\" in generation.lower():\n",
    "            return False, generation\n",
    "        else:\n",
    "            # print(\"NO YES or NO answer!\" + generation)\n",
    "            return None, generation\n",
    "\n",
    "\n",
    "evaluate_prompt_CT = \"\"\"You are given a creative short-story. Read it carefully. You are then given some background about specific aspects of creative writing, as well as a binary (Yes/No) question. Your objective is to use the background information to answer the question about the story. Start your answer with \"Yes\" or \"No\". You can optionally then provide a short explanation for your answer.\n",
    "\n",
    "==========\n",
    "Story:\n",
    "{story}\n",
    "==========\n",
    "Background:\n",
    "{background}\n",
    "\n",
    "==========\n",
    "Question: {question}\n",
    "\n",
    "Remember to start your answer with Yes or No. You can optionally then provide a short explanation for your answer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def full_prompt2context(full_prompt):\n",
    "    lines = full_prompt.strip().split(\"\\n\")\n",
    "    kept1 = \"\\n\".join(lines[:-1]).strip().split(\"\\n\")\n",
    "    kept2 = kept1[:-1]\n",
    "    return \"\\n\".join(kept2).strip()\n",
    "\n",
    "\n",
    "for test in tests:\n",
    "    test[\"expanded_context\"] = full_prompt2context(test[\"full_prompt\"])\n",
    "\n",
    "evaluate_prompt_template_CT = PromptTemplate.from_template(evaluate_prompt_CT)\n",
    "\n",
    "\n",
    "def classification_score(prediction, ground_truth, all_classes):\n",
    "    em_match_list = []\n",
    "    for class_name in all_classes:\n",
    "        if class_name in prediction:\n",
    "            em_match_list.append(class_name)\n",
    "    for match_term in em_match_list:\n",
    "        if match_term in ground_truth and match_term != ground_truth:\n",
    "            em_match_list.remove(match_term)\n",
    "    if ground_truth in em_match_list:\n",
    "        score = 1.0 / len(em_match_list)\n",
    "    else:\n",
    "        score = 0.0\n",
    "    return score\n",
    "\n",
    "\n",
    "categories = [\n",
    "    \"Food\",\n",
    "    \"Date\",\n",
    "    \"Order, rank\",\n",
    "    \"Speed\",\n",
    "    \"Disease and medicine\",\n",
    "    \"Word with a special property\",\n",
    "    \"Abbreviation\",\n",
    "    \"Language\",\n",
    "    \"Letter like a-z\",\n",
    "    \"Other entity\",\n",
    "    \"Animal\",\n",
    "    \"Expression abbreviated\",\n",
    "    \"Price\",\n",
    "    \"Techniques and method\",\n",
    "    \"Musical instrument\",\n",
    "    \"Mountain\",\n",
    "    \"Currency name\",\n",
    "    \"Event\",\n",
    "    \"Product\",\n",
    "    \"State\",\n",
    "    \"Individual\",\n",
    "    \"Organ of body\",\n",
    "    \"Reason\",\n",
    "    \"Manner of an action\",\n",
    "    \"City\",\n",
    "    \"Religion\",\n",
    "    \"Invention, book and other creative piece\",\n",
    "    \"Distance, linear measure\",\n",
    "    \"Temperature\",\n",
    "    \"Postcode or other code\",\n",
    "    \"Size, area and volume\",\n",
    "    \"Sport\",\n",
    "    \"Country\",\n",
    "    \"Other location\",\n",
    "    \"Lasting time of somethin\",\n",
    "    \"Equivalent term\",\n",
    "    \"Description of something\",\n",
    "    \"Weight\",\n",
    "    \"Vehicle\",\n",
    "    \"Color\",\n",
    "    \"Other number\",\n",
    "    \"Definition of something\",\n",
    "    \"Element and substance\",\n",
    "    \"Description of a person\",\n",
    "    \"Symbols and sign\",\n",
    "    \"Number of something\",\n",
    "    \"Plant\",\n",
    "    \"Percent, fraction\",\n",
    "    \"Group or organization of person\",\n",
    "    \"Title of a person\",\n",
    "]\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    common = Counter(prediction) & Counter(ground_truth)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction)\n",
    "    recall = 1.0 * num_same / len(ground_truth)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def process_string(input_string):\n",
    "    processed_string = input_string.strip(\"[]\").replace(\"\\\\\", \"\")\n",
    "    questions = processed_string.split(\"\\n\")\n",
    "    questions = [q.strip(\"'\") for q in questions]\n",
    "    return questions\n",
    "\n",
    "\n",
    "scorer = BERTScorer(model_type=\"bert-large-uncased\")\n",
    "bleu = BLEU(tokenize=\"flores101\", effective_order=True)\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(generation, reference):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"])\n",
    "    scores = scorer.score(generation, reference)\n",
    "    precision = scores[\"rougeL\"][0]\n",
    "    recall = scores[\"rougeL\"][1]\n",
    "    fmeasure = scores[\"rougeL\"][2]\n",
    "    return {\"precision\": precision, \"recall\": recall, \"fmeasure\": fmeasure}\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_bleu_score(row):\n",
    "    generated_response = row[\"generate_response\"]\n",
    "    reference_answer = row[\"target\"]\n",
    "    # Calculate BLEU score\n",
    "    score = bleu.sentence_score(\n",
    "        hypothesis=generated_response, references=[reference_answer]\n",
    "    ).score\n",
    "    return score\n",
    "\n",
    "\n",
    "def compute_rouge_score(row):\n",
    "    generated_response = row[\"generate_response\"]\n",
    "    reference_answer = row[\"target\"]\n",
    "    # Calculate ROUGE scores\n",
    "    answer_dict = calculate_rouge_scores(generated_response, reference_answer)\n",
    "    return answer_dict[\"fmeasure\"]  # Return the f-measure score\n",
    "\n",
    "\n",
    "def compute_icl_score(row):\n",
    "    generated_response = row[\"generate_response\"]\n",
    "    reference_answer = row[\"target\"].strip(\"[]' \")  # Clean up target string\n",
    "    # Assume classification_score is a defined function\n",
    "    score = classification_score(generated_response, reference_answer, categories)\n",
    "    return score\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "MT_df = df[df[\"category\"] == \"MT\"]\n",
    "MT_df[\"MT_accuracy\"] = MT_df.progress_apply(compute_bleu_score, axis=1)\n",
    "MT_df.to_json(f\"{output_folder}/{output_prefix}_MT.jsonl\", lines=True, orient=\"records\")\n",
    "\n",
    "tqdm.pandas()\n",
    "SUMM_df = df[df[\"category\"] == \"SUMM\"]\n",
    "SUMM_df[\"SUMM_accuracy\"] = SUMM_df.progress_apply(compute_rouge_score, axis=1)\n",
    "SUMM_df.to_json(\n",
    "    f\"{output_folder}/{output_prefix}_SUMM.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "ICL_df = df[df[\"category\"] == \"ICL\"]\n",
    "ICL_df[\"ICL_accuracy\"] = ICL_df.progress_apply(compute_icl_score, axis=1)\n",
    "ICL_df.to_json(\n",
    "    f\"{output_folder}/{output_prefix}_ICL.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "\n",
    "\n",
    "# # CR Dataset Processing\n",
    "# for i, row in tqdm(\n",
    "#     df[df[\"category\"] == \"CR\"].iterrows(),\n",
    "#     total=df[df[\"category\"] == \"CR\"].shape[0],\n",
    "#     desc=\"Processing CR Rows\",\n",
    "#     unit=\"Row\",\n",
    "# ):\n",
    "#     update_row = row.copy()\n",
    "#     # Get Basic Information\n",
    "#     SYS_MSG = \"\"\"Evaluate the provided answer (if available) and the generated answer, and respond to the following question only with either 'Yes' or 'No'. Choose 'Yes' if both answers convey the same meaning. Choose 'No' if the meanings of the two answers differ.\"\"\"\n",
    "#     generated_response = row[\"generate_response\"]\n",
    "#     reference_answer = row[\"target\"]\n",
    "#     message_content = (\n",
    "#         f'{SYS_MSG}\\nAnswer 1:\\n\"{generated_response}\"\\nAnswer 2:\\n{reference_answer}\\n'\n",
    "#     )\n",
    "\n",
    "#     # Do the Evaluation\n",
    "#     evaluator.evaluation_method = evaluate_response_CR\n",
    "#     label, generated_response = evaluator.evaluate(question_content=message_content)\n",
    "#     update_row[\"CR_question_evaluation\"] = message_content\n",
    "#     update_row[\"CR_label\"] = label\n",
    "#     update_row[\"CR_evaluation_response\"] = generated_response\n",
    "#     update_row[\"CR_accuracy\"] = int(label) if label is not None else 0\n",
    "#     with open(f\"{output_folder}/{output_prefix}_CR.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "#         f.write(json.dumps(update_row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# CT Dataset Processing\n",
    "for i, row in tqdm(\n",
    "    df[df[\"category\"] == \"CT\"].iterrows(),\n",
    "    total=df[df[\"category\"] == \"CT\"].shape[0],\n",
    "    desc=\"Processing CT Rows\",\n",
    "    unit=\"Row\",\n",
    "):\n",
    "    update_row = row.copy()\n",
    "    generated_response = row[\"generate_response\"]\n",
    "\n",
    "    evaluator.evaluation_method = evaluate_response_CT\n",
    "    labels = []\n",
    "    gen_eval_responses = []\n",
    "    message_contents = []\n",
    "    for question in tests:\n",
    "        background = question[\"expanded_context\"]\n",
    "        Q = question[\"question\"]\n",
    "        message_content = evaluate_prompt_template_CT.format(\n",
    "            story=generated_response,\n",
    "            background=background,\n",
    "            question=Q,\n",
    "        )\n",
    "        evaluator.openai_api_key = api_key\n",
    "        label, gen_eval_response = evaluator.evaluate(question_content=message_content)\n",
    "        labels.append(label)\n",
    "        gen_eval_responses.append(gen_eval_response)\n",
    "        message_contents.append(message_content)\n",
    "\n",
    "    update_row[\"CT_Label_{i}\"] = str(labels)\n",
    "    update_row[\"CT_Q{i}_responses\"] = str(gen_eval_responses)\n",
    "    update_row[\"CT_question_evaluation\"] = str(message_contents)\n",
    "    true_count = labels.count(True)\n",
    "    accuracy = true_count / len(labels) if labels else 0\n",
    "    update_row[\"CT_accuracy\"] = accuracy\n",
    "    with open(f\"{output_folder}/{output_prefix}_CT.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(update_row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "    evaluator.openai_api_key = None\n",
    "\n",
    "# IF Dataset Processing\n",
    "# for i, row in tqdm(\n",
    "#     df[df[\"category\"] == \"IF\"].iterrows(),\n",
    "#     total=df[df[\"category\"] == \"IF\"].shape[0],\n",
    "#     desc=\"Processing IF Rows\",\n",
    "#     unit=\"Row\",\n",
    "# ):\n",
    "#     update_row = row.copy()\n",
    "#     SYS_MSG = \"Based on the provided Input (if any) and Generated Text, answer the ensuing Questions with either a YES or NO choice. Your selection should be based on your judgment as well as the following rules:\\n\\n- YES: Select 'YES' if the generated text entirely fulfills the condition specified in the question. However, note that even minor inaccuracies exclude the text from receiving a 'YES' rating. As an illustration. consider a question that asks. \\\"Does each sentence in the generated text use a second person?” If even one sentence does not use the second person, the answer should NOT be 'YES'. To qualify for a 'YES' rating, the generated text must be entirely accurate and relevant to the question\\n\\n- NO: Opt for 'NO' if the generated text fails to meet the question's requirements or provides no information that could be utilized to answer the question. For instance, if the question asks. \\\"Is the second sentence in the generated text a compound sentence?\\\" and the generated text only has one sentence. it offers no relevant information to answer the question. Consequently, the answer should be 'NO'.'''\"\n",
    "#     generated_response = row[\"generate_response\"]\n",
    "#     reference_answer = row[\"target\"]\n",
    "#     prompt_input = row[\"input\"]\n",
    "#     gen_eval_responses = []\n",
    "#     labels = []\n",
    "#     message_contents = []\n",
    "#     for question in process_string(reference_answer):\n",
    "\n",
    "#         content = f'{SYS_MSG}\\n\\nGenerated Text:\\n\"{generated_response}\"\\n\\nQuestion:\\n{question}\\n'\n",
    "#         evaluator.evaluation_method = evaluate_response_IF\n",
    "#         label, gen_eval_response = evaluator.evaluate(question_content=content)\n",
    "#         labels.append(label)\n",
    "#         gen_eval_responses.append(gen_eval_response)\n",
    "#         message_contents.append(content)\n",
    "\n",
    "#     update_row[\"IF_question_evaluation\"] = str(message_contents)\n",
    "#     true_count = labels.count(True)\n",
    "#     accuracy = true_count / len(labels) if labels else 0\n",
    "#     update_row[\"IF_accuracy\"] = accuracy\n",
    "#     update_row[\"IF_label\"] = str(labels)\n",
    "#     update_row[\"IF_evaluation_response\"] = str(gen_eval_responses)\n",
    "#     with open(f\"{output_folder}/{output_prefix}_IF.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "#         f.write(json.dumps(update_row.to_dict(), ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env_lujun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
