{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.foyer.lu/repository/pypi-all/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting autoawq\n",
      "  Downloading https://nexus.foyer.lu/repository/pypi-all/packages/autoawq/0.2.7.post3/autoawq-0.2.7.post3-py3-none-any.whl (107 kB)\n",
      "Requirement already satisfied: torch>=2.2.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from autoawq) (2.4.0)\n",
      "Requirement already satisfied: triton in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from autoawq) (3.0.0)\n",
      "Requirement already satisfied: transformers>=4.45.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from autoawq) (4.45.2)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from autoawq) (0.20.3)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from autoawq) (4.12.2)\n",
      "Requirement already satisfied: accelerate in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from autoawq) (1.2.0)\n",
      "Collecting datasets>=2.20 (from autoawq)\n",
      "  Downloading https://nexus.foyer.lu/repository/pypi-all/packages/datasets/3.2.0/datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Collecting zstandard (from autoawq)\n",
      "  Downloading https://nexus.foyer.lu/repository/pypi-all/packages/zstandard/0.23.0/zstandard-0.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface_hub>=0.26.5 (from autoawq)\n",
      "  Downloading https://nexus.foyer.lu/repository/pypi-all/packages/huggingface-hub/0.27.0/huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Requirement already satisfied: filelock in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (15.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (2.1.4)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.20->autoawq)\n",
      "  Downloading https://nexus.foyer.lu/repository/pypi-all/packages/requests/2.32.3/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets>=2.20->autoawq)\n",
      "  Downloading https://nexus.foyer.lu/repository/pypi-all/packages/tqdm/4.67.1/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: xxhash in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.20->autoawq) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (3.9.3)\n",
      "Requirement already satisfied: packaging in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from datasets>=2.20->autoawq) (6.0.1)\n",
      "Requirement already satisfied: sympy in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (1.12)\n",
      "Requirement already satisfied: networkx in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->autoawq) (12.4.99)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from transformers>=4.45.0->autoawq) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from transformers>=4.45.0->autoawq) (0.4.5)\n",
      "Requirement already satisfied: psutil in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from accelerate->autoawq) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from aiohttp->datasets>=2.20->autoawq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from aiohttp->datasets>=2.20->autoawq) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from aiohttp->datasets>=2.20->autoawq) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from aiohttp->datasets>=2.20->autoawq) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from aiohttp->datasets>=2.20->autoawq) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from aiohttp->datasets>=2.20->autoawq) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from jinja2->torch>=2.2.0->autoawq) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from pandas->datasets>=2.20->autoawq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from pandas->datasets>=2.20->autoawq) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from pandas->datasets>=2.20->autoawq) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from sympy->torch>=2.2.0->autoawq) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/llama/Personal_Directories/srb/causalEnv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.16.0)\n",
      "Installing collected packages: zstandard, tqdm, requests, huggingface_hub, datasets, autoawq\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.25.1\n",
      "    Uninstalling huggingface-hub-0.25.1:\n",
      "      Successfully uninstalled huggingface-hub-0.25.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.18.0\n",
      "    Uninstalling datasets-2.18.0:\n",
      "      Successfully uninstalled datasets-2.18.0\n",
      "Successfully installed autoawq-0.2.7.post3 datasets-3.2.0 huggingface_hub-0.27.0 requests-2.32.3 tqdm-4.67.1 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8015f748764da094284d0748fb6455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ:   0%|          | 0/80 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
      "AWQ:  76%|███████▋  | 61/80 [1:41:46<31:42, 100.11s/it]  \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 3 has a total capacity of 79.11 GiB of which 299.62 MiB is free. Process 1280192 has 71.44 GiB memory in use. Including non-PyTorch memory, this process has 7.34 GiB memory in use. Of the allocated memory 6.84 GiB is allocated by PyTorch, and 1.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/llama/Personal_Directories/srb/temperature_eval/vllm_quantization_awq.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/temperature_eval/vllm_quantization_awq.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/temperature_eval/vllm_quantization_awq.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 应用量化配置\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/temperature_eval/vllm_quantization_awq.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39;49mquantize(tokenizer, quant_config\u001b[39m=\u001b[39;49mquant_config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/temperature_eval/vllm_quantization_awq.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# 保存量化后的模型\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/temperature_eval/vllm_quantization_awq.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39msave_quantized(quant_path)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/awq/models/base.py:239\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.quantize\u001b[0;34m(self, tokenizer, quant_config, calib_data, split, text_column, duo_scaling, export_compatible, apply_clip, n_parallel_calib_samples, max_calib_samples, max_calib_seq_len, max_chunk_memory, quantizer_cls, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_config\u001b[39m.\u001b[39mmodules_to_not_convert \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m    218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantizer \u001b[39m=\u001b[39m quantizer_cls(\n\u001b[1;32m    219\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    220\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    238\u001b[0m )\n\u001b[0;32m--> 239\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantizer\u001b[39m.\u001b[39;49mquantize()\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_quantized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/awq/quantize/quantizer.py:154\u001b[0m, in \u001b[0;36mAwqQuantizer.quantize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minps\u001b[39m.\u001b[39mto(common_device)\n\u001b[1;32m    151\u001b[0m \u001b[39m# We need to move the rotary embedding every time we move to a new module.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m# Transformers 4.45.0 moved rotary embedding to model definition as of this PR:\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m# https://github.com/huggingface/transformers/pull/32617\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mawq_model\u001b[39m.\u001b[39;49mmove_embed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, common_device)\n\u001b[1;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    157\u001b[0m     \u001b[39m# position embeddings found in tuple\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/awq/models/llama.py:33\u001b[0m, in \u001b[0;36mLlamaAWQForCausalLM.move_embed\u001b[0;34m(model, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmove_embed\u001b[39m(model: OldLlamaForCausalLM, device: \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49membed_tokens\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     34\u001b[0m     model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mrotary_emb \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mrotary_emb\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1161\u001b[0m         device,\n\u001b[1;32m   1162\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1163\u001b[0m         non_blocking,\n\u001b[1;32m   1164\u001b[0m     )\n\u001b[1;32m   1165\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 3 has a total capacity of 79.11 GiB of which 299.62 MiB is free. Process 1280192 has 71.44 GiB memory in use. Including non-PyTorch memory, this process has 7.34 GiB memory in use. Of the allocated memory 6.84 GiB is allocated by PyTorch, and 1.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/home/llama/models/base_models/Meta-Llama-3-70B-Instruct/\"  # 原始模型路径\n",
    "quant_path = \"/home/llama/models/base_models/Meta-Llama-3-70B-Instruct-awq\"  # 保存量化后模型的路径\n",
    "\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 64, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "\n",
    "# 加载原始模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "# 应用量化配置\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# 保存量化后的模型\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "print(f\"Model is quantized and saved at '{quant_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ:   0%|          | 0/28 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
      "AWQ: 100%|██████████| 28/28 [05:30<00:00, 11.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is quantized and saved at '/home/llama/models/base_models/Qwen2.5-1.5B-Instruct-awq'\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/home/llama/models/base_models/Qwen2.5-1.5B-Instruct/\"  # 原始模型路径\n",
    "quant_path = \"/home/llama/models/base_models/Qwen2.5-1.5B-Instruct-awq\"  # 保存量化后模型的路径\n",
    "\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 64, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "\n",
    "# 加载原始模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, device_map=\"cuda:0\")\n",
    "\n",
    "# 应用量化配置\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# 保存量化后的模型\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "print(f\"Model is quantized and saved at '{quant_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d116ec1e185c4b79b93ede270a0837a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 36/36 [10:42<00:00, 17.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is quantized and saved at '/home/llama/models/base_models/Qwen2.5-3B-Instruct-awq'\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/home/llama/models/base_models/Qwen2.5-3B-Instruct/\"  # 原始模型路径\n",
    "quant_path = \"/home/llama/models/base_models/Qwen2.5-3B-Instruct-awq\"  # 保存量化后模型的路径\n",
    "\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 64, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "\n",
    "# 加载原始模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, device_map=\"cuda:0\")\n",
    "\n",
    "# 应用量化配置\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# 保存量化后的模型\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "print(f\"Model is quantized and saved at '{quant_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>ref</th>\n",
       "      <th>category</th>\n",
       "      <th>dataset</th>\n",
       "      <th>temperature</th>\n",
       "      <th>seed</th>\n",
       "      <th>generate_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>The treasure chest would have remained closed.</td>\n",
       "      <td>Premise: A feather falls from a skyscraper. \\n...</td>\n",
       "      <td>CR</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>47</td>\n",
       "      <td>The treasure chest would have remained closed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>The hostages would have remained in danger.</td>\n",
       "      <td>Premise: A feather falls from a skyscraper. \\n...</td>\n",
       "      <td>CR</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>47</td>\n",
       "      <td>The hostages would have remained in danger.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>Without a barrier, the man would have been eaten.</td>\n",
       "      <td>Premise: A feather falls from a skyscraper. \\n...</td>\n",
       "      <td>CR</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>47</td>\n",
       "      <td>Without a barrier, the lion would have been ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>The girl would have been angry.</td>\n",
       "      <td>Premise: A feather falls from a skyscraper. \\n...</td>\n",
       "      <td>CR</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>47</td>\n",
       "      <td>That is not possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>She would have been liable to prosecution.</td>\n",
       "      <td>Premise: A feather falls from a skyscraper. \\n...</td>\n",
       "      <td>CR</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>47</td>\n",
       "      <td>That is not possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33343</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>A 40-year inquest has been halted due to the l...</td>\n",
       "      <td>None</td>\n",
       "      <td>SUMM</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>49</td>\n",
       "      <td>The delayed inquest into an IRA massacre occur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33344</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>Rare drawings by Leonardo da Vinci have gone o...</td>\n",
       "      <td>None</td>\n",
       "      <td>SUMM</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>49</td>\n",
       "      <td>Rare creations by Renaissance master Leonardo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33345</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>A Bangkok Airways plane crashed killing the pi...</td>\n",
       "      <td>None</td>\n",
       "      <td>SUMM</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>49</td>\n",
       "      <td>The Bangkok Airways ATR-72 turboprop plane tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33346</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>The 31-year-old Emmanual Adebayor took to Twit...</td>\n",
       "      <td>None</td>\n",
       "      <td>SUMM</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>49</td>\n",
       "      <td>Emmanuel Adebayor, being outgoing and reaffirm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33347</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>Emmanuel Adebayor took to Twitter to announce ...</td>\n",
       "      <td>None</td>\n",
       "      <td>SUMM</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>49</td>\n",
       "      <td>Emmanuel Adebayor, being outgoing and reaffirm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33348 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0      Instruction: You are a conscientious assistant...   \n",
       "1      Instruction: You are a conscientious assistant...   \n",
       "2      Instruction: You are a conscientious assistant...   \n",
       "3      Instruction: You are a conscientious assistant...   \n",
       "4      Instruction: You are a conscientious assistant...   \n",
       "...                                                  ...   \n",
       "33343  \\nRole: You are a writer who is good at summar...   \n",
       "33344  \\nRole: You are a writer who is good at summar...   \n",
       "33345  \\nRole: You are a writer who is good at summar...   \n",
       "33346  \\nRole: You are a writer who is good at summar...   \n",
       "33347  \\nRole: You are a writer who is good at summar...   \n",
       "\n",
       "                                                  target  \\\n",
       "0         The treasure chest would have remained closed.   \n",
       "1            The hostages would have remained in danger.   \n",
       "2      Without a barrier, the man would have been eaten.   \n",
       "3                        The girl would have been angry.   \n",
       "4             She would have been liable to prosecution.   \n",
       "...                                                  ...   \n",
       "33343  A 40-year inquest has been halted due to the l...   \n",
       "33344  Rare drawings by Leonardo da Vinci have gone o...   \n",
       "33345  A Bangkok Airways plane crashed killing the pi...   \n",
       "33346  The 31-year-old Emmanual Adebayor took to Twit...   \n",
       "33347  Emmanuel Adebayor took to Twitter to announce ...   \n",
       "\n",
       "                                                     ref category dataset  \\\n",
       "0      Premise: A feather falls from a skyscraper. \\n...       CR    None   \n",
       "1      Premise: A feather falls from a skyscraper. \\n...       CR    None   \n",
       "2      Premise: A feather falls from a skyscraper. \\n...       CR    None   \n",
       "3      Premise: A feather falls from a skyscraper. \\n...       CR    None   \n",
       "4      Premise: A feather falls from a skyscraper. \\n...       CR    None   \n",
       "...                                                  ...      ...     ...   \n",
       "33343                                               None     SUMM    None   \n",
       "33344                                               None     SUMM    None   \n",
       "33345                                               None     SUMM    None   \n",
       "33346                                               None     SUMM    None   \n",
       "33347                                               None     SUMM    None   \n",
       "\n",
       "       temperature  seed                                  generate_response  \n",
       "0              0.1    47     The treasure chest would have remained closed.  \n",
       "1              0.1    47        The hostages would have remained in danger.  \n",
       "2              0.1    47  Without a barrier, the lion would have been ea...  \n",
       "3              0.1    47                              That is not possible.  \n",
       "4              0.1    47                              That is not possible.  \n",
       "...            ...   ...                                                ...  \n",
       "33343          1.9    49  The delayed inquest into an IRA massacre occur...  \n",
       "33344          1.9    49  Rare creations by Renaissance master Leonardo ...  \n",
       "33345          1.9    49  The Bangkok Airways ATR-72 turboprop plane tra...  \n",
       "33346          1.9    49  Emmanuel Adebayor, being outgoing and reaffirm...  \n",
       "33347          1.9    49  Emmanuel Adebayor, being outgoing and reaffirm...  \n",
       "\n",
       "[33348 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = '/home/llama/Personal_Directories/srb/temperature_eval/data/output/vllm_exp_dataset_csv_Qwen2.5-3B-Instruct-awq__20241226_024442.jsonl'\n",
    "\n",
    "# Read the JSONL file into a DataFrame\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python -m vllm.entrypoints.openai.api_server --model /home/llama/models/base_models/Phi-3.5-mini-instruct-awq --tensor-parallel-size 1 --port 4363 --\n",
    "CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server --model /home/llama/models/base_models/Qwen2.5-3B-Instruct-awq --tensor-parallel-size 1 --port 4364\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=3 python -m vllm.entrypoints.openai.api_server --model /home/llama/models/base_models/Qwen2.5-1.5B-Instruct-awq --tensor-parallel-size 1 --port 4361\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=3 python -m vllm.entrypoints.openai.api_server --model /home/llama/models/base_models/Llama-3.3-70B-Instruct --tensor-parallel-size 1 --port 4361\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=3,2 python -m vllm.entrypoints.openai.api_server --model /home/llama/models/base_models/Llama-3.3-70B-Instruct --tensor-parallel-size 2 --port 4361 --max_model_len 10240\n",
    "\n",
    "python vllm_running_inference.py --model_name Qwen2.5-3B-Instruct-awq --model_path_base /home/llama/models/base_models/ --input_path_folder /home/llama/Personal_Directories/srb/temperature_eval/data/Intermediate/ --rep 3 --server_url http://0.0.0.0:4364/v1/chat/completions\n",
    "\n",
    "python vllm_running_inference.py --model_name Qwen2.5-3B-Instruct-awq --model_path_base /home/llama/models/base_models/ --input_path_folder /home/llama/Personal_Directories/srb/temperature_eval/data/Intermediate/ --rep 3 --server_url http://0.0.0.0:4364/v1/chat/completions\n",
    "\n",
    "python vllm_running_inference.py --model_name Qwen2.5-1.5B-Instruct-awq --model_path_base /home/llama/models/base_models/ --input_path_folder /home/llama/Personal_Directories/srb/temperature_eval/data/Intermediate/ --rep 3 --server_url http://0.0.0.0:4361/v1/chat/completions\n",
    "\n",
    "\n",
    "python vllm_running_inference_generalization.py --model_name Llama-3.2-3B-Instruct-awq --model_path_base /home/llama/models/base_models/ --input_path_folder /home/llama/Personal_Directories/srb/temperature_eval/data/Augemented/ --rep 3 --server_url http://0.0.0.0:4362/v1/chat/completions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/llama/Personal_Directories/srb/temperature_eval/data/output/vllm_exp_dataset_csv_Llama-3.2-1B-Instruct-awq__20241224_155253.csv\", nrows=29593)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"/home/llama/Personal_Directories/srb/temperature_eval/data/output/vllm_exp_dataset_csv_Llama-3.2-1B-Instruct-awq__20241226_020016.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['input', 'target', 'ref', 'category', 'dataset', 'temperature', 'seed',\n",
       "       'generate_response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['input', 'target', 'ref', 'category', 'dataset', 'temperature', 'seed',\n",
       "       'generate_response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df,df2], ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
