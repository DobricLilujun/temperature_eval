{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model Fine-tuning\n",
    "\n",
    "This notebook primarily utilizes the state-of-the-art (SOTA) Google BERT model, originally designed to learn how to order shuffled sentences within a paragraph. While most use cases focus on classification and Named Entity Recognition (NER), we often need to fine-tune this model to suit our specific needs.\n",
    "\n",
    "One important consideration is that the context input length is limited to less than 512 tokens due to the modelâ€™s structural constraints. Additionally, high-quality training data is essential to achieve good generalization. We also test other spliting of dataset and we obtain the similar accuracy (Originally set to 42) but different trace of training loss. This highlights that the model learns something from the data.\n",
    "\n",
    "Model used: BERT-base-multilingual-uncased\n",
    "\n",
    "This [slides](https://docs.google.com/presentation/d/165GxBIZ-Jxk8uaEKK9h02GtFsKsCCQdm8ouar2JkCKU/edit?usp=sharing) has more details see here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    random_split,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import os\n",
    "\n",
    "# Specify your cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "################MT################\n",
    "\n",
    "folder_path = \"/home/lujun_li/projects/temperature_eval/data/MT/\"\n",
    "all_dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and \"evaluated\" in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.split(\"_\")[2]\n",
    "        df[\"model_name\"] = model_name\n",
    "        all_dfs.append(df)\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df = df[~df[\"spbleu\"].isna()]\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float).round(1)\n",
    "df[\"performance_score\"] = df[\"spbleu\"]\n",
    "df[\"ability\"] = \"MT\"\n",
    "df_MT = df[\n",
    "    [\"initial_prompt\", \"model_name\", \"performance_score\", \"Temperature\", \"ability\"]\n",
    "]\n",
    "\n",
    "############################SUM############################\n",
    "folder_path = \"/home/lujun_li/projects/temperature_eval/data/SUM\"\n",
    "all_dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and \"evaluated\" in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.split(\"_\")[2]\n",
    "        df[\"model_name\"] = model_name\n",
    "        all_dfs.append(df)\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df = df[~df[\"rouge_l_fmeasure\"].isna()]\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float).round(1)\n",
    "df[\"performance_score\"] = df[\"rouge_l_fmeasure\"]\n",
    "df[\"ability\"] = \"SUM\"\n",
    "df_SUM = df[\n",
    "    [\"initial_prompt\", \"model_name\", \"performance_score\", \"Temperature\", \"ability\"]\n",
    "]\n",
    "\n",
    "#############################CT############################\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "folder_path = \"/home/lujun_li/projects/temperature_eval/data/CT\"\n",
    "all_dfs = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and \"evaluated\" in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.split(\"_\")[2]\n",
    "        df[\"model_name\"] = model_name\n",
    "        all_dfs.append(df)\n",
    "\n",
    "\n",
    "def label_function(x):\n",
    "    if str(x).startswith(\"Yes\"):\n",
    "        return True\n",
    "    elif str(x).startswith(\"No\"):\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df[\"label\"] = df[\"OpenAI_response\"].apply(label_function)\n",
    "df = df[~df[\"label\"].isna()]\n",
    "df = df[df[\"Temperature\"] != \"temperature\"]\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float).round(1)\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Instruction: Write a New Yorker-style story given the plot below. Make sure it is at least 500 words. Directly start with the story, do not say things like 'Here's the story [...]'\n",
    "Input: \n",
    "Current plot: {current_plot}\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt = PromptTemplate(input_variables=[\"current_plot\"], template=template)\n",
    "\n",
    "df[\"initial_prompt\"] = df[\"plot\"].apply(lambda x: prompt.format(current_plot=x))\n",
    "\n",
    "df = (\n",
    "    df.groupby([\"initial_prompt\", \"Temperature\", \"model_name\"])[\"label\"]\n",
    "    .mean()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df[\"performance_score\"] = df[\"label\"]\n",
    "df[\"ability\"] = \"CT\"\n",
    "df_CT = df[\n",
    "    [\"initial_prompt\", \"model_name\", \"performance_score\", \"Temperature\", \"ability\"]\n",
    "]\n",
    "\n",
    "\n",
    "#############################IF############################\n",
    "\n",
    "folder_path = \"/home/lujun_li/projects/temperature_eval/data/IF\"\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and \"evaluated\" in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.split(\"_\")[2]\n",
    "        df[\"model_name\"] = model_name\n",
    "        all_dfs.append(df)\n",
    "model_map = {\n",
    "    \"Llama-2-7b-chat-hf\": 0,\n",
    "    \"Llama-2-13b-chat-hf\": 1,\n",
    "    \"Llama-2-70b-chat-hf\": 2,\n",
    "    \"Meta-Llama-3-8B-Instruct\": 3,\n",
    "    \"Meta-Llama-3-70B-Instruct\": 4,\n",
    "    \"Mistral-7B-Instruct-v0.2\": 5,\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\": 6,\n",
    "}\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df[\"model_encoded\"] = df[\"model_name\"].map(model_map)\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float).round(1)\n",
    "df = df[~df[\"eval\"].str.contains(\"None\", na=False)]\n",
    "\n",
    "\n",
    "def count_true_false(string):\n",
    "    string_lower = string.lower()\n",
    "    count_true = string_lower.count(\"true\")\n",
    "    count_false = string_lower.count(\"false\")\n",
    "    return count_true, count_false\n",
    "\n",
    "\n",
    "df[[\"true_count\", \"false_count\"]] = df[\"eval\"].apply(count_true_false).apply(pd.Series)\n",
    "df[\"DRFR\"] = df[\"true_count\"] / (df[\"true_count\"] + df[\"false_count\"])\n",
    "df[\"performance_score\"] = df[\"DRFR\"]\n",
    "df[\"ability\"] = \"IF\"\n",
    "df_IF = df[\n",
    "    [\"initial_prompt\", \"model_name\", \"performance_score\", \"Temperature\", \"ability\"]\n",
    "]\n",
    "\n",
    "#############################ICL############################\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "current_time_str = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "model_weight_sizes = {\n",
    "    \"Llama-2-13b-chat-hf\": 13,\n",
    "    \"Mistral-7B-Instruct-v0.2\": 7,\n",
    "    \"Llama-2-70b-chat-hf\": 70,\n",
    "    \"Meta-Llama-3-8B-Instruct\": 8,\n",
    "    \"Llama-2-7b-chat-hf\": 7,\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\": 56,\n",
    "}\n",
    "\n",
    "folder_path = \"/home/lujun_li/projects/temperature_eval/data/ICL\"\n",
    "output_photo_name = f\"ICL_{current_time_str}.svg\"\n",
    "output_photo_path = os.path.join(folder_path, output_photo_name)\n",
    "all_dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and \"evaluated\" in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.split(\"_\")[2]\n",
    "        df[\"model_name\"] = model_name\n",
    "        df[\"weight_size\"] = model_weight_sizes.get(model_name, None)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float).round(1)\n",
    "df = df[df[\"dataset_name\"] == \"trec\"]\n",
    "df[\"performance_score\"] = df[\"classification_score\"].round(2)\n",
    "df[\"ability\"] = \"ICL\"\n",
    "df_ICL = df[\n",
    "    [\"initial_prompt\", \"model_name\", \"performance_score\", \"Temperature\", \"ability\"]\n",
    "]\n",
    "\n",
    "#############################CR############################\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "current_time_str = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "model_weight_sizes = {\n",
    "    \"Llama-2-13b-chat-hf\": 13,\n",
    "    \"Mistral-7B-Instruct-v0.2\": 7,\n",
    "    \"Llama-2-70b-chat-hf\": 70,\n",
    "    \"Meta-Llama-3-8B-Instruct\": 8,\n",
    "    \"Llama-2-7b-chat-hf\": 7,\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\": 56,\n",
    "}\n",
    "\n",
    "folder_path = \"/home/lujun_li/projects/temperature_eval/data/CR\"\n",
    "\n",
    "all_dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and \"evaluated\" in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.split(\"_\")[2]\n",
    "        df[\"model_name\"] = model_name\n",
    "        all_dfs.append(df)\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float).round(1)\n",
    "\n",
    "\n",
    "def string_to_boolean(s):\n",
    "    s_stripped = s.strip(\"[]\")\n",
    "    return s_stripped.lower() == \"true\"\n",
    "\n",
    "\n",
    "df[\"eval\"] = df[\"eval\"].apply(lambda x: string_to_boolean(x))\n",
    "df[\"eval\"] = df[\"eval\"].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "df[\"performance_score\"] = df[\"eval\"].round(2)\n",
    "df[\"ability\"] = \"CR\"\n",
    "df_CR = df[\n",
    "    [\"initial_prompt\", \"model_name\", \"performance_score\", \"Temperature\", \"ability\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_CR, df_CT, df_ICL, df_IF, df_MT, df_SUM], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = (\n",
    "    \"/home/lujun_li/projects/temperature_eval/data/all_data_for_bert_training.csv\"\n",
    ")\n",
    "\n",
    "df.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_prompt</th>\n",
       "      <th>model_name</th>\n",
       "      <th>performance_score</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>ability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>Meta-Llama-3-70B-Instruct</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>Meta-Llama-3-70B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>Meta-Llama-3-70B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>Meta-Llama-3-70B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction: You are a conscientious assistant...</td>\n",
       "      <td>Meta-Llama-3-70B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83158</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>1.9</td>\n",
       "      <td>SUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83159</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>0.291971</td>\n",
       "      <td>1.9</td>\n",
       "      <td>SUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83160</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>1.9</td>\n",
       "      <td>SUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83161</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.9</td>\n",
       "      <td>SUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83162</th>\n",
       "      <td>\\nRole: You are a writer who is good at summar...</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>0.251969</td>\n",
       "      <td>1.9</td>\n",
       "      <td>SUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83163 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          initial_prompt  \\\n",
       "0      Instruction: You are a conscientious assistant...   \n",
       "1      Instruction: You are a conscientious assistant...   \n",
       "2      Instruction: You are a conscientious assistant...   \n",
       "3      Instruction: You are a conscientious assistant...   \n",
       "4      Instruction: You are a conscientious assistant...   \n",
       "...                                                  ...   \n",
       "83158  \\nRole: You are a writer who is good at summar...   \n",
       "83159  \\nRole: You are a writer who is good at summar...   \n",
       "83160  \\nRole: You are a writer who is good at summar...   \n",
       "83161  \\nRole: You are a writer who is good at summar...   \n",
       "83162  \\nRole: You are a writer who is good at summar...   \n",
       "\n",
       "                      model_name performance_score  Temperature ability  \n",
       "0      Meta-Llama-3-70B-Instruct                 0          0.1      CR  \n",
       "1      Meta-Llama-3-70B-Instruct                 1          0.1      CR  \n",
       "2      Meta-Llama-3-70B-Instruct                 1          0.1      CR  \n",
       "3      Meta-Llama-3-70B-Instruct                 1          0.1      CR  \n",
       "4      Meta-Llama-3-70B-Instruct                 1          0.1      CR  \n",
       "...                          ...               ...          ...     ...  \n",
       "83158   Meta-Llama-3-8B-Instruct          0.238095          1.9     SUM  \n",
       "83159   Meta-Llama-3-8B-Instruct          0.291971          1.9     SUM  \n",
       "83160   Meta-Llama-3-8B-Instruct          0.321429          1.9     SUM  \n",
       "83161   Meta-Llama-3-8B-Instruct          0.285714          1.9     SUM  \n",
       "83162   Meta-Llama-3-8B-Instruct          0.251969          1.9     SUM  \n",
       "\n",
       "[83163 rows x 5 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Convert the 'Rappel' column to a categorical type\n",
    "dataset[\"ability\"] = dataset[\"ability\"].astype(\"category\")\n",
    "\n",
    "# Encode the categorical 'Rappel' column as numerical codes\n",
    "dataset[\"ability\"] = dataset[\"ability\"].cat.codes\n",
    "\n",
    "# Extract the email content and labels into separate variables\n",
    "texts = dataset.initial_prompt.values\n",
    "labels = dataset.ability.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Iterate over each text in the dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Tokenize the text and add special tokens\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Update max_len with the length of the current tokenized input if it's greater than the current max_len\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_len, \u001b[38;5;28mlen\u001b[39m(input_ids))\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2751\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2771\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2772\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2775\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2788\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2791\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3207\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3197\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3199\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3200\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3205\u001b[0m )\n\u001b[0;32m-> 3207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3226\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/site-packages/transformers/tokenization_utils.py:801\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    799\u001b[0m     )\n\u001b[0;32m--> 801\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    805\u001b[0m     first_ids,\n\u001b[1;32m    806\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    821\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    822\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/site-packages/transformers/tokenization_utils.py:768\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 768\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/site-packages/transformers/tokenization_utils.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    649\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[1;32m    652\u001b[0m     ]\n\u001b[1;32m    653\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 654\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[1;32m    657\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/re.py:209\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/causalLLM/lib/python3.10/site-packages/transformers/tokenization_utils.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.<lambda>\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m    648\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    649\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[1;32m    652\u001b[0m     ]\n\u001b[1;32m    653\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 654\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[1;32m    657\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the path to the pre-trained model\n",
    "model = \"/home/lujun_li/models/bert-base-multilingual-uncased\"\n",
    "\n",
    "# Initialize the tokenizer from the pre-trained model, with lowercasing enabled\n",
    "tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n",
    "\n",
    "# Get the maximum token length supported by the tokenizer\n",
    "max_token_length = tokenizer.model_max_length\n",
    "\n",
    "# Initialize a variable to keep track of the maximum length of tokenized input\n",
    "max_len = 0\n",
    "\n",
    "# Iterate over each text in the dataset\n",
    "for text in texts:\n",
    "    # Tokenize the text and add special tokens\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True)\n",
    "    # Update max_len with the length of the current tokenized input if it's greater than the current max_len\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "# Determine the maximum padding length\n",
    "if max_len < max_token_length:\n",
    "    max_padding = max_len\n",
    "else:\n",
    "    max_padding = max_token_length\n",
    "\n",
    "max_padding  # Need to padding to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# `encode_plus` (1) Tokenize the sentence. (2) Prepend the `[CLS]` token to the start. (3) Append the `[SEP]` token to the end.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#  (4) Map tokens to their IDs. (5) Pad or truncate the sentence to `max_length` (6) Create attention masks for [PAD] tokens.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     encoded_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m      8\u001b[0m         text,  \u001b[38;5;66;03m# Sentence to encode.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Add '[CLS]' and '[SEP]'\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_padding,  \u001b[38;5;66;03m# Pad & truncate all sentences.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         pad_to_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Construct attn. masks.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Return pytorch tensors.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(encoded_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     17\u001b[0m     attention_masks\u001b[38;5;241m.\u001b[39mappend(encoded_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "    # `encode_plus` (1) Tokenize the sentence. (2) Prepend the `[CLS]` token to the start. (3) Append the `[SEP]` token to the end.\n",
    "    #  (4) Map tokens to their IDs. (5) Pad or truncate the sentence to `max_length` (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=max_padding,  # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "# Convert the lists into tensors. [Samples, max_length]\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels).to(torch.int64)\n",
    "labels = torch.nn.functional.one_hot(\n",
    "    labels\n",
    ").float()  # Change to Float in order to calcualte Loss [Samples, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,688 training samples\n",
      "2,460 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Set the random seed for reproducibility and shuffle the dataset\n",
    "torch.manual_seed(40)  # Orginally set to 42 for training\n",
    "sample_indices = torch.randperm(len(dataset))\n",
    "test_dataset = torch.utils.data.Subset(dataset, sample_indices)\n",
    "\n",
    "# Calculate the sizes for training and validation datasets\n",
    "train_size = int(0.6 * len(test_dataset))\n",
    "val_size = len(test_dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(test_dataset, [train_size, val_size])\n",
    "\n",
    "# Print the number of samples in the training and validation sets\n",
    "print(\"{:>5,} training samples\".format(train_size))\n",
    "print(\"{:>5,} validation samples\".format(val_size))\n",
    "\n",
    "# Dataset preparation\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  # The training samples.\n",
    "    sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "    batch_size=batch_size,  # Trains with this batch size.\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,  # The validation samples.\n",
    "    sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "    batch_size=batch_size,  # Evaluate with this batch size.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Engine Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/llama/Personal_Directories/srb/binary_classfication/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/llama/Personal_Directories/srb/env_minicpm_vllm/lib64/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model,  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=2,  # The number of output labels--2 for binary classification.\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-6,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "    eps=1e-8,  # args.adam_epsilon  - default is 1e-8.\n",
    ")\n",
    "\n",
    "# Number of training epochs.\n",
    "epochs = 20\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler. (This is important to decrease learning while training)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,  # Default value in run_glue.py\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = np.argmax(labels, axis=1).flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "\n",
      "======== Epoch 7 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "\n",
      "======== Epoch 8 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "\n",
      "======== Epoch 9 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "\n",
      "======== Epoch 10 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "\n",
      "======== Epoch 11 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "\n",
      "======== Epoch 12 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "\n",
      "======== Epoch 13 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "\n",
      "======== Epoch 14 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "\n",
      "======== Epoch 15 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "\n",
      "======== Epoch 16 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "\n",
      "======== Epoch 17 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "\n",
      "======== Epoch 18 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "\n",
      "======== Epoch 19 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "\n",
      "======== Epoch 20 / 20 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:11:34 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 43  # Set the seed value for reproducibility originally 42\n",
    "random.seed(seed_val)  # Set the seed for the Python random module\n",
    "np.random.seed(seed_val)  # Set the seed for NumPy's random number generator\n",
    "torch.manual_seed(seed_val)  # Set the seed for PyTorch's random number generator\n",
    "torch.cuda.manual_seed_all(\n",
    "    seed_val\n",
    ")  # Set the seed for all CUDA (GPU) operations in PyTorch\n",
    "\n",
    "\n",
    "training_stats = []  # Initialize an empty list to store training statistics\n",
    "total_t0 = time.time()  # Measure the total training time for the whole run.\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\n",
    "    print(\"Training...\")\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        loss = output.loss  # Loss is calculate by forwarding\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    best_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "        loss = output.loss\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        # Move logits and labels to CPU if we are using GPU\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    if avg_val_accuracy > best_eval_accuracy:\n",
    "        torch.save(model, f\"bert_model_target_\")\n",
    "        best_eval_accuracy = avg_val_accuracy\n",
    "    torch.save(model, f\"bert_model_target_{epoch_i}\")\n",
    "    # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    # print(\"  Validation took: {:}\".format(validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            \"epoch\": epoch_i + 1,\n",
    "            \"Training Loss\": avg_train_loss,\n",
    "            \"Valid. Loss\": avg_val_loss,\n",
    "            \"Valid. Accur.\": avg_val_accuracy,\n",
    "            \"Training Time\": training_time,\n",
    "            \"Validation Time\": validation_time,\n",
    "        }\n",
    "    )\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on single email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "\n",
    "From: \"Elton Silva\" <Elton.Silva@post.lu>\n",
    "Sent: mercredi 6 octobre 2021 21:49:23\n",
    "To: \"Foyer Assurances\" <sinistres-habitation@foyer.lu>\n",
    "Cc: \"Sandra Dias\" <sandra.dias@foyer.lu>\n",
    "Subject: Re: Groupe Foyer - Sinistre 3158166 - Contrat 0020586108 00001 - MOZAIK\n",
    "Soyez vigilant.e !\n",
    "Ce message est envoyÃ© par un correspondant externe.\n",
    "Veuillez ne pas cliquer sur des liens ou des piÃ¨ces jointes sauf sice message est sollicitÃ© et que vous avezacquis l'assurance qu'il\n",
    "provient d'une source sÃ»re.\n",
    "Un doute sur son origine ? PrÃ©venezla hotline.\n",
    "Bonjour,\n",
    "Nâ€™ayant toujours pas obtenude remboursement, je me permets de relancer la demande annexe.\n",
    "Cordialement\n",
    "EltonSilva\n",
    "Responsable Jumper&Planification\n",
    "ConseillerCommercial\n",
    "RÃ©seauVente\n",
    "Tel. :+352 2462-1\n",
    "elton.silva@post.lu\n",
    "Telecom\n",
    "1, rue Emile Bian\n",
    "L-2996 Luxembourg\n",
    "[Link Deleted] / Autorisationd'Ã©tablissement nÂ° 00116288 / 55\n",
    "POSTLuxembourgest acteur dudÃ©veloppement durable\n",
    "Ce message et toutes les piÃ¨ces jointes (ci-aprÃ¨s le \"message\") sont Ã©tablis Ã  l'intentionexclusive de ses destinataires et sont confidentiels.\n",
    "Sivous recevezce message par erreur, mercide le dÃ©truire et d'enavertir immÃ©diatement l'expÃ©diteur.\n",
    "Toute utilisationde ce message nonconforme Ã  sa destination, toute diffusionoutoute publication, totale oupartielle, est interdite, saufautorisation\n",
    "expresse.\n",
    "Internet ne permettant pas d'assurer l'intÃ©gritÃ© de ce message, POSTLuxembourgdÃ©cline toute responsabilitÃ© autitre de ce message, dans l'hypothÃ¨se\n",
    "oÃ¹ilaurait Ã©tÃ© modifiÃ©.\n",
    "Le 24 sept. 2021 Ã  14:23, EltonSilva <Elton.Silva@post.lu> a Ã©crit :\n",
    "â€‹ Bonjour,\n",
    "Veuilleztrouver enannexe, la facture finale.\n",
    "Cordialement\n",
    "EltonSilva\n",
    "Responsable Jumper &Planification\n",
    "Conseiller Commercial\n",
    "RÃ©seauVente\n",
    "Tel. :+352 2462-1\n",
    "elton.silva@post.lu\n",
    "Telecom\n",
    "1, rue Emile Bian\n",
    "L-2996 Luxembourg\n",
    "[Link Deleted] / Autorisationd'Ã©tablissement nÂ° 00116288/55\n",
    "POST Luxembourg est acteur du dÃ©veloppement durable\n",
    "Ce message et toutes les piÃ¨ces jointes (ci-aprÃ¨s le \"message\") sont Ã©tablis Ã  l'intentionexclusive de ses destinataires et sont confidentiels.\n",
    "Sivous recevezce message par erreur, mercide le dÃ©truire et d'enavertir immÃ©diatement l'expÃ©diteur.\n",
    "\fToute utilisationde ce message nonconforme Ã  sa destination, toute diffusionoutoute publication, totale oupartielle, est interdite, sauf\n",
    "autorisationexpresse.\n",
    "Internet ne permettant pas d'assurer l'intÃ©gritÃ© de ce message, POSTLuxembourgdÃ©cline toute responsabilitÃ© autitre de ce message,\n",
    "dans l'hypothÃ¨se oÃ¹ilaurait Ã©tÃ© modifiÃ©.\n",
    "Le 21 sept. 2021 Ã  09:52, Foyer Assurances <sinistres-habitation@foyer.lu> a Ã©crit :\n",
    "ATTENTION : Ce mail provient de l'extÃ©rieur de Post. Ne cliquez pas sur les liens ou n'ouvrez pas les piÃ¨ces jointes Ã  moins de connaitre l'expÃ©diteur et d'Ãªtre sÃ»r que\n",
    "le contenu est inoffensif. En cas de doute sur son origine ou si vous pensez qu'il est suspect, nous vous prions de rapporter cet Ã©vÃ¨nement par email Ã \n",
    "cybersos@post.lu.\n",
    "Madame, Monsieur,\n",
    "Vous trouvezci-dessous uncourrier sous format PDF, le cas Ã©chÃ©ant avec ses annexes, vous concernant.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "texts = [email]\n",
    "for text in texts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=max_padding,  # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor([[0, 1]]).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3547319/4100754386.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('/home/llama/Personal_Directories/srb/binary_classfication/fine-tuning-bert/models_finetuning_second/bert_model_target_19')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load(\n",
    "    \"/home/llama/Personal_Directories/srb/binary_classfication/fine-tuning-bert/models_finetuning_second/bert_model_target_19\"\n",
    ")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded and set to evaluation mode.\")\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "total_eval_accuracy = 0\n",
    "best_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "# Evaluate data for one epoch\n",
    "b_input_ids = input_ids.to(device)\n",
    "b_input_mask = attention_masks.to(device)\n",
    "b_labels = labels.to(device)\n",
    "# Tell pytorch not to bother with constructing the compute graph during\n",
    "# the forward pass, since this is only needed for backprop (training).\n",
    "with torch.no_grad():\n",
    "    output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "logits = output.logits\n",
    "logits = logits.detach().cpu().numpy()\n",
    "label_ids = b_labels.to(\"cpu\").numpy()\n",
    "pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "labels_flat = np.argmax(label_ids, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results is correct but when we change something, prediction changed but we cannot explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_flat : [1]\n",
      "labels_flat : [1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"pred_flat : {pred_flat}\")\n",
    "print(f\"labels_flat : {labels_flat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing On All datasets for summarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3547319/2751142205.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(folder_path+\"/\"+model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 0.89\n",
      "Global Accuracy: 0.93\n",
      "Global Accuracy: 0.94\n",
      "Global Accuracy: 0.96\n",
      "Global Accuracy: 0.97\n",
      "Global Accuracy: 0.95\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.97\n",
      "Global Accuracy: 0.97\n",
      "Global Accuracy: 0.97\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n",
      "Global Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    # List all files in the given folder\n",
    "    files = os.listdir(folder_path)\n",
    "    return files\n",
    "\n",
    "\n",
    "# We use all data for seeting the global results\n",
    "all_dataloader = DataLoader(\n",
    "    test_dataset,  # test_dataset(whole dataset), train_dataset (0.6 whole dataset),  val_dataset (rest),\n",
    "    sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "    batch_size=batch_size,  # Trains with this batch size.\n",
    ")\n",
    "\n",
    "folder_path = \"models_finetuning_second/\"\n",
    "models = list_files_in_folder(folder_path)\n",
    "accuracy_list = []\n",
    "epochs_list = []\n",
    "for model_path in models:\n",
    "    model = torch.load(folder_path + \"/\" + model_path)\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    best_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in all_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "        loss = output.loss\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        # Move logits and labels to CPU if we are using GPU\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(all_dataloader)\n",
    "    print(\"Global Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    accuracy_list.append(avg_val_accuracy)\n",
    "    epochs_list.append(model_path.split(\"_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_loss = []\n",
    "for training_stat in training_stats:\n",
    "    training_loss.append(float(training_stat[\"Training Loss\"]))\n",
    "\n",
    "epochs = [int(epoch) for epoch in epochs_list]\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot accuracy on the first y-axis\n",
    "ax1.plot(epochs, accuracy_list, \"b-o\", label=\"Accuracy\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"b\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"b\")\n",
    "\n",
    "# Create a second y-axis sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, training_loss, \"r-x\", label=\"Training Loss\")\n",
    "ax2.set_ylabel(\"Training Loss\", color=\"r\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"r\")\n",
    "\n",
    "# Adding title\n",
    "plt.title(\"Model Accuracy and Training Loss over Epochs\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on one single checkpoint model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this block to test different checkpoint during training to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    # List all files in the given folder\n",
    "    files = os.listdir(folder_path)\n",
    "    return files\n",
    "\n",
    "\n",
    "folder_path = \"models_finetuning_second/\"\n",
    "models = list_files_in_folder(folder_path)\n",
    "accuracy_list = []\n",
    "epochs_list = []\n",
    "model = torch.load(folder_path + \"/\" + \"bert_model_target_19\")\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# Tracking variables\n",
    "total_eval_accuracy = 0\n",
    "best_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "# Evaluate data for one epoch\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "\n",
    "    logits = output.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.extend(np.argmax(logits, axis=1))\n",
    "    true_labels.extend(np.argmax(label_ids, axis=1))\n",
    "\n",
    "report = classification_report(\n",
    "    true_labels, predictions, target_names=[\"Reminder\", \"Not Reminder\"]\n",
    ")\n",
    "# Report the final accuracy for this validation run.\n",
    "avg_val_accuracy = total_eval_accuracy / len(all_dataloader)\n",
    "print(\"Global Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "accuracy_list.append(avg_val_accuracy)\n",
    "epochs_list.append(model_path.split(\"_\")[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
