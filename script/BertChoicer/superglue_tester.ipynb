{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先做validaiton set 选出一部分 300个， 然后再送到super GLUE里面进行测试，不同温度下的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/vllm_env_lujun/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: boolq\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 9427\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3270\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3245\n",
      "    })\n",
      "})\n",
      "{'question': 'do iran and afghanistan speak the same language', 'passage': 'Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.', 'idx': 0, 'label': 1}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: cb\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "})\n",
      "{'premise': 'It was a complex language. Not written down but handed down. One might say it was peeled down.', 'hypothesis': 'the language was peeled down', 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1, 2}\n",
      "Task: copa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n",
      "{'premise': 'My body cast a shadow over the grass.', 'choice1': 'The sun was rising.', 'choice2': 'The grass was cut.', 'question': 'cause', 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: multirc\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['paragraph', 'question', 'answer', 'idx', 'label'],\n",
      "        num_rows: 27243\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['paragraph', 'question', 'answer', 'idx', 'label'],\n",
      "        num_rows: 4848\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['paragraph', 'question', 'answer', 'idx', 'label'],\n",
      "        num_rows: 9693\n",
      "    })\n",
      "})\n",
      "{'paragraph': 'While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department\\'s counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government\\'s legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on  Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President\\'s life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed  Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding  Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he\\'d help us get  Bin Laden and deal with another issue or two.\" The U.S. effort continued. ', 'question': 'What did the high-level effort to persuade Pakistan include?', 'answer': 'Children, Gerd, or Dorian Popa', 'idx': {'paragraph': 0, 'question': 0, 'answer': 0}, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: record\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['passage', 'query', 'entities', 'entity_spans', 'answers', 'idx'],\n",
      "        num_rows: 100730\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['passage', 'query', 'entities', 'entity_spans', 'answers', 'idx'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['passage', 'query', 'entities', 'entity_spans', 'answers', 'idx'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "{'passage': \"The harrowing stories of women and children locked up for so-called 'moral crimes' in Afghanistan's notorious female prison have been revealed after cameras were allowed inside. Mariam has been in Badam Bagh prison for three months after she shot a man who just raped her at gunpoint and then turned the weapon on herself - but she has yet to been charged. Nuria has eight months left to serve of her sentence for trying to divorce her husband. She gave birth in prison to her son and they share a cell together. Scroll down for video Nuria was jailed for trying to divorce her husband. Her son is one of 62 children living at Badam Bagh prison\\n@highlight\\nMost of the 202 Badam Bagh inmates are jailed for so-called 'moral crimes'\\n@highlight\\nCrimes include leaving their husbands or refusing an arrange marriage\\n@highlight\\n62 children live there and share cells with their mothers and five others\", 'query': 'The baby she gave birth to is her husbands and he has even offered to have the courts set her free if she returns, but @placeholder has refused.', 'entities': ['Badam Bagh', 'Nuria', 'Mariam', 'Afghanistan'], 'entity_spans': {'text': ['Afghanistan', 'Mariam', 'Badam Bagh', 'Nuria', 'Nuria', 'Badam Bagh', 'Badam Bagh'], 'start': [86, 178, 197, 357, 535, 627, 672], 'end': [97, 184, 207, 362, 540, 637, 682]}, 'answers': ['Nuria'], 'idx': {'passage': 0, 'query': 0}}\n",
      "No 'label' field in the test set.\n",
      "Task: rte\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 2490\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 277\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "{'premise': 'No Weapons of Mass Destruction Found in Iraq Yet.', 'hypothesis': 'Weapons of Mass Destruction Found in Iraq.', 'idx': 0, 'label': 1}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: wic\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'sentence1', 'sentence2', 'start1', 'start2', 'end1', 'end2', 'idx', 'label'],\n",
      "        num_rows: 5428\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['word', 'sentence1', 'sentence2', 'start1', 'start2', 'end1', 'end2', 'idx', 'label'],\n",
      "        num_rows: 638\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'sentence1', 'sentence2', 'start1', 'start2', 'end1', 'end2', 'idx', 'label'],\n",
      "        num_rows: 1400\n",
      "    })\n",
      "})\n",
      "{'word': 'place', 'sentence1': 'Do you want to come over to my place later?', 'sentence2': 'A political system with no place for the less prominent groups.', 'start1': 31, 'start2': 27, 'end1': 36, 'end2': 32, 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: wsc\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 554\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 146\n",
      "    })\n",
      "})\n",
      "{'text': 'Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical.', 'span1_index': 0, 'span2_index': 13, 'span1_text': 'Mark', 'span2_text': 'He', 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "\n",
    "\n",
    "tasks = [\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\"]\n",
    "datasets = {task: load_dataset(\"super_glue\", task) for task in tasks}\n",
    "\n",
    "server_url = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "dataset_label = \"test\"\n",
    "\n",
    "if dataset_label == \"test\":\n",
    "    num_examples = None  # Number of examples to test per task\n",
    "else:\n",
    "    num_examples = 300\n",
    "output_name = f\"Llama-3.2-1B-Instruct_{dataset_label}\"\n",
    "\n",
    "for task, dataset in datasets.items():\n",
    "    print(f\"Task: {task}\")\n",
    "\n",
    "    # Show the structure of the dataset for the current task\n",
    "    print(dataset)\n",
    "    print(dataset[\"train\"][0])\n",
    "    if \"label\" in dataset[\"validation\"][0]:  # Check if 'label' key exists\n",
    "        # Get all unique labels from the test set\n",
    "        labels = set(example[\"label\"] for example in dataset[\"validation\"])\n",
    "        print(f\"Unique labels in the test set: {labels}\")\n",
    "    else:\n",
    "        print(\"No 'label' field in the test set.\")\n",
    "\n",
    "options = {  # Configuration options for model generation\n",
    "    \"temperature\": 1.0,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"top_p\": 0.9,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "    \"seed\": 47,  # Default\n",
    "    \"n\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_text_with_vllm(\n",
    "    prompt,\n",
    "    model_name=\"/home/snt/llm_models/Llama-3.2-1B-Instruct\",\n",
    "    options={},\n",
    "    server_url=\"http://0.0.0.0:8000/v1/chat/completions\",\n",
    "):\n",
    "    \"\"\"Generate text using the specified language model.\"\"\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    }\n",
    "    payload.update(options)\n",
    "    response = requests.post(server_url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoolQ (Boolean Questions)\n",
    "def format_boolq_prompt(example):\n",
    "    base_instruction = (\n",
    "        \"Answer the following question based on the passage. Type 'True' or 'False'.\\n\"\n",
    "    )\n",
    "    return f\"{base_instruction}Question: {example['question']}\\nPassage: {example['passage']}\\n:\"\n",
    "\n",
    "\n",
    "# CB (CommitmentBank)\n",
    "def format_cb_prompt(example):\n",
    "    base_instruction = \"Answer the following question based on the Premise and Hypothesis. Type 'entailment', 'contradiction', or 'neutral'.\\n\"\n",
    "    return f\"{base_instruction}Premise: {example['premise']}\\nHypothesis: {example['hypothesis']}\\n Does the premise entail the hypothesis? (entailment, contradiction, or neutral):\"\n",
    "\n",
    "\n",
    "# COPA (Choice of Plausible Alternatives)\n",
    "def format_copa_prompt(example):\n",
    "    base_instruction = \"\"\"Answer the following question based on the Premise and Question. Type 'choice 1' or 'choice 2'.\\n If the question is \"What was the cause?\" select the option most likely to explain why the premise happened. If the question is \"What happened as a result?\" select the option most likely to occur as a result of the premise.\"\"\"\n",
    "    return f\"{base_instruction}Premise: {example['premise']}\\nQuestion: {example['question']}\\nChoice 1: {example['choice1']}\\nChoice 2: {example['choice2']}\\n Which choice is more plausible? \"\n",
    "\n",
    "\n",
    "# MultiRC (Multi-Sentence Reading Comprehension)\n",
    "def format_multirc_prompt(example):\n",
    "    base_instruction = \"Based on the provided (Paragraph, Question, Answer) pair, answer the question with 'True' or 'False'.\\n\"\n",
    "    return f\"{base_instruction}Paragraph: {example['paragraph']}\\nQuestion: {example['question']}\\nAnswer: {example['answer']}\\nIs the answer correct? (True/False):\"\n",
    "\n",
    "\n",
    "# ReCoRD (Reading Comprehension with Commonsense Reasoning)\n",
    "def format_record_prompt(example):\n",
    "    base_instruction = \"Based on the provided passage and entities, answer the question with the correct entity.\\n\"\n",
    "    return f\"{base_instruction}Passage: {example['passage']}\\nQuery: {example['query']}\\nEntities: {', '.join(example['entities'])}\\nWhich entity best fills in the blank?\"\n",
    "\n",
    "\n",
    "# RTE (Recognizing Textual Entailment)\n",
    "def format_rte_prompt(example):\n",
    "    base_instruction = \"Answer the following question based on the Premise and Hypothesis. Type 'Yes' or 'No'.\\n\"\n",
    "    return f\"{base_instruction}Premise: {example['premise']}\\nHypothesis: {example['hypothesis']}\\nDoes the premise entail the hypothesis? (Yes or No):\"\n",
    "\n",
    "\n",
    "# WiC (Word-in-Context)\n",
    "def format_wic_prompt(data):\n",
    "    # Extract relevant information from the input data\n",
    "    word = data[\"word\"]\n",
    "    sentence1 = data[\"sentence1\"]\n",
    "    sentence2 = data[\"sentence2\"]\n",
    "    start1, end1 = data[\"start1\"], data[\"end1\"]\n",
    "    start2, end2 = data[\"start2\"], data[\"end2\"]\n",
    "\n",
    "    # Extract the highlighted word in both sentences\n",
    "    sentence1_highlighted = sentence1[start1 : end1 + 1]\n",
    "    sentence2_highlighted = sentence2[start2 : end2 + 1]\n",
    "\n",
    "    # Generate the prompt text\n",
    "    prompt = f\"\"\"\n",
    "    Task: Determine whether the word \"{word}\" is used with the same meaning in both sentences below.\n",
    "\n",
    "    Sentence 1: \"{sentence1}\"  \n",
    "    Sentence 2: \"{sentence2}\"  \n",
    "\n",
    "    The word appears in Sentence 1 as: \"{sentence1_highlighted}\"  \n",
    "    The word appears in Sentence 2 as: \"{sentence2_highlighted}\"  \n",
    "\n",
    "    Question: Is the word \"{word}\" used with the same meaning in both sentences?  Type (Yes, No)\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# WSC (Winograd Schema Challenge)\n",
    "def format_wsc_prompt(data):\n",
    "    base_instruction = \"Based on the provided text and pronoun, answer the question with the correct referent.\\n\"\n",
    "    text = data[\"text\"]\n",
    "    span1_text = data[\"span1_text\"]\n",
    "    span2_text = data[\"span2_text\"]\n",
    "    span1_index = data[\"span1_index\"]\n",
    "    span2_index = data[\"span2_index\"]\n",
    "    prompt = f\"\"\"\n",
    "    {base_instruction}\n",
    "    Text:  \n",
    "    {text}\n",
    "    \n",
    "    The first mention of the entity is: \"{span1_text}\" at index {span1_index}\n",
    "    The second mention of the entity is: \"{span2_text}\" at index {span2_index}\n",
    "    \n",
    "    Task: Identify whether the second mention of the entity refers to the same entity as the first mention.\n",
    "    \n",
    "    Question: Does the second mention of the entity (\"{span2_text}\") refer to the same entity as the first mention (\"{span1_text}\") in the text?  Type (Yes, No)\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing task: boolq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing task: cb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing task: copa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing task: multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Examples for multirc:   4%|▍         | 374/9693 [09:58<39:49,  3.90it/s]   "
     ]
    }
   ],
   "source": [
    "# import openai\n",
    "\n",
    "# file_path = \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/.vscode/api_key.txt\"\n",
    "\n",
    "# # Read the API key from the file\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     openai.api_key = file.read().strip()\n",
    "\n",
    "\n",
    "# def call_openai_api(prompt):\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ],\n",
    "#         temperature=0.7,\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def evaluate_prediction_with_conversion(task, example, prediction):\n",
    "    # Helper functions for conversion\n",
    "    def bool_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"true\"\n",
    "        if value == \"true\":\n",
    "            return 1\n",
    "        elif value == \"false\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"true\" but does not contain \"false\"\n",
    "        elif \"true\" in value and \"false\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"false\" but does not contain \"true\"\n",
    "        elif \"false\" in value and \"true\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"true\" and \"false\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def yes_no_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"yes\"\n",
    "        if value == \"yes\":\n",
    "            return 1\n",
    "        elif value == \"no\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"yes\" but does not contain \"no\"\n",
    "        elif \"yes\" in value and \"no\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"no\" but does not contain \"yes\"\n",
    "        elif \"no\" in value and \"yes\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"yes\" and \"no\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def entailment_to_label(value):\n",
    "        # Define the mapping for entailment, contradiction, and neutral\n",
    "        mapping = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input matches exactly one of the keys in the mapping\n",
    "        if value in mapping:\n",
    "            return mapping[value]\n",
    "\n",
    "        # Check if the input contains one of the keys without ambiguity\n",
    "        elif (\n",
    "            \"entailment\" in value\n",
    "            and \"contradiction\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"entailment\"]\n",
    "        elif (\n",
    "            \"contradiction\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"contradiction\"]\n",
    "        elif (\n",
    "            \"neutral\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"contradiction\" not in value\n",
    "        ):\n",
    "            return mapping[\"neutral\"]\n",
    "\n",
    "        # If the input is ambiguous or invalid, return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def choice_to_binary(value):\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input contains 'choice 1' and does not contain 'choice 2'\n",
    "        if \"choice 1\" in value and \"choice 2\" not in value:\n",
    "            return 0\n",
    "        elif \"choice 2\" in value:\n",
    "            return 1\n",
    "\n",
    "        # If the input does not match any of the conditions, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Task-specific evaluation\n",
    "    if task == \"boolq\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"cb\":\n",
    "        # Convert prediction (entailment/contradiction/neutral) to label (0/1/2)\n",
    "        return entailment_to_label(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"copa\":\n",
    "        # Convert prediction (choice1/choice2) to binary and compare with label (0/1)\n",
    "        return choice_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"multirc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"record\":\n",
    "        # Direct comparison of prediction with the correct entity\n",
    "        processed_answers = [answer.strip().lower() for answer in example[\"answers\"]]\n",
    "        return prediction.strip().lower() in processed_answers\n",
    "\n",
    "    elif task == \"rte\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (1/0)\n",
    "        return yes_no_to_binary(prediction) == (1 - int(example[\"label\"]))\n",
    "\n",
    "    elif task == \"wic\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"wsc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    # Default case: unknown task\n",
    "    return False\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for task, dataset in datasets.items():\n",
    "    print(f\"Testing task: {task}\")\n",
    "    if num_examples is not None:\n",
    "        test_data = dataset[dataset_label][0:num_examples]\n",
    "    else:\n",
    "        test_data = dataset[dataset_label][0 : len(dataset[dataset_label])]\n",
    "\n",
    "    test_data = [\n",
    "        {key: test_data[key][i] for key in test_data}\n",
    "        for i in range(len(test_data[list(test_data.keys())[0]]))\n",
    "    ]\n",
    "\n",
    "    for example in tqdm(test_data, desc=f\"Examples for {task}\", leave=False):\n",
    "        if task == \"boolq\":\n",
    "            prompt = format_boolq_prompt(example)\n",
    "        elif task == \"cb\":\n",
    "            prompt = format_cb_prompt(example)\n",
    "        elif task == \"copa\":\n",
    "            prompt = format_copa_prompt(example)\n",
    "        elif task == \"multirc\":\n",
    "            prompt = format_multirc_prompt(example)\n",
    "        elif task == \"record\":\n",
    "            prompt = format_record_prompt(example)\n",
    "        elif task == \"rte\":\n",
    "            prompt = format_rte_prompt(example)\n",
    "        elif task == \"wic\":\n",
    "            prompt = format_wic_prompt(example)\n",
    "        elif task == \"wsc\":\n",
    "            prompt = format_wsc_prompt(example)\n",
    "\n",
    "        options[\"temperature\"] = 0.1\n",
    "        prediction = generate_text_with_vllm(\n",
    "            prompt,\n",
    "            options=options,\n",
    "            server_url=server_url,\n",
    "        )\n",
    "\n",
    "        if dataset_label != \"test\":\n",
    "            label = evaluate_prediction_with_conversion(task, example, prediction)\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        result_df = pd.DataFrame(\n",
    "            {\n",
    "                \"task\": task,\n",
    "                \"example\": json.dumps(example),\n",
    "                \"prompt\": prompt,\n",
    "                \"prediction\": prediction,\n",
    "                \"label\": label,\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "        result_df.to_json(\n",
    "            f\"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/superGlue/results_test_{output_name}.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "            mode=\"a\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_results = pd.read_json(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/superGlue/results_test_Llama-3.2-1B-Instruct_test.jsonl\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2196, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folders:   0%|          | 0/5 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 内层 tqdm 处理每个文件夹中的文件\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     14\u001b[0m     files_per_folder,\n\u001b[1;32m     15\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     19\u001b[0m ):\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 模拟文件处理的延迟\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 模拟多个文件夹和每个文件夹中的文件\n",
    "folders = [f\"Folder_{i}\" for i in range(5)]\n",
    "files_per_folder = [f\"File_{j}\" for j in range(10)]\n",
    "\n",
    "# 外层 tqdm 处理文件夹\n",
    "for folder in tqdm(folders, desc=\"Processing Folders\", position=0):\n",
    "    time.sleep(0.5)  # 模拟文件夹处理的延迟\n",
    "    # 内层 tqdm 处理每个文件夹中的文件\n",
    "    for file in tqdm(\n",
    "        files_per_folder,\n",
    "        desc=f\"Processing Files in {folder}\",\n",
    "        position=1,\n",
    "        leave=False,\n",
    "        mininterval=0.05,\n",
    "    ):\n",
    "        time.sleep(0.5)  # 模拟文件处理的延迟\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env_lujun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
