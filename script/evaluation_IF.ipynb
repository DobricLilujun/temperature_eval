{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF evluation beased on Inforbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m eval_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0125\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/openai/_client.py:104\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    102\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Read data using\n",
    "SYS_MSG = \"Based on the provided Input (if any) and Generated Text, answer the ensuing Questions with either a YES or NO choice. Your selection should be based on your judgment as well as the following rules:\\n\\n- YES: Select 'YES' if the generated text entirely fulfills the condition specified in the question. However, note that even minor inaccuracies exclude the text from receiving a 'YES' rating. As an illustration. consider a question that asks. \\\"Does each sentence in the generated text use a second person?” If even one sentence does not use the second person, the answer should NOT be 'YES'. To qualify for a 'YES' rating, the generated text must be entirely accurate and relevant to the question\\n\\n- NO: Opt for 'NO' if the generated text fails to meet the question's requirements or provides no information that could be utilized to answer the question. For instance, if the question asks. \\\"Is the second sentence in the generated text a compound sentence?\\\" and the generated text only has one sentence. it offers no relevant information to answer the question. Consequently, the answer should be 'NO'.'''\"\n",
    "\n",
    "foler_path = \"Paper Experiment Results/New_filtered/IF\"\n",
    "file_name = \"exp_result_Llama-2-13b-chat-hf_20240503202441.csv\"\n",
    "output_file_name = file_name.replace(\".csv\", \"_evaluated.csv\")\n",
    "input_path = os.path.join(foler_path, file_name)\n",
    "output_path = os.path.join(foler_path, output_file_name)\n",
    "_data = pd.read_csv(input_path)\n",
    "eval_model = \"gpt-3.5-turbo-0125\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_response(response, prompt):\n",
    "    return response[len(prompt) :]\n",
    "\n",
    "\n",
    "def match_prompt_column(model):\n",
    "    if \"Llama-2\" in model:\n",
    "        return \"llama2_chat_initial_prompt\"\n",
    "    elif \"Mixtral\" in model or \"Mistral\" in model:\n",
    "        return \"mixtral_instruct_initial_prompt\"\n",
    "    elif \"Llama-3\" in model:\n",
    "        return \"llama3_chat_initial_prompt\"\n",
    "    else:\n",
    "        print(\"The model name didn't match anything, please check!!!!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_pure_response(row):\n",
    "    model = row[\"model\"]\n",
    "    prompt_column = match_prompt_column(model)\n",
    "    response = row[\"generated_response\"]\n",
    "    prompt = row[prompt_column]\n",
    "    return extract_final_response(response=response, prompt=prompt)\n",
    "\n",
    "\n",
    "def process_string(input_string):\n",
    "    processed_string = input_string.strip(\"[]\").replace(\"\\\\\", \"\")\n",
    "    questions = processed_string.split(\"\\n\")\n",
    "    questions = [q.strip(\"'\") for q in questions]\n",
    "    return questions\n",
    "\n",
    "\n",
    "_data[\"pure_response\"] = _data.apply(extract_pure_response, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 718/3500 [15:59:09<61:56:23, 80.15s/it]\n",
      "1it [00:04,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:08,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:10,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:14,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:20,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:24,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:28,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:31,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:35,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:39,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_times = 0\n",
    "times = 0\n",
    "pbar = tqdm(total=len(_data))\n",
    "_data = rows_in_difference_list\n",
    "for index, entry in tqdm(_data.iterrows()):\n",
    "    updated_row = entry.copy()\n",
    "    input_task = entry[\"input\"]\n",
    "    output = entry[\"pure_response\"]\n",
    "    if output is None:  # skip if result hasn't been generated\n",
    "        continue\n",
    "    message = []\n",
    "    answer = \"\"\n",
    "    for question in process_string(entry[\"decomposed_questions\"]):\n",
    "        if len(message) == 0:\n",
    "            if input_task:\n",
    "                content = f'{SYS_MSG}\\n\\nInput:\\n\"{input_task}\"\\n\\nGenerated Text:\\n\"{output}\"\\n\\nQuestion:\\n{question}\\n'\n",
    "            else:\n",
    "                content = f'{SYS_MSG}\\n\\nGenerated Text:\\n\"{output}\"\\n\\nQuestion:\\n{question}\\n'\n",
    "        else:\n",
    "            content = f\"{question}\\n\"\n",
    "        message.append({\"role\": \"user\", \"content\": content})\n",
    "        # create a chat completion\n",
    "        success = False\n",
    "        early_stop = False\n",
    "        while not success:\n",
    "            try:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=eval_model,\n",
    "                    messages=message,\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "                generation = completion.choices[0].message.content\n",
    "                message.append({\"role\": \"assistant\", \"content\": generation})\n",
    "                # check if generation is yes or no\n",
    "                if generation.lower().startswith(\n",
    "                    \"yes\"\n",
    "                ) or generation.lower().startswith(\"no\"):\n",
    "                    if generation.lower().startswith(\"yes\"):\n",
    "                        answer += \"Yes\\n\"\n",
    "                    else:\n",
    "                        answer += \"No\\n\"\n",
    "                else:\n",
    "                    if \"YES\" in generation and \"NO\" not in generation:\n",
    "                        answer += \"Yes\\n\"\n",
    "                    elif \"YES\" not in generation and \"NO\" in generation:\n",
    "                        answer += \"No\\n\"\n",
    "                    else:\n",
    "                        for msg in message:\n",
    "                            print(msg[\"content\"])\n",
    "                        print(\"NO YES or NO answer!\" + generation)\n",
    "                        answer += \"None\\n\"\n",
    "                        early_stop = True\n",
    "                        break\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(\"ERROR!\")\n",
    "                print(e)\n",
    "                print(\"Retry!\")\n",
    "                time.sleep(20)\n",
    "\n",
    "        # when no answer occurs, break the loop and continue to next instance\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    answer = answer[:-1]\n",
    "    # save eval results as List[bool]\n",
    "    bool_results = []\n",
    "    for i in answer.split(\"\\n\"):\n",
    "        if i == \"Yes\":\n",
    "            bool_results.append(True)\n",
    "        elif i == \"No\":\n",
    "            bool_results.append(False)\n",
    "        else:\n",
    "            bool_results.append(None)\n",
    "\n",
    "    updated_row[\"eval\"] = bool_results\n",
    "    updated_row[\"messages_openai\"] = message\n",
    "    test_times += 1\n",
    "    # if test_times > 5:\n",
    "    #     break\n",
    "    updated_dataframe = pd.DataFrame([updated_row])\n",
    "    pbar.update(1)\n",
    "    if not os.path.exists(output_path):\n",
    "        updated_dataframe.to_csv(output_path, index=False, mode=\"w\", header=True)\n",
    "    else:\n",
    "        print(\"append\")\n",
    "        updated_dataframe.to_csv(output_path, index=False, mode=\"a\", header=False)\n",
    "    times = times + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
