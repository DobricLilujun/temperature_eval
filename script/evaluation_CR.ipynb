{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Reasoning Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{CRASS}: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models\n",
    "\n",
    "\n",
    "The CRASS benchmarks is a new test question set for the evaluation of the performance of large language models (LLMs) by utilizing so called questionized counterfactual conditionals (QCC).\n",
    "\n",
    "\n",
    "In this study, this CR tasks are implemented by the following steps:\n",
    "\n",
    "1. Collect all questions from the dataset and then formulate prompt tempelate with specific few-shot learning prompts.\n",
    "\n",
    "| prompt_question                                                                                                                          | Answer | Evaluation_prompt | Evaluation_results |\n",
    "|--------------------------------------------------------------------------------------------------------------------------------------------|--------|------------------|--------------------|\n",
    "|You are a conscientious assistant, and you must answer my questions exactly as my questions require. <</SYS>> Please reference the following three examples (including premise, query, possible answers, and correct answer) to answer the final query. Premise: A feather falls from a skyscraper.  Query: What would have happened if a computer had fallen from the skyscraper?  Possible answers: The computer would have remained intact. That is not possible. The computer would have been crushed.  Correct answer: The computer would have been crushed. Premise: A lightning hits a tree.  Query: What would have happened if a marble would have hit the tree?  Possible answers: It would have burned down. Nothing special would have happened. The tree would have kissed the lightning.  Correct answer: Nothing special would have happened. Premise: A man drinks a beer.  Query: What would have happened if the man had drunk a rainbow?  Possible answers: It would have been tasty. It would have been awful. That is not possible.  Correct answer: That is not possible My premise: A woman finds the keys of a man. My query: What would have happened if the woman had lost the keys? The Possible answers: The man would have been angry with the woman.The man would not have been angry with the woman. Nothing special would have happened. What is the correct answer for my query? Just give me your answer without explanation and other words.|The correct answer is: The man would have been angry with the woman. |Based on the provided answer (if any) and the generated answer, respond to the following question with either \"Yes\" or \"No\". Your choice should be based on your judgment and the following rules: Yes: If the meanings of the two answers hold the same meaning, choose \"Yes\". No: If the meanings of the two answers are different, choose \"No\".\\n\\\\Answer 1:\\n\"The man would have been angry with the woman.\"\\n\\\\Answer 2:\\n  The correct answer is: The man would have been angry with the woman.| 'Yes'\n",
    "\n",
    "\n",
    "2. Then get the response from GPT models and calculate the accuracy score from ten times trials.\n",
    "3. Finally, do the statistical analysis and causal modeling for future usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def extract_final_response(response, prompt):\n",
    "    return response[len(prompt) :]\n",
    "\n",
    "\n",
    "def match_prompt_column(model):\n",
    "    if \"Llama-2\" in model:\n",
    "        return \"llama2_chat_initial_prompt\"\n",
    "    elif \"Mixtral\" in model or \"Mistral\" in model:\n",
    "        return \"mixtral_instruct_initial_prompt\"\n",
    "    elif \"Llama-3\" in model:\n",
    "        return \"llama3_chat_initial_prompt\"\n",
    "    else:\n",
    "        print(\"The model name didn't match anything, please check!!!!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_pure_response(row):\n",
    "    model = row[\"model\"]\n",
    "    prompt_column = match_prompt_column(model)\n",
    "    response = row[\"generated_response\"]\n",
    "    prompt = row[prompt_column]\n",
    "    return extract_final_response(response=response, prompt=prompt)\n",
    "\n",
    "\n",
    "def process_string(input_string):\n",
    "    processed_string = input_string.strip(\"[]\").replace(\"\\\\\", \"\")\n",
    "    questions = processed_string.split(\"\\n\")\n",
    "    questions = [q.strip(\"'\") for q in questions]\n",
    "    return questions\n",
    "\n",
    "\n",
    "SYS_MSG = \"\"\"Based on the provided answer (if any) and the generated answer, respond to the following question with either \"Yes\" or \"No\". Your choice should be based on your judgment and the following rules: Yes: If the meanings of the two answers hold the same meaning, choose \"Yes\". No: If the meanings of the two answers are different, choose \"No\".\"\"\"\n",
    "foler_path = \"data/CR\"\n",
    "file_name = \"exp_result_Mixtral-8x7B-Instruct-v0.1_20240601004917_953113.csv\"\n",
    "output_file_name = file_name.replace(\".csv\", \"_evaluated.csv\")\n",
    "output_file_name_jsonl = file_name.replace(\".csv\", \"_evaluated.jsonl\")\n",
    "input_path = os.path.join(foler_path, file_name)\n",
    "output_path = os.path.join(foler_path, output_file_name)\n",
    "output_path_jsonl = os.path.join(foler_path, output_file_name_jsonl)\n",
    "_data = pd.read_csv(input_path)\n",
    "eval_model = \"gpt-3.5-turbo-0125\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "temperature = 0\n",
    "_data[\"pure_response\"] = _data.apply(extract_pure_response, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(output_path):\n",
    "    print(\"Output path exists.\")\n",
    "    data_evaluated = pd.read_csv(output_path)\n",
    "    last_idx = data_evaluated.shape[0]\n",
    "else:\n",
    "    print(\"Output path does not exist.\")\n",
    "    last_idx = 0\n",
    "\n",
    "test_times = 0\n",
    "times = 0\n",
    "pbar = tqdm(total=len(_data))\n",
    "for index, entry in tqdm(_data.iterrows()):\n",
    "    if index <= last_idx:\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "    updated_row = entry.copy()\n",
    "    correct_answer = entry[\"CorrectAnswer\"]\n",
    "    output = entry[\"pure_response\"]\n",
    "    if output is None:  # skip if result hasn't been generated\n",
    "        continue\n",
    "    message = []\n",
    "    answer = \"\"\n",
    "    content = f'{SYS_MSG}\\n\\Answer 1:\\n\"{correct_answer}\"\\n\\Answer 2:\\n{output}\\n'\n",
    "    message.append({\"role\": \"user\", \"content\": content})\n",
    "    # create a chat completion\n",
    "    success = False\n",
    "    early_stop = False\n",
    "    while not success:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=eval_model,\n",
    "                messages=message,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            generation = completion.choices[0].message.content\n",
    "            message.append({\"role\": \"assistant\", \"content\": generation})\n",
    "            # check if generation is yes or no\n",
    "            if generation.lower().startswith(\"yes\") or generation.lower().startswith(\n",
    "                \"no\"\n",
    "            ):\n",
    "                if generation.lower().startswith(\"yes\"):\n",
    "                    answer += \"Yes\\n\"\n",
    "                else:\n",
    "                    answer += \"No\\n\"\n",
    "            else:\n",
    "                if \"YES\" in generation and \"NO\" not in generation:\n",
    "                    answer += \"Yes\\n\"\n",
    "                elif \"YES\" not in generation and \"NO\" in generation:\n",
    "                    answer += \"No\\n\"\n",
    "                else:\n",
    "                    for msg in message:\n",
    "                        print(msg[\"content\"])\n",
    "                    print(\"NO YES or NO answer!\" + generation)\n",
    "                    answer += \"None\\n\"\n",
    "                    # early_stop = True Dont early stop\n",
    "                    break\n",
    "            success = True\n",
    "        except Exception as e:\n",
    "            print(\"ERROR!\")\n",
    "            print(e)\n",
    "            print(\"Retry!\")\n",
    "            time.sleep(20)\n",
    "\n",
    "    # when no answer occurs, break the loop and continue to next instance\n",
    "    # if early_stop:\n",
    "    #     break\n",
    "\n",
    "    answer = answer[:-1]\n",
    "    # save eval results as List[bool]\n",
    "    bool_results = []\n",
    "    for i in answer.split(\"\\n\"):\n",
    "        if i == \"Yes\":\n",
    "            bool_results.append(True)\n",
    "        elif i == \"No\":\n",
    "            bool_results.append(False)\n",
    "        else:\n",
    "            bool_results.append(None)\n",
    "\n",
    "    updated_row[\"eval\"] = bool_results\n",
    "    updated_row[\"messages_openai\"] = message\n",
    "    test_times += 1\n",
    "    updated_dataframe = pd.DataFrame([updated_row])\n",
    "    pbar.update(1)\n",
    "    if not os.path.exists(output_path):\n",
    "        updated_dataframe.to_csv(output_path, index=False, mode=\"w\", header=True)\n",
    "    else:\n",
    "        updated_dataframe.to_csv(output_path, index=False, mode=\"a\", header=False)\n",
    "    times = times + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def str_to_bool(s):\n",
    "    return s == \"[True]\"\n",
    "\n",
    "\n",
    "def top1_accuracy_per_group(\n",
    "    df, pctid_col=\"PCTID\", temp_col=\"Temperature\", pred_col=\"bool_eval\"\n",
    "):\n",
    "    def top1_accuracy(group):\n",
    "        correct = (group[pred_col]).sum()\n",
    "        total = len(group)\n",
    "        return correct / total\n",
    "\n",
    "    accuracy_df = (\n",
    "        df.groupby([pctid_col, temp_col, \"model\"])\n",
    "        .apply(top1_accuracy)\n",
    "        .reset_index(name=\"top1_accuracy\")\n",
    "    )\n",
    "    return accuracy_df\n",
    "\n",
    "\n",
    "foler_path = \"data/CR\"\n",
    "output_file_name = file_name.replace(\".csv\", \"_evaluated.csv\")\n",
    "input_path = os.path.join(foler_path, file_name)\n",
    "output_path = os.path.join(foler_path, output_file_name)\n",
    "df = pd.read_csv(output_path)\n",
    "df[\"bool_eval\"] = df[\"eval\"].apply(str_to_bool)\n",
    "accuracy_df = top1_accuracy_per_group(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = accuracy_df.groupby([\"model\", \"Temperature\"])[\"top1_accuracy\"].mean()\n",
    "\n",
    "models = accuracy_df[\"model\"].unique()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model in models:\n",
    "    model_data = grouped_data[model]\n",
    "    plt.plot(\n",
    "        model_data.index.get_level_values(\"Temperature\"),\n",
    "        model_data,\n",
    "        marker=\"o\",\n",
    "        label=model,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Temperature Scaling\")\n",
    "plt.ylabel(\"Rouge L fmeasure\")\n",
    "plt.title(\"Mean Rouge L fmeasure with Temperature Across Models\\nTREC Dataset\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"Temperature\", y=\"top1_accuracy\", hue=\"model\", data=accuracy_df)\n",
    "\n",
    "plt.xlabel(\"Temperature Scaling\")\n",
    "plt.ylabel(\"Top-1 Accuracy\")\n",
    "plt.title(\"Top-1 Accuracy with Temperature Across Models\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "current_time_str = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "model_weight_sizes = {\n",
    "    \"Llama-2-13b-chat-hf\": 13,\n",
    "    \"Mistral-7B-Instruct-v0.2\": 7,\n",
    "    \"Llama-2-70b-chat-hf\": 70,\n",
    "    \"Meta-Llama-3-8B-Instruct\": 8,\n",
    "    \"Llama-2-7b-chat-hf\": 7,\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\": 56,\n",
    "}\n",
    "\n",
    "folder_path = \"data/CR\"\n",
    "output_photo_name = f\"CR_{current_time_str}.svg\"\n",
    "output_photo_path = os.path.join(folder_path, output_photo_name)\n",
    "all_dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and \"evaluated\" in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.split(\"_\")[2]\n",
    "        df[\"model_name\"] = model_name\n",
    "        df[\"weight_size\"] = model_weight_sizes.get(model_name, None)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_boolean(s):\n",
    "    s_stripped = s.strip(\"[]\")\n",
    "    return s_stripped.lower() == \"true\"\n",
    "\n",
    "\n",
    "df[\"eval\"] = df[\"eval\"].apply(lambda x: string_to_boolean(x))\n",
    "df[\"eval\"] = df[\"eval\"].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical results evaluation & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.groupby([\"model\", \"Temperature\"])[\"eval\"].mean()\n",
    "\n",
    "models = [\n",
    "    \"Llama-2-7b-chat-hf\",\n",
    "    \"Llama-2-13b-chat-hf\",\n",
    "    \"Llama-2-70b-chat-hf\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Meta-Llama-3-70B-Instruct\",\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "]\n",
    "\n",
    "markers = [\n",
    "    \".\",\n",
    "    \"*\",\n",
    "    \"o\",\n",
    "    \"+\",\n",
    "    \"^\",\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"8\",\n",
    "    \"s\",\n",
    "    \"p\",\n",
    "    \",\",\n",
    "    \"v\",\n",
    "    \"D\",\n",
    "    \"d\",\n",
    "    \"x\",\n",
    "    \"|\",\n",
    "    \"_\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, model in enumerate(models):\n",
    "    model_data = grouped_data[model]\n",
    "    marker = markers[i % len(markers)]  # Select marker cyclically from the list\n",
    "    plt.plot(\n",
    "        model_data.index.get_level_values(\"Temperature\"),\n",
    "        model_data,\n",
    "        marker=marker,\n",
    "        markersize=8,\n",
    "        label=model,\n",
    "    )\n",
    "\n",
    "grouped_data_temperature = df.groupby([\"Temperature\"])[\"eval\"].mean()\n",
    "plt.plot(\n",
    "    grouped_data_temperature.index,\n",
    "    grouped_data_temperature,\n",
    "    linestyle=\"--\",\n",
    "    marker=\"x\",\n",
    "    markersize=8,\n",
    "    color=\"black\",\n",
    "    label=\"Average Score Among All Models\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Temperature Scaling\")\n",
    "plt.ylabel(\"T1 Accuracy\")\n",
    "# plt.title(\"Caual Reasoning\\n Mean T1 Accuracy with Temperature Across Models - CRASS\")\n",
    "plt.grid(\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=4)\n",
    "plt.savefig(output_photo_path, format=\"svg\", bbox_inches=\"tight\")\n",
    "# plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "models = [\n",
    "    \"Llama-2-7b-chat-hf\",\n",
    "    \"Llama-2-13b-chat-hf\",\n",
    "    \"Llama-2-70b-chat-hf\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Meta-Llama-3-70B-Instruct\",\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model_name in models:\n",
    "    model_data = df[df[\"model\"] == model_name]\n",
    "    temperatures = model_data[\"Temperature\"].values\n",
    "    accuracy = model_data[\"eval\"].values\n",
    "    corr, p_value = spearmanr(temperatures, accuracy)\n",
    "\n",
    "    temperatures = sm.add_constant(temperatures)\n",
    "    model = sm.OLS(accuracy, temperatures).fit()\n",
    "\n",
    "    predictions = model.predict(temperatures)\n",
    "    se = np.sqrt(mean_squared_error(accuracy, predictions))\n",
    "\n",
    "    r_squared = model.rsquared\n",
    "    r_squared_adj = model.rsquared_adj\n",
    "\n",
    "    beta_hat = model.params\n",
    "    results.append(\n",
    "        {\n",
    "            \"model\": model_name,\n",
    "            \"beta+-SE\": f\"{beta_hat[1]:.3f} ± {se:.3f}\",\n",
    "            \"r_squared\": f\"{r_squared:.3f}\",\n",
    "            \"r_squared_adj\": f\"{r_squared_adj:.3f}\",\n",
    "            \"Spearman correlation Coefficient\": f\"{corr:.3f}\",\n",
    "            \"P_value\": f\"{p_value:.3f}\",\n",
    "            \"se\": f\"{se:.3f}\",\n",
    "            \"beta_hat_0\": f\"{beta_hat[0]:.3f}\",\n",
    "            \"beta_hat_1\": f\"{beta_hat[1]:.3f}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# for model in models:\n",
    "#     # print(f\"Model: {model}\")\n",
    "#     print(f\"{correlations[model]}\")\n",
    "# print()\n",
    "# for model in models:\n",
    "#     print(f\"{p_values[model]}\")\n",
    "\n",
    "# for model in models:\n",
    "#     print(f\"Model: {model}\")\n",
    "#     print(f\"Kendall correlation: {correlations[model]}\")\n",
    "#     print(f\"p-value: {p_values[model]}\")\n",
    "#     print(f\"OLS summary:\")\n",
    "#     print(ols_results[model].summary())\n",
    "#     print()\n",
    "\n",
    "results_df.to_csv(\"output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
