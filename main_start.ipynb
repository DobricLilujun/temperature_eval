{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertChoicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from BertChoicer.static import model_id, assets_path\n",
    "from BertChoicer.gradioUIManager import GradioUIManager\n",
    "from BertChoicer.bertChoicer import bertClassifier, bertClassifierList\n",
    "from BertChoicer.services import find_performance_score, get_best_temperature, get_average_best_temperature\n",
    "import json\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set On button Click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model_path = \"Volavion/bert-base-multilingual-uncased-temperature-cls\"\n",
    "tokenizer_path = \"Volavion/bert-base-multilingual-uncased-temperature-cls\"\n",
    "df = pd.read_json(assets_path, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/gradio/layouts/column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/lujun_li/anaconda3/envs/causalLLM/lib/python3.10/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_37349/415950295.py\", line 43, in on_button_click\n",
      "    _, prob_dict = bertClassifier(\n",
      "TypeError: bertClassifier() got an unexpected keyword argument 'input_model_name'\n"
     ]
    }
   ],
   "source": [
    "def on_experiment_button_click(\n",
    "    input_text,  # The input prompt that will be sent to the API\n",
    "    best_temperature,  # The best temperature value to be used in the request\n",
    "    input_model=\"llama2-7b\",  # Default model name to be used, can be overridden\n",
    "    input_api=\"http://localhost:11434/api/generate\",  # Default API URL, can be overridden\n",
    "):\n",
    "    url = input_api  # Set the API URL to the provided URL or the default URL\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}  # Set the headers for the request to indicate JSON data\n",
    "\n",
    "    # Prepare the data to send in the request body\n",
    "    data = {\n",
    "        \"model\": \"llama3.2:3b\",  # The model to be used for generating the response\n",
    "        \"prompt\": input_text,  # The input text (prompt) to be processed by the model\n",
    "        \"stream\": False,  # Whether to stream the response (set to False here, meaning the response is not streamed)\n",
    "        \"temperature\": best_temperature,  # The temperature value that controls the randomness of the response\n",
    "    }\n",
    "\n",
    "    # Send a POST request to the API with the prepared data and headers\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    print(response)  # Print the response object for debugging purposes\n",
    "\n",
    "    # Check if the response status code is 200 (indicating a successful request)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()  # Parse the JSON response from the API\n",
    "        return response_json.get(\n",
    "            \"response\", None\n",
    "        )  # Return the value of the 'response' key in the JSON, or None if not present\n",
    "    else:\n",
    "        # If the response status code is not 200, return an error message with the status code\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "\n",
    "\n",
    "def on_button_click(input_text, input_file, input_model):\n",
    "\n",
    "    if input_file is not None:\n",
    "        input_df = pd.read_csv(input_file.name)\n",
    "\n",
    "\n",
    "    # If text input is provided, classify it\n",
    "    if input_text:\n",
    "        _, prob_dict = bertClassifier(\n",
    "            input_text=input_text,\n",
    "            input_model_name=input_model,\n",
    "            model_path=input_model_path,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_padding=512,\n",
    "        )\n",
    "        best_temperature = get_best_temperature(\n",
    "            input_text=input_text,\n",
    "            input_model_path=input_model_path,\n",
    "            input_model_name=input_model,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            df=df,\n",
    "        )\n",
    "\n",
    "    # If file input is provided, classify all texts in the file\n",
    "    if input_file:\n",
    "        input_texts = input_df[\"input\"].tolist()\n",
    "        _, prob_dict = bertClassifierList(\n",
    "            input_text_list=input_texts,\n",
    "            input_model_name=input_model,\n",
    "            model_path=input_model_path,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_padding=512,\n",
    "        )\n",
    "\n",
    "        best_temperature = get_average_best_temperature(\n",
    "            input_text_list=input_texts,\n",
    "            input_model_path=input_model_path,\n",
    "            input_model_name=input_model,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            df=df,\n",
    "        )\n",
    "\n",
    "    prob_list = list(prob_dict.items())\n",
    "    # class_labels = [item[0] for item in prob_list]  # Extract labels (keys)\n",
    "    # probabilities = [item[1] for item in prob_list]  # Extract values (probabilities)\n",
    "\n",
    "    # colors = [\"#ff9999\", \"#66b3ff\", \"#99ff99\", \"#ffcc99\"]\n",
    "\n",
    "    # Create pie chart with improved aesthetics\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(5, 5)\n",
    "    )  # Set the figure size for better visual appeal\n",
    "    table = ax.table(\n",
    "        cellText=df.values,\n",
    "        colLabels=df.columns,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\"\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(df.columns))))\n",
    "\n",
    "    # # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    # ax.axis(\"equal\")\n",
    "\n",
    "    # # Add a title\n",
    "    # ax.set_title(\"Class Distribution\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    return fig, best_temperature, best_temperature\n",
    "\n",
    "choices = [\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Phi-3.5-mini-instruct\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"Llama-2-7b-chat-hf\",\n",
    "    \"Llama-2-13b-chat-hf\",\n",
    "    \"Llama-2-70b-chat-hf\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Meta-Llama-3-70B-Instruct\",\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\"\n",
    "]\n",
    "\n",
    "\n",
    "BertChoicerUIManager = GradioUIManager(choices = choices, on_button_click = on_button_click, on_experiment_button_click = on_experiment_button_click)\n",
    "BertChoicerUIManager.create_interface()\n",
    "BertChoicerUIManager.demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_experiment_button_click(\n",
    "    input_text,  # The input prompt that will be sent to the API\n",
    "    best_temperature,  # The best temperature value to be used in the request\n",
    "    input_model=\"llama2-7b\",  # Default model name to be used, can be overridden\n",
    "    input_api=\"http://localhost:11434/api/generate\",  # Default API URL, can be overridden\n",
    "):\n",
    "    url = input_api  # Set the API URL to the provided URL or the default URL\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}  # Set the headers for the request to indicate JSON data\n",
    "\n",
    "    # Prepare the data to send in the request body\n",
    "    data = {\n",
    "        \"model\": \"llama3.2:3b\",  # The model to be used for generating the response\n",
    "        \"prompt\": input_text,  # The input text (prompt) to be processed by the model\n",
    "        \"stream\": False,  # Whether to stream the response (set to False here, meaning the response is not streamed)\n",
    "        \"temperature\": best_temperature,  # The temperature value that controls the randomness of the response\n",
    "    }\n",
    "\n",
    "    # Send a POST request to the API with the prepared data and headers\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    print(response)  # Print the response object for debugging purposes\n",
    "\n",
    "    # Check if the response status code is 200 (indicating a successful request)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()  # Parse the JSON response from the API\n",
    "        return response_json.get(\n",
    "            \"response\", None\n",
    "        )  # Return the value of the 'response' key in the JSON, or None if not present\n",
    "    else:\n",
    "        # If the response status code is not 200, return an error message with the status code\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "\n",
    "\n",
    "def on_button_click(input_text, input_file, input_model):\n",
    "\n",
    "    if input_file is not None:\n",
    "        input_df = pd.read_csv(input_file.name)\n",
    "\n",
    "\n",
    "    # If text input is provided, classify it\n",
    "    if input_text:\n",
    "        _, prob_dict = bertClassifier(\n",
    "            input_text=input_text,\n",
    "            input_model_name=input_model,\n",
    "            model_path=input_model_path,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_padding=512,\n",
    "        )\n",
    "        best_temperature = get_best_temperature(\n",
    "            input_text=input_text,\n",
    "            input_model_path=input_model_path,\n",
    "            input_model_name=input_model,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            df=df,\n",
    "        )\n",
    "\n",
    "    # If file input is provided, classify all texts in the file\n",
    "    if input_file:\n",
    "        input_texts = input_df[\"input\"].tolist()\n",
    "        _, prob_dict = bertClassifierList(\n",
    "            input_text_list=input_texts,\n",
    "            input_model_name=input_model,\n",
    "            model_path=input_model_path,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_padding=512,\n",
    "        )\n",
    "\n",
    "        best_temperature = get_average_best_temperature(\n",
    "            input_text_list=input_texts,\n",
    "            input_model_path=input_model_path,\n",
    "            input_model_name=input_model,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            df=df,\n",
    "        )\n",
    "\n",
    "    prob_list = list(prob_dict.items())\n",
    "    # class_labels = [item[0] for item in prob_list]  # Extract labels (keys)\n",
    "    # probabilities = [item[1] for item in prob_list]  # Extract values (probabilities)\n",
    "\n",
    "    # colors = [\"#ff9999\", \"#66b3ff\", \"#99ff99\", \"#ffcc99\"]\n",
    "\n",
    "    # Create pie chart with improved aesthetics\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(5, 5)\n",
    "    )  # Set the figure size for better visual appeal\n",
    "    table = ax.table(\n",
    "        cellText=df.values,\n",
    "        colLabels=df.columns,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\"\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(df.columns))))\n",
    "\n",
    "    # # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    # ax.axis(\"equal\")\n",
    "\n",
    "    # # Add a title\n",
    "    # ax.set_title(\"Class Distribution\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    return fig, best_temperature, best_temperature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/vllm_env_lujun/lib/python3.11/site-packages/gradio/layouts/column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices = [\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Phi-3.5-mini-instruct\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"Llama-2-7b-chat-hf\",\n",
    "    \"Llama-2-13b-chat-hf\",\n",
    "    \"Llama-2-70b-chat-hf\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Meta-Llama-3-70B-Instruct\",\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\"\n",
    "]\n",
    "\n",
    "\n",
    "BertChoicerUIManager = GradioUIManager(choices = choices, on_button_click = on_button_click, on_experiment_button_click = on_experiment_button_click)\n",
    "BertChoicerUIManager.create_interface()\n",
    "BertChoicerUIManager.demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
