# Temperature_eval

##  Introduction

This is a respository for evaluation and visulization of the results obtaining from observations. This is the respository acoomplanied by the following paper:

> **Hot N Cold: A Comprehensive Analysis of Temperature on the Performance of Llms**

## Abstract

> When employing Large Language Models (LLMs) on different tasks, a crucial hyperparameter to alternate is the sampling temperature, typically adjusted through logarithmic probability indicating the level of randomness. Recent research focuses on empirical performance analysis within a single scenario and one specific ability, without providing a comprehensive causality analysis encompassing several use cases.  Furthermore, identifying the optimal temperature for specific applications (use case/task) remains a significant challenge in research and industry contexts. In this work, we provide a comprehensive study to understand the behavior and capabilities of LLMs at various sampling temperatures. By following the first two levels of the causality ladder\cite{pearl2018book},  association and intervention. This paper shifts the focus on the effect of temperature from general mixed-task benchmarks to examine six distinct-specific abilities, by developing both statistical and causal models, thereby providing a thorough causality analysis and guidelines for optimal temperature settings.
## Abilities
This respository mainly contains several LLM abilities evaluations include: 

- **Causal Reasoning**: A cognitive faculty historically ascribed solely to humans that derive conclusions from given premises by adhering to strict logical principles.
- **Creativity**: An ability defined involves generating novel and valuable ideas, concepts, or products which require both originality and effectiveness.
- **Instruction Following**: This reflects the crucial ability to adhere to instructions presented in prompts, particularly important in the application of LLMs.
- **In-Context Learning**: The emerging verified ability reflects the skill to comprehend text and perform tasks within its context and few examples, and this skill has become a new paradigm for natural language processing (NLP).
- **Summarization**: This entails condensing lengthy texts or discussions into concise and informative summaries, while preserving key information and main ideas.
- **Machine Translation**: MT is a subfield of computational linguistics, and LLMs have shown outstanding potential in translating text from one language to another.









