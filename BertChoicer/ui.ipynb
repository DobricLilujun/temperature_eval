{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/vllm_env_lujun/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/snt/miniconda3/envs/vllm_env_lujun/lib/python3.11/site-packages/gradio/layouts/column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/15 12:21:16 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from script.functions import evaluate_email, evaluate_email_list\n",
    "import matplotlib.pyplot as plt\n",
    "from BertChoicer.static import model_id, performance_distribution\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(performance_distribution)\n",
    "optimal_temps = (\n",
    "    dataset.groupby([\"model_name\", \"ability\", \"Temperature\"])[\"performance_score\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "optimal_temps[\"normalized_score\"] = optimal_temps.groupby(\"ability\")[\"performance_score\"].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "\n",
    "def find_performance_score(df, model_name, ability, Temperature):\n",
    "    # Step 1: Filter the DataFrame based on the input conditions\n",
    "    result = df[\n",
    "        (df[\"model_name\"] == model_name)\n",
    "        & (df[\"ability\"] == ability)\n",
    "        & (df[\"Temperature\"] == Temperature)\n",
    "    ]\n",
    "\n",
    "    # Step 2: Return the performance_score or None if no matching row is found\n",
    "    if not result.empty:\n",
    "        return result[\"performance_score\"].iloc[\n",
    "            0\n",
    "        ]  # Return the first matching performance_score\n",
    "    else:\n",
    "        return None  # Return None if no matching row is found\n",
    "\n",
    "\n",
    "def get_best_temperature(\n",
    "    input_text,\n",
    "    input_model_path,\n",
    "    input_model_name,\n",
    "    tokenizer_path,\n",
    "    input_distribution_df,\n",
    "):\n",
    "\n",
    "    output, prob_dict = evaluate_email(\n",
    "        input_text=input_text,\n",
    "        input_model_name=input_model_name,\n",
    "        model_path=input_model_path,\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        max_padding=512,\n",
    "    )\n",
    "    best_temperature = None\n",
    "    best_score = 0\n",
    "    for temp in input_distribution_df[\"Temperature\"].unique().tolist():\n",
    "        score = 0\n",
    "        for ability, probability in prob_dict.items():\n",
    "            score += probability * find_performance_score(\n",
    "                input_distribution_df, input_model_name, ability, temp\n",
    "            )\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_temperature = temp\n",
    "    return best_temperature\n",
    "\n",
    "\n",
    "def get_average_best_temperature(\n",
    "    input_text_list,\n",
    "    input_model_path,\n",
    "    input_model_name,\n",
    "    tokenizer_path,\n",
    "    input_distribution_df,\n",
    "):\n",
    "    best_temperatures = []\n",
    "\n",
    "    for input_text in input_text_list:\n",
    "        # Evaluate the email and get the probability distribution\n",
    "        output, prob_dict = evaluate_email(\n",
    "            input_text=input_text,\n",
    "            input_model_name=input_model_name,\n",
    "            model_path=input_model_path,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_padding=512,\n",
    "        )\n",
    "\n",
    "        # Initialize variables to track the best temperature\n",
    "        best_temperature = None\n",
    "        best_score = 0\n",
    "\n",
    "        # Iterate over all unique temperatures in the dataframe\n",
    "        for temp in input_distribution_df[\"Temperature\"].unique().tolist():\n",
    "            score = 0\n",
    "            for ability, probability in prob_dict.items():\n",
    "                # Calculate the performance score\n",
    "                score += probability * find_performance_score(\n",
    "                    input_distribution_df, input_model_name, ability, temp\n",
    "                )\n",
    "            # Update the best score and temperature if a better one is found\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_temperature = temp\n",
    "\n",
    "        # Add the best temperature for this text to the list\n",
    "        if best_temperature is not None:\n",
    "            best_temperatures.append(best_temperature)\n",
    "\n",
    "    # Calculate and return the average of the best temperatures\n",
    "    if best_temperatures:\n",
    "        return sum(best_temperatures) / len(best_temperatures)\n",
    "    else:\n",
    "        return None  # Return None if no best temperatures were found\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def on_experiment_button_click(\n",
    "    input_text,\n",
    "    best_temperature,\n",
    "    input_model=\"llama2-7b\",\n",
    "    input_api=\"http://localhost:11434/api/generate\",\n",
    "):\n",
    "    url = input_api\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # The data you want to send in the request body\n",
    "    data = {\n",
    "        \"model\": \"llama3.2:3b\",\n",
    "        \"prompt\": input_text,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": best_temperature,\n",
    "    }\n",
    "\n",
    "    # Send a POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    print(response)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()  # Parse the JSON response\n",
    "        return response_json.get(\n",
    "            \"response\", None\n",
    "        )  # Get the 'responses' key, or None if not present\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "\n",
    "def on_button_click(input_text, input_file, input_model):\n",
    "    result_text = f\"The input is: {input_text}\"\n",
    "    output_text = \"\"\n",
    "    label_probs = None  # Initialize label_probs in case input_text is empty\n",
    "\n",
    "    if input_file is not None:\n",
    "        df = pd.read_csv(input_file.name)\n",
    "        print(input_file.name)\n",
    "        file_result = df\n",
    "    else:\n",
    "        file_result = None\n",
    "\n",
    "    result_model = f\"The model you choose is: {input_model}\"\n",
    "\n",
    "    # If text input is provided, classify it\n",
    "    if input_text:\n",
    "        output_dist_ability, prob_dict = evaluate_email(\n",
    "            input_text=input_text,\n",
    "            input_model_name=input_model,\n",
    "            model_path=input_model_path,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_padding=512,\n",
    "        )\n",
    "        best_temperature = get_best_temperature(\n",
    "            input_text=input_text,\n",
    "            input_model_path=input_model_path,\n",
    "            input_model_name=input_model,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            input_distribution_df=optimal_temps,\n",
    "        )\n",
    "    if input_file:\n",
    "        input_texts = df[\"input\"].tolist()\n",
    "        output_dist_ability, prob_dict = evaluate_email_list(\n",
    "            input_text_list=input_texts,\n",
    "            input_model_name=input_model,\n",
    "            model_path=input_model_path,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_padding=512,\n",
    "        )\n",
    "        best_temperature = get_average_best_temperature(\n",
    "            input_text_list=input_texts,\n",
    "            input_model_path=input_model_path,\n",
    "            input_model_name=input_model,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            input_distribution_df=optimal_temps,\n",
    "        )\n",
    "\n",
    "    prob_list = list(prob_dict.items())\n",
    "    class_labels = [item[0] for item in prob_list]  # Extract labels (keys)\n",
    "    probabilities = [item[1] for item in prob_list]  # Extract values (probabilities)\n",
    "\n",
    "    colors = [\"#ff9999\", \"#66b3ff\", \"#99ff99\", \"#ffcc99\"]\n",
    "\n",
    "    # Create pie chart with improved aesthetics\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(5, 5)\n",
    "    )  # Set the figure size for better visual appeal\n",
    "    ax.pie(\n",
    "        probabilities,\n",
    "        labels=class_labels,\n",
    "        autopct=\"%1.2f%%\",\n",
    "        startangle=90,\n",
    "        colors=colors,\n",
    "        wedgeprops={\"edgecolor\": \"black\", \"linewidth\": 1.5},\n",
    "        textprops={\"fontsize\": 7, \"fontweight\": \"bold\"},\n",
    "    )\n",
    "\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(\"Class Distribution\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    return fig, best_temperature, best_temperature\n",
    "\n",
    "\n",
    "choices = [\n",
    "    \"Llama-2-7b-chat-hf\",\n",
    "    \"Llama-2-13b-chat-hf\",\n",
    "    \"Llama-2-70b-chat-hf\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Meta-Llama-3-70B-Instruct\",\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    gr.HTML(\n",
    "        \"<div style='text-align: left;'>\"\n",
    "        \"<span style='color: gray; font-size: 25px; font-weight: bold;'>This is the first section where you can input and analyze data.</span><br>\"\n",
    "        \"<span style='color: gray; font-size: 12px;'><small>This section allows you to upload your data, which will be processed and analyzed for insights.</small></span>\"\n",
    "        \"</div>\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=0.5):\n",
    "            input_text_box = gr.Textbox(label=\"Input Text\", elem_id=\"input_text\")\n",
    "            input_file_box = gr.File(\n",
    "                label=\"Input File\",\n",
    "                elem_id=\"input_file\",\n",
    "                elem_classes=\"small-input-file\",\n",
    "            )\n",
    "\n",
    "            input_model_radio = gr.Radio(\n",
    "                choices=choices,\n",
    "                label=\"Select Model\",\n",
    "                elem_id=\"input_model\",\n",
    "                value=choices[0],\n",
    "                interactive=True,\n",
    "            )\n",
    "        with gr.Column(scale=0.5):\n",
    "            # file_analysis_output_box = gr.Textbox(\n",
    "            #     label=\"File Analysis Output\",\n",
    "            #     elem_id=\"file_analysis_output\",\n",
    "            #     interactive=False,\n",
    "            # )\n",
    "            cls_label_output_box = gr.Plot(\n",
    "                label=\"Class Label Output (Pie Chart)\",\n",
    "                elem_id=\"cls_label_output\",\n",
    "            )\n",
    "\n",
    "            best_temperature_output_box = gr.Textbox(\n",
    "                label=\"Best Temperature Output\",\n",
    "                elem_id=\"best_temperature_output\",\n",
    "                interactive=False,\n",
    "            )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1.0):\n",
    "            input_button = gr.Button(\"Analyze\", elem_id=\"input_button\")\n",
    "\n",
    "    gr.HTML(\n",
    "        \"<div style='text-align: left;'>\"\n",
    "        \"<span style='color: gray; font-size: 25px; font-weight: bold;'>Recommendation section</span><br>\"\n",
    "        \"<span style='color: gray; font-size: 12px;'><small>Give you the best temperature settings according to your input or prompts</small></span>\"\n",
    "        \"</div>\"\n",
    "    )\n",
    "    gr.HTML(\"<hr>\")\n",
    "\n",
    "    with gr.Row():  # 第二行\n",
    "        with gr.Column(scale=1):\n",
    "            input_temperature_slider = gr.Slider(\n",
    "                minimum=0,\n",
    "                maximum=2.0,\n",
    "                step=0.1,\n",
    "                label=\"Input Temperature\",\n",
    "                elem_id=\"input_temperature\",\n",
    "            )\n",
    "            input_api_box = gr.Textbox(\n",
    "                label=\"API Key\",\n",
    "                elem_id=\"input_api\",\n",
    "                value=\"http://localhost:11434/api/generate\",\n",
    "            )\n",
    "            start_experiment_button = gr.Button(\n",
    "                \"Start Experiment\", elem_id=\"start_experiment\"\n",
    "            )\n",
    "\n",
    "    gr.HTML(\n",
    "        \"<div style='text-align: left;'>\"\n",
    "        \"<span style='color: gray; font-size: 25px; font-weight: bold;'>Outputs</span><br>\"\n",
    "        \"<span style='color: gray; font-size: 12px;'><small>The output files and analysis</small></span>\"\n",
    "        \"</div>\"\n",
    "    )\n",
    "    gr.HTML(\"<hr>\")\n",
    "    with gr.Row():\n",
    "        output_text_box = gr.Textbox(\n",
    "            label=\"Output Text\", elem_id=\"output_text\", lines=10, interactive=False\n",
    "        )\n",
    "        download_results = gr.Button(\"Download Results\", elem_id=\"download_results\")\n",
    "\n",
    "    input_button.click(\n",
    "        on_button_click,\n",
    "        inputs=[input_text_box, input_file_box, input_model_radio],\n",
    "        outputs=[\n",
    "            cls_label_output_box,\n",
    "            best_temperature_output_box,\n",
    "            input_temperature_slider,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    start_experiment_button.click(\n",
    "        on_experiment_button_click,\n",
    "        inputs=[\n",
    "            input_text_box,\n",
    "            input_temperature_slider,\n",
    "            input_model_radio,\n",
    "            input_api_box,\n",
    "        ],\n",
    "        outputs=[output_text_box],\n",
    "    )\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m abilities \u001b[38;5;241m=\u001b[39m optimal_temps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mability\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "abilities = optimal_temps[\"ability\"].unique()\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(len(abilities), 1, figsize=(10, 6 * len(abilities)))\n",
    "\n",
    "if len(abilities) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, ability in enumerate(abilities):\n",
    "    ax = axes[idx]\n",
    "    ability_data = optimal_temps[optimal_temps[\"ability\"] == ability]\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=\"Temperature\",\n",
    "        y=\"performance_score\",\n",
    "        hue=\"model_name\",\n",
    "        data=ability_data,\n",
    "        ax=ax,\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Performance Score vs Temperature for {ability}\")\n",
    "    ax.set_xlabel(\"Temperature\")\n",
    "    ax.set_ylabel(\"Performance Score\")\n",
    "    ax.legend(title=\"Model Name\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and set to evaluation mode.\n",
      "pred_flat : [[0.6521716  0.10458722 0.05230493 0.0358962  0.08756703 0.06747298]]\n",
      "labels_flat : [[0 1 2 3 4 5]]\n",
      "mapping: {0: 'CR', 1: 'CT', 2: 'ICL', 3: 'IF', 4: 'MT', 5: 'SUM'}\n",
      "CR: 0.65 CT: 0.10 ICL: 0.05 IF: 0.04 MT: 0.09 SUM: 0.07 \n"
     ]
    }
   ],
   "source": [
    "from script.functions import evaluate_email\n",
    "\n",
    "input_model_path = \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/training/bert_model_target_5\"\n",
    "tokenizer_path = \"/home/snt/llm_models/bert-base-multilingual-uncased\"\n",
    "\n",
    "dataset_path = (\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/all_data_for_bert_training.csv\"\n",
    ")\n",
    "input_text = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output_dist_ability, prob_dict = evaluate_email(\n",
    "    input_text=input_text,\n",
    "    input_model_name=\"llama2-7b\",\n",
    "    model_path=input_model_path,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_padding=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env_lujun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
