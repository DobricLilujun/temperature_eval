{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "file_path = \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/.vscode/api_key.txt\"\n",
    "\n",
    "# # Read the API key from the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    openai.api_key = file.read().strip()\n",
    "\n",
    "\n",
    "def call_openai_api(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def evaluate_prediction_with_conversion(task, example, prediction):\n",
    "    # Helper functions for conversion\n",
    "    def bool_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"true\"\n",
    "        if value == \"true\":\n",
    "            return 1\n",
    "        elif value == \"false\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"true\" but does not contain \"false\"\n",
    "        elif \"true\" in value and \"false\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"false\" but does not contain \"true\"\n",
    "        elif \"false\" in value and \"true\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"true\" and \"false\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def yes_no_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"yes\"\n",
    "        if value == \"yes\":\n",
    "            return 1\n",
    "        elif value == \"no\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"yes\" but does not contain \"no\"\n",
    "        elif \"yes\" in value and \"no\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"no\" but does not contain \"yes\"\n",
    "        elif \"no\" in value and \"yes\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"yes\" and \"no\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def entailment_to_label(value):\n",
    "        # Define the mapping for entailment, contradiction, and neutral\n",
    "        mapping = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input matches exactly one of the keys in the mapping\n",
    "        if value in mapping:\n",
    "            return mapping[value]\n",
    "\n",
    "        # Check if the input contains one of the keys without ambiguity\n",
    "        elif (\n",
    "            \"entailment\" in value\n",
    "            and \"contradiction\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"entailment\"]\n",
    "        elif (\n",
    "            \"contradiction\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"contradiction\"]\n",
    "        elif (\n",
    "            \"neutral\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"contradiction\" not in value\n",
    "        ):\n",
    "            return mapping[\"neutral\"]\n",
    "\n",
    "        # If the input is ambiguous or invalid, return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def choice_to_binary(value):\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input contains 'choice 1' and does not contain 'choice 2'\n",
    "        if \"choice 1\" in value and \"choice 2\" not in value:\n",
    "            return 0\n",
    "        elif \"choice 2\" in value:\n",
    "            return 1\n",
    "\n",
    "        # If the input does not match any of the conditions, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Task-specific evaluation\n",
    "    if task == \"boolq\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"cb\":\n",
    "        # Convert prediction (entailment/contradiction/neutral) to label (0/1/2)\n",
    "        return entailment_to_label(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"copa\":\n",
    "        # Convert prediction (choice1/choice2) to binary and compare with label (0/1)\n",
    "        return choice_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"multirc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"record\":\n",
    "        # Direct comparison of prediction with the correct entity\n",
    "        processed_answers = [answer.strip().lower() for answer in example[\"answers\"]]\n",
    "        for answer in processed_answers:\n",
    "            if answer in prediction.strip().lower():\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    elif task == \"rte\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (1/0)\n",
    "        return yes_no_to_binary(prediction) == (1 - int(example[\"label\"]))\n",
    "\n",
    "    elif task == \"wic\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"wsc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    # Default case: unknown task\n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_json(\"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Additional_Results/results_superglue_temperature_0.1/vllm_exp_dataset_csv_superGLUE_Mixtral-8x7B-Instruct-v0.1-awq_FUll__20250119_164943.jsonl\", lines=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task boolq: 0.7573\n",
      "Proportion of True predictions for BERT model on task boolq: 0.7560\n",
      "Proportion of True predictions for GPT model on task boolq: 0.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task cb: 0.7798\n",
      "Proportion of True predictions for BERT model on task cb: 0.7202\n",
      "Proportion of True predictions for GPT model on task cb: 0.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task copa: 0.7600\n",
      "Proportion of True predictions for BERT model on task copa: 0.7867\n",
      "Proportion of True predictions for GPT model on task copa: 0.7867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task multirc: 0.8300\n",
      "Proportion of True predictions for BERT model on task multirc: 0.8283\n",
      "Proportion of True predictions for GPT model on task multirc: 0.8283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task record: 0.7727\n",
      "Proportion of True predictions for BERT model on task record: 0.7710\n",
      "Proportion of True predictions for GPT model on task record: 0.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task rte: 0.7401\n",
      "Proportion of True predictions for BERT model on task rte: 0.7569\n",
      "Proportion of True predictions for GPT model on task rte: 0.7569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task wic: 0.6160\n",
      "Proportion of True predictions for BERT model on task wic: 0.6102\n",
      "Proportion of True predictions for GPT model on task wic: 0.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of True predictions for default model on task wsc: 0.2917\n",
      "Proportion of True predictions for BERT model on task wsc: 0.2981\n",
      "Proportion of True predictions for GPT model on task wsc: 0.2981\n",
      "Proportion of True predictions for default model: 0.7444\n",
      "Proportion of True predictions for BERT model : 0.7436\n",
      "Proportion of True predictions for GPT model : 0.7437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "tasks = df['task'].unique()\n",
    "model_names = df['model_name'].unique()\n",
    "# tasks = [\"copa\",\"wic\",\"wsc\"]\n",
    "\n",
    "total_default = 0\n",
    "total_bert = 0\n",
    "total_gpt = 0\n",
    "total = 0\n",
    "# Iterate through tasks and models to accumulate true predictions and total counts\n",
    "for task in tasks:\n",
    "    true_default = 0\n",
    "    true_bert = 0\n",
    "    true_gpt = 0\n",
    "    total_task = 0\n",
    "    df_task = df[df[\"task\"] == task]\n",
    "    for model_name in model_names:\n",
    "        for i, row in tqdm(df_task[df_task[\"model_name\"] == model_name].iterrows(), desc=\"Running Evaluation\", leave=False):\n",
    "            task = row[\"task\"]\n",
    "            example = json.loads(row[\"example\"])\n",
    "            label_default = evaluate_prediction_with_conversion(task, example, row[\"generate_response_default\"])\n",
    "            label_bert = evaluate_prediction_with_conversion(task, example, row[\"generate_response_bert\"])\n",
    "            label_gpt = evaluate_prediction_with_conversion(task, example, row[\"generate_response_gpt\"])\n",
    "\n",
    "            true_default += label_default\n",
    "            total_default += label_default\n",
    "            true_bert += label_bert\n",
    "            total_bert += label_bert\n",
    "            true_gpt += label_gpt\n",
    "            total_gpt += label_gpt  \n",
    "            total_task += 1\n",
    "            total += 1\n",
    "            row[\"label_default\"] = label_default\n",
    "            row[\"label_bert\"] = label_bert\n",
    "            row[\"label_gpt\"] = label_gpt\n",
    "            # Store results for each row\n",
    "            results.append(row)\n",
    "\n",
    "    true_ratio_default = true_default / total_task\n",
    "    true_ratio_bert = true_bert / total_task\n",
    "    true_ratio_gpt = true_gpt / total_task\n",
    "\n",
    "    print(f\"Proportion of True predictions for default model on task {task}: {true_ratio_default:.4f}\")\n",
    "    print(f\"Proportion of True predictions for BERT model on task {task}: {true_ratio_bert:.4f}\")\n",
    "    print(f\"Proportion of True predictions for GPT model on task {task}: {true_ratio_gpt:.4f}\")\n",
    "\n",
    "\n",
    "true_ratio_default = total_default / total\n",
    "true_ratio_bert = total_bert / total\n",
    "true_ratio_gpt = total_gpt / total\n",
    "print(f\"Proportion of True predictions for default model: {true_ratio_default:.4f}\")\n",
    "print(f\"Proportion of True predictions for BERT model : {true_ratio_bert:.4f}\")\n",
    "print(f\"Proportion of True predictions for GPT model : {true_ratio_gpt:.4f}\")\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Optionally save to a CSV file\n",
    "results_df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
