{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3349925/3490585213.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"category\"] = filtered_df[\"Category\"].map(Category_to_category_id)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "CR      131\n",
       "CT      106\n",
       "SUMM     82\n",
       "MT       32\n",
       "IF       30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"SoftAge-AI/prompt-eng_dataset\")\n",
    "\n",
    "df_train = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "df_validation = pd.DataFrame(dataset[\"validation\"]) if \"validation\" in dataset else None\n",
    "df_test = pd.DataFrame(dataset[\"test\"]) if \"test\" in dataset else None\n",
    "\n",
    "categories_to_save = [\n",
    "    \"Writing\",\n",
    "    \"Problem solving\",\n",
    "    \"Text Summarization\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Language Translation\",\n",
    "    \"Roleplaying\",\n",
    "]\n",
    "\n",
    "filtered_df = df_train[df_train[\"Category\"].isin(categories_to_save)]\n",
    "Category_to_category_id = {\n",
    "    \"Writing\": \"CT\",\n",
    "    \"Problem solving\": \"CR\",\n",
    "    \"Text Summarization\": \"SUMM\",\n",
    "    \"Logical Reasoning\": \"CR\",\n",
    "    \"Language Translation\": \"MT\",\n",
    "    \"Roleplaying\": \"IF\",\n",
    "}\n",
    "\n",
    "filtered_df[\"category\"] = filtered_df[\"Category\"].map(Category_to_category_id)\n",
    "filtered_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3349925/3669320389.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"initial_prompt\"]   = filtered_df[\"Prompt\"]\n"
     ]
    }
   ],
   "source": [
    "filtered_df[\"initial_prompt\"] = filtered_df[\"Prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    random.shuffle(random_word_list)\n",
    "\n",
    "    for random_word in random_word_list[:n]:\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "def random_insertion(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        random_word = random.choice(words)\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            insert_position = random.randint(0, len(words))\n",
    "            words.insert(insert_position, synonym)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def random_swap(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def random_deletion(sentence, p=0.2):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:  # 如果只有一个单词，直接返回\n",
    "        return sentence\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    return \" \".join(new_words) if new_words else random.choice(words)\n",
    "\n",
    "\n",
    "def spelling_error_injection(sentence, n=1):\n",
    "    def typo(word):\n",
    "        if len(word) <= 1:\n",
    "            return word\n",
    "        idx = random.randint(0, len(word) - 2)\n",
    "        return word[:idx] + word[idx + 1] + word[idx] + word[idx + 2 :]\n",
    "\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        words[idx] = typo(words[idx])\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "keyboard_map = {\n",
    "    \"a\": [\"q\", \"w\", \"s\", \"z\"],\n",
    "    \"b\": [\"v\", \"g\", \"h\", \"n\"],\n",
    "}\n",
    "\n",
    "\n",
    "def keyboard_error_injection(sentence, n=1):\n",
    "    def keyboard_typo(word):\n",
    "        chars = list(word)\n",
    "        idx = random.randint(0, len(chars) - 1)\n",
    "        if chars[idx] in keyboard_map:\n",
    "            chars[idx] = random.choice(keyboard_map[chars[idx]])\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        words[idx] = keyboard_typo(words[idx])\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# 假设已有训练语料库的unigram分布\n",
    "unigram_freq = Counter({\"the\": 5000, \"a\": 3000, \"cat\": 1000})\n",
    "\n",
    "\n",
    "def unigram_noising(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        replacement_word = random.choices(\n",
    "            list(unigram_freq.keys()), weights=list(unigram_freq.values()), k=1\n",
    "        )[0]\n",
    "        words[idx] = replacement_word\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/snt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "# 定义增强函数集合|\n",
    "def augment_text(text, times=5):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(times):\n",
    "        # EDA 操作\n",
    "        augmented_texts.append(synonym_replacement(text))\n",
    "        augmented_texts.append(random_insertion(text))\n",
    "        augmented_texts.append(random_swap(text, n=5))\n",
    "        augmented_texts.append(random_deletion(text))\n",
    "\n",
    "        # 噪声注入操作\n",
    "        augmented_texts.append(spelling_error_injection(text))\n",
    "        augmented_texts.append(keyboard_error_injection(text))\n",
    "        augmented_texts.append(unigram_noising(text))\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "\n",
    "augmented_data = []\n",
    "for index, row in filtered_df.iterrows():\n",
    "    original_prompt = row[\"initial_prompt\"]\n",
    "    augmented_prompts = augment_text(original_prompt, 2)\n",
    "    for augmented_prompt in augmented_prompts:\n",
    "        augmented_data.append(\n",
    "            {\n",
    "                \"initial_prompt\": original_prompt,\n",
    "                \"augmented_prompt\": augmented_prompt,\n",
    "                \"ability\": row[\"category\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/all_data_for_bert_training_augmented.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_df = pd.concat([df, augmented_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_df.to_csv(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/all_data_for_bert_training_augmented_opensource.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env_lujun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
