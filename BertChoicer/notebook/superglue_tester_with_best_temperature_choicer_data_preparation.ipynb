{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/causal_env_transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# BERT Selector\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Specify your cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_model(model_path, tokenizer_path):\n",
    "    \"\"\"\n",
    "    Load the pre-trained model and tokenizer from the specified paths.\n",
    "\n",
    "    Args:\n",
    "    - model_path (str): Path to the saved model.\n",
    "    - tokenizer_path (str): Path to the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    - model: The pre-trained model.\n",
    "    - tokenizer: The pre-trained tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "    model = torch.load(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded and set to evaluation mode.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def evaluate_email(input_text, model, tokenizer, max_padding=512):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    texts = [input_text]\n",
    "    for text in texts:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=max_padding,  # Pad & truncate all sentences.\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict[\"input_ids\"])\n",
    "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor([[0, 1, 2, 3, 4, 5]]).to(torch.int64)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(\"Model loaded and set to evaluation mode.\")\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    b_input_ids = input_ids.to(device)\n",
    "    b_input_mask = attention_masks.to(device)\n",
    "    b_labels = labels.to(device)\n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = output.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "    probabilities = np.exp(\n",
    "        logits - np.max(logits, axis=1, keepdims=True)\n",
    "    )  # Stabilized softmax\n",
    "    probabilities = probabilities / np.sum(probabilities, axis=1, keepdims=True)\n",
    "\n",
    "    # Get the mapping of categories to codes before converting to codes\n",
    "    ability_mapping = {0: \"CR\", 1: \"CT\", 2: \"ICL\", 3: \"IF\", 4: \"MT\", 5: \"SUMM\"}\n",
    "\n",
    "    print(f\"pred_flat : {probabilities}\")\n",
    "    print(f\"labels_flat : {label_ids}\")\n",
    "    print(f\"mapping: {ability_mapping}\")\n",
    "\n",
    "    output = \"\"\n",
    "    prob_dict = {}\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        for code, category in ability_mapping.items():\n",
    "            output += f\"{category}: {prob[code]:.2f} \"\n",
    "            prob_dict[category] = prob[code]\n",
    "\n",
    "    print(output)\n",
    "    return output, prob_dict\n",
    "\n",
    "\n",
    "def find_optimal_temperature(\n",
    "    prompt, model_name, grouped_avg_accuracy, model, tokenizer\n",
    "):\n",
    "\n",
    "    performance_map = grouped_avg_accuracy[\n",
    "        grouped_avg_accuracy[\"model_name\"] == model_name\n",
    "    ]\n",
    "\n",
    "    best_performance_prediction = -float(\"inf\")\n",
    "    best_temperature = None\n",
    "    _, prediction_dict = evaluate_email(prompt, model, tokenizer, max_padding=512)\n",
    "    predicted_category = max(prediction_dict, key=prediction_dict.get)\n",
    "\n",
    "    for temperature in np.arange(0.1, 2.0, 0.3):\n",
    "        temperature = round(temperature, 2)\n",
    "        matching_rows = performance_map[\n",
    "            (performance_map[\"temperature\"] == temperature)\n",
    "            & (performance_map[\"category\"] == predicted_category)\n",
    "        ]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            performance_prediction = matching_rows[\"accuracy\"].values[0]\n",
    "\n",
    "            if performance_prediction > best_performance_prediction:\n",
    "                best_performance_prediction = performance_prediction\n",
    "                best_temperature = temperature\n",
    "\n",
    "    return best_temperature, best_performance_prediction\n",
    "\n",
    "\n",
    "# model_path = \"/home/snt/projects_lujun/temperature_eval/bert_model_target_2\"\n",
    "# tokenizer_path = \"/home/snt/llm_models/bert-base-multilingual-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "# model = torch.load(\n",
    "#     \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/training/bert_model_target_8\"\n",
    "# )\n",
    "# email = \"\"\"Prompt:\n",
    "\n",
    "# Please summarize the following content, ensuring that key points are covered and expressed concisely:\n",
    "\n",
    "# Given Content:\n",
    "\n",
    "# In recent years, the impacts of global climate change have become increasingly evident, especially in the form of frequent extreme weather events. Scientists have found that the rise in greenhouse gas emissions, particularly carbon dioxide and methane, is accelerating global temperature increases. This warming trend has led to the intensification of extreme weather phenomena, including heatwaves, floods, and powerful hurricanes. In response to this challenge, governments and environmental organizations worldwide are actively promoting emission reduction policies and the development of green energy. However, despite global efforts, the rate of increase in greenhouse gas emissions has not significantly slowed, and the pace of global warming has not been effectively controlled. As a result, scientists are calling on countries to intensify their actions and implement stricter climate policies to mitigate climate change and avoid potentially catastrophic consequences in the future.\n",
    "# \"\"\"\n",
    "\n",
    "# output, prob_dict = evaluate_email(email, model, tokenizer, max_padding=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "folder_path = \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Results_General/evaluation\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            records = [json.loads(line) for line in file if line.strip()]\n",
    "            df = pd.DataFrame(records)\n",
    "            if \"Qwen2.5-1.5B-Instruct\" in filename:\n",
    "                model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "            elif \"Phi-3.5-mini-instruct\" in filename:\n",
    "                model_name = \"Phi-3.5-mini-instruct\"\n",
    "            elif \"Llama-3.2-3B-Instruct\" in filename:\n",
    "                model_name = \"Llama-3.2-3B-Instruct\"\n",
    "            elif \"Qwen2.5-3B-Instruct\" in filename:\n",
    "                model_name = \"Qwen2.5-3B-Instruct\"\n",
    "            elif \"Llama-3.2-1B-Instruct\" in filename:\n",
    "                model_name = \"Llama-3.2-1B-Instruct\"\n",
    "            elif \"Llama-2-7b-chat-hf\" in filename:\n",
    "                model_name = \"Llama-2-7b-chat-hf\"\n",
    "            elif \"Llama-2-13b-chat-hf\" in filename:\n",
    "                model_name = \"Llama-2-13b-chat-hf\"\n",
    "            elif \"Llama-2-70b-chat-hf\" in filename:\n",
    "                model_name = \"Llama-2-70b-chat-hf\"\n",
    "            elif \"Meta-Llama-3-8B-Instruct\" in filename:\n",
    "                model_name = \"Meta-Llama-3-8B-Instruct\"\n",
    "            elif \"Meta-Llama-3-70B-Instruct\" in filename:\n",
    "                model_name = \"Meta-Llama-3-70B-Instruct\"\n",
    "            elif \"Mistral-7B-Instruct-v0.2\" in filename:\n",
    "                model_name = \"Mistral-7B-Instruct-v0.2\"\n",
    "            elif \"Mixtral-8x7B-Instruct-v0.1\" in filename:\n",
    "                model_name = \"Mixtral-8x7B-Instruct-v0.1\"\n",
    "            else:\n",
    "                model_name = \"Unknown\"\n",
    "            df[\"model_name\"] = model_name\n",
    "            if \"accuracy\" in df.columns:\n",
    "                df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "            else:\n",
    "                df[\"accuracy\"] = df.apply(\n",
    "                    lambda row: (\n",
    "                        row[f\"{row['category']}_accuracy\"]\n",
    "                        if f\"{row['category']}_accuracy\" in df.columns\n",
    "                        else None\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "            df[\"temperature\"] = pd.to_numeric(df[\"temperature\"], errors=\"coerce\")\n",
    "            df[\"temperature\"] = df[\"temperature\"].apply(\n",
    "                lambda x: round(x, 2) if pd.notna(x) else x\n",
    "            )\n",
    "            columns_to_keep = [\"model_name\", \"temperature\", \"category\", \"accuracy\"]\n",
    "            df = df[columns_to_keep]\n",
    "            dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_382701/2080009334.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/snt/miniconda3/envs/causal_env_transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and set to evaluation mode.\n",
      "pred_flat : [[0.03827352 0.78679097 0.02313212 0.04687883 0.03759086 0.0673337 ]]\n",
      "labels_flat : [[0 1 2 3 4 5]]\n",
      "mapping: {0: 'CR', 1: 'CT', 2: 'ICL', 3: 'IF', 4: 'MT', 5: 'SUMM'}\n",
      "CR: 0.04 CT: 0.79 ICL: 0.02 IF: 0.05 MT: 0.04 SUMM: 0.07 \n",
      "1.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df[\"temperature\"] = df[\"temperature\"].astype(float).round(2)\n",
    "\n",
    "grouped_avg_accuracy = (\n",
    "    df.groupby([\"model_name\", \"temperature\", \"category\"])[\"accuracy\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grouped_avg_accuracy[\"temperature\"] = grouped_avg_accuracy[\"temperature\"].round(2)\n",
    "\n",
    "tokenizer_path = \"/home/snt/llm_models/bert-base-multilingual-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "model = torch.load(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/training/bert_model_target_8\"\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Prompt:\n",
    "\n",
    "The year is 2075. Decades of unchecked climate change have transformed Earth into an unpredictable, chaotic environment. Cities lie submerged, deserts stretch endlessly, and hurricanes form with unnatural precision. Despite the warning signs, humanity's response came too late—or did it?\n",
    "\n",
    "Write a short story from the perspective of a climate scientist who stumbles upon an abandoned research facility containing groundbreaking technology. This discovery could reverse the effects of climate change, but using it carries immense risks. As the protagonist grapples with this ethical dilemma, explore their internal conflict, the societal implications of their choice, and how the scars of a warming planet have shaped humanity's resilience.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "optimal_temperature, _ = find_optimal_temperature(\n",
    "    prompt=prompt,\n",
    "    model_name=\"Llama-2-7b-chat-hf\",\n",
    "    grouped_avg_accuracy=grouped_avg_accuracy,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(optimal_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset boolq loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/boolq\n",
      "Dataset cb loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/cb\n",
      "Dataset copa loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/copa\n",
      "Dataset multirc loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/multirc\n",
      "Dataset record loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/record\n",
      "Dataset rte loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/rte\n",
      "Dataset wic loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/wic\n",
      "Dataset wsc loaded from /home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets/wsc\n",
      "Task: boolq\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 9427\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3270\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3245\n",
      "    })\n",
      "})\n",
      "{'question': 'do iran and afghanistan speak the same language', 'passage': 'Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.', 'idx': 0, 'label': 1}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: cb\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "})\n",
      "{'premise': 'It was a complex language. Not written down but handed down. One might say it was peeled down.', 'hypothesis': 'the language was peeled down', 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1, 2}\n",
      "Task: copa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n",
      "{'premise': 'My body cast a shadow over the grass.', 'choice1': 'The sun was rising.', 'choice2': 'The grass was cut.', 'question': 'cause', 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: multirc\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['paragraph', 'question', 'answer', 'idx', 'label'],\n",
      "        num_rows: 27243\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['paragraph', 'question', 'answer', 'idx', 'label'],\n",
      "        num_rows: 4848\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['paragraph', 'question', 'answer', 'idx', 'label'],\n",
      "        num_rows: 9693\n",
      "    })\n",
      "})\n",
      "{'paragraph': 'While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department\\'s counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government\\'s legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on  Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President\\'s life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed  Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding  Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he\\'d help us get  Bin Laden and deal with another issue or two.\" The U.S. effort continued. ', 'question': 'What did the high-level effort to persuade Pakistan include?', 'answer': 'Children, Gerd, or Dorian Popa', 'idx': {'paragraph': 0, 'question': 0, 'answer': 0}, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: record\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['passage', 'query', 'entities', 'entity_spans', 'answers', 'idx'],\n",
      "        num_rows: 100730\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['passage', 'query', 'entities', 'entity_spans', 'answers', 'idx'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['passage', 'query', 'entities', 'entity_spans', 'answers', 'idx'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "{'passage': \"The harrowing stories of women and children locked up for so-called 'moral crimes' in Afghanistan's notorious female prison have been revealed after cameras were allowed inside. Mariam has been in Badam Bagh prison for three months after she shot a man who just raped her at gunpoint and then turned the weapon on herself - but she has yet to been charged. Nuria has eight months left to serve of her sentence for trying to divorce her husband. She gave birth in prison to her son and they share a cell together. Scroll down for video Nuria was jailed for trying to divorce her husband. Her son is one of 62 children living at Badam Bagh prison\\n@highlight\\nMost of the 202 Badam Bagh inmates are jailed for so-called 'moral crimes'\\n@highlight\\nCrimes include leaving their husbands or refusing an arrange marriage\\n@highlight\\n62 children live there and share cells with their mothers and five others\", 'query': 'The baby she gave birth to is her husbands and he has even offered to have the courts set her free if she returns, but @placeholder has refused.', 'entities': ['Badam Bagh', 'Nuria', 'Mariam', 'Afghanistan'], 'entity_spans': {'text': ['Afghanistan', 'Mariam', 'Badam Bagh', 'Nuria', 'Nuria', 'Badam Bagh', 'Badam Bagh'], 'start': [86, 178, 197, 357, 535, 627, 672], 'end': [97, 184, 207, 362, 540, 637, 682]}, 'answers': ['Nuria'], 'idx': {'passage': 0, 'query': 0}}\n",
      "No 'label' field in the test set.\n",
      "Task: rte\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 2490\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 277\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "{'premise': 'No Weapons of Mass Destruction Found in Iraq Yet.', 'hypothesis': 'Weapons of Mass Destruction Found in Iraq.', 'idx': 0, 'label': 1}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: wic\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'sentence1', 'sentence2', 'start1', 'start2', 'end1', 'end2', 'idx', 'label'],\n",
      "        num_rows: 5428\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['word', 'sentence1', 'sentence2', 'start1', 'start2', 'end1', 'end2', 'idx', 'label'],\n",
      "        num_rows: 638\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'sentence1', 'sentence2', 'start1', 'start2', 'end1', 'end2', 'idx', 'label'],\n",
      "        num_rows: 1400\n",
      "    })\n",
      "})\n",
      "{'word': 'place', 'sentence1': 'Do you want to come over to my place later?', 'sentence2': 'A political system with no place for the less prominent groups.', 'start1': 31, 'start2': 27, 'end1': 36, 'end2': 32, 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n",
      "Task: wsc\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 554\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 146\n",
      "    })\n",
      "})\n",
      "{'text': 'Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical.', 'span1_index': 0, 'span2_index': 13, 'span1_text': 'Mark', 'span2_text': 'He', 'idx': 0, 'label': 0}\n",
      "Unique labels in the test set: {0, 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "tasks = [\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\"]\n",
    "save_path = (\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/datasets\"\n",
    ")\n",
    "\n",
    "\n",
    "datasets = {}\n",
    "for task in tasks:\n",
    "    dataset_path = os.path.join(save_path, task)\n",
    "    datasets[task] = load_from_disk(dataset_path)\n",
    "    print(f\"Dataset {task} loaded from {dataset_path}\")\n",
    "\n",
    "\n",
    "server_url = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "dataset_label = \"validation\"\n",
    "\n",
    "output_name = f\"Llama-3.2-1B-Instruct_{dataset_label}\"\n",
    "\n",
    "for task, dataset in datasets.items():\n",
    "    print(f\"Task: {task}\")\n",
    "\n",
    "    # Show the structure of the dataset for the current task\n",
    "    print(dataset)\n",
    "    print(dataset[\"train\"][0])\n",
    "    if \"label\" in dataset[\"validation\"][0]:  # Check if 'label' key exists\n",
    "        # Get all unique labels from the test set\n",
    "        labels = set(example[\"label\"] for example in dataset[\"validation\"])\n",
    "        print(f\"Unique labels in the test set: {labels}\")\n",
    "    else:\n",
    "        print(\"No 'label' field in the test set.\")\n",
    "\n",
    "options = {  # Configuration options for model generation\n",
    "    \"temperature\": 1.0,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"top_p\": 0.9,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "    \"seed\": 47,  # Default\n",
    "    \"n\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_text_with_vllm(\n",
    "    prompt,\n",
    "    model_name=\"/home/snt/llm_models/Llama-3.2-1B-Instruct\",\n",
    "    options={},\n",
    "    server_url=\"http://0.0.0.0:8000/v1/chat/completions\",\n",
    "):\n",
    "    \"\"\"Generate text using the specified language model.\"\"\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    }\n",
    "    payload.update(options)\n",
    "    response = requests.post(server_url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoolQ (Boolean Questions)\n",
    "def format_boolq_prompt(example):\n",
    "    base_instruction = (\n",
    "        \"Answer the following question based on the passage. Type 'True' or 'False'.\\n\"\n",
    "    )\n",
    "    return f\"{base_instruction}Question: {example['question']}\\nPassage: {example['passage']}\\n:\"\n",
    "\n",
    "\n",
    "# CB (CommitmentBank)\n",
    "def format_cb_prompt(example):\n",
    "    base_instruction = \"Answer the following question based on the Premise and Hypothesis. Type 'entailment', 'contradiction', or 'neutral'.\\n\"\n",
    "    return f\"{base_instruction}Premise: {example['premise']}\\nHypothesis: {example['hypothesis']}\\n Does the premise entail the hypothesis? (entailment, contradiction, or neutral):\"\n",
    "\n",
    "\n",
    "# COPA (Choice of Plausible Alternatives)\n",
    "def format_copa_prompt(example):\n",
    "    base_instruction = \"\"\"Answer the following question based on the Premise and Question. Type 'choice 1' or 'choice 2'.\\n If the question is \"What was the cause?\" select the option most likely to explain why the premise happened. If the question is \"What happened as a result?\" select the option most likely to occur as a result of the premise.\"\"\"\n",
    "    return f\"{base_instruction}Premise: {example['premise']}\\nQuestion: {example['question']}\\nChoice 1: {example['choice1']}\\nChoice 2: {example['choice2']}\\n Which choice is more plausible? \"\n",
    "\n",
    "\n",
    "# MultiRC (Multi-Sentence Reading Comprehension)\n",
    "def format_multirc_prompt(example):\n",
    "    base_instruction = \"Based on the provided (Paragraph, Question, Answer) pair, answer the question with 'True' or 'False'.\\n\"\n",
    "    return f\"{base_instruction}Paragraph: {example['paragraph']}\\nQuestion: {example['question']}\\nAnswer: {example['answer']}\\nIs the answer correct? (True/False):\"\n",
    "\n",
    "\n",
    "# ReCoRD (Reading Comprehension with Commonsense Reasoning)\n",
    "def format_record_prompt(example):\n",
    "    base_instruction = \"Based on the provided passage and entities, answer the question with the correct entity.\\n\"\n",
    "    return f\"{base_instruction}Passage: {example['passage']}\\nQuery: {example['query']}\\nEntities: {', '.join(example['entities'])}\\nWhich entity best fills in the blank?\"\n",
    "\n",
    "\n",
    "# RTE (Recognizing Textual Entailment)\n",
    "def format_rte_prompt(example):\n",
    "    base_instruction = \"Answer the following question based on the Premise and Hypothesis. Type 'Yes' or 'No'.\\n\"\n",
    "    return f\"{base_instruction}Premise: {example['premise']}\\nHypothesis: {example['hypothesis']}\\nDoes the premise entail the hypothesis? (Yes or No):\"\n",
    "\n",
    "\n",
    "# WiC (Word-in-Context)\n",
    "def format_wic_prompt(data):\n",
    "    # Extract relevant information from the input data\n",
    "    word = data[\"word\"]\n",
    "    sentence1 = data[\"sentence1\"]\n",
    "    sentence2 = data[\"sentence2\"]\n",
    "    start1, end1 = data[\"start1\"], data[\"end1\"]\n",
    "    start2, end2 = data[\"start2\"], data[\"end2\"]\n",
    "\n",
    "    # Extract the highlighted word in both sentences\n",
    "    sentence1_highlighted = sentence1[start1 : end1 + 1]\n",
    "    sentence2_highlighted = sentence2[start2 : end2 + 1]\n",
    "\n",
    "    # Generate the prompt text\n",
    "    prompt = f\"\"\"\n",
    "    Task: Determine whether the word \"{word}\" is used with the same meaning in both sentences below.\n",
    "\n",
    "    Sentence 1: \"{sentence1}\"  \n",
    "    Sentence 2: \"{sentence2}\"  \n",
    "\n",
    "    The word appears in Sentence 1 as: \"{sentence1_highlighted}\"  \n",
    "    The word appears in Sentence 2 as: \"{sentence2_highlighted}\"  \n",
    "\n",
    "    Question: Is the word \"{word}\" used with the same meaning in both sentences?  Type (Yes, No)\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# WSC (Winograd Schema Challenge)\n",
    "def format_wsc_prompt(data):\n",
    "    base_instruction = \"Based on the provided text and pronoun, answer the question with the correct referent.\\n\"\n",
    "    text = data[\"text\"]\n",
    "    span1_text = data[\"span1_text\"]\n",
    "    span2_text = data[\"span2_text\"]\n",
    "    span1_index = data[\"span1_index\"]\n",
    "    span2_index = data[\"span2_index\"]\n",
    "    prompt = f\"\"\"\n",
    "    {base_instruction}\n",
    "    Text:  \n",
    "    {text}\n",
    "    \n",
    "    The first mention of the entity is: \"{span1_text}\" at index {span1_index}\n",
    "    The second mention of the entity is: \"{span2_text}\" at index {span2_index}\n",
    "    \n",
    "    Task: Identify whether the second mention of the entity refers to the same entity as the first mention.\n",
    "    \n",
    "    Question: Does the second mention of the entity (\"{span2_text}\") refer to the same entity as the first mention (\"{span1_text}\") in the text?  Type (Yes, No)\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "file_path = \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/.vscode/api_key.txt\"\n",
    "\n",
    "# # Read the API key from the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    openai.api_key = file.read().strip()\n",
    "\n",
    "\n",
    "def call_openai_api(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def evaluate_prediction_with_conversion(task, example, prediction):\n",
    "    # Helper functions for conversion\n",
    "    def bool_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"true\"\n",
    "        if value == \"true\":\n",
    "            return 1\n",
    "        elif value == \"false\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"true\" but does not contain \"false\"\n",
    "        elif \"true\" in value and \"false\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"false\" but does not contain \"true\"\n",
    "        elif \"false\" in value and \"true\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"true\" and \"false\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def yes_no_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"yes\"\n",
    "        if value == \"yes\":\n",
    "            return 1\n",
    "        elif value == \"no\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"yes\" but does not contain \"no\"\n",
    "        elif \"yes\" in value and \"no\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"no\" but does not contain \"yes\"\n",
    "        elif \"no\" in value and \"yes\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"yes\" and \"no\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def entailment_to_label(value):\n",
    "        # Define the mapping for entailment, contradiction, and neutral\n",
    "        mapping = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input matches exactly one of the keys in the mapping\n",
    "        if value in mapping:\n",
    "            return mapping[value]\n",
    "\n",
    "        # Check if the input contains one of the keys without ambiguity\n",
    "        elif (\n",
    "            \"entailment\" in value\n",
    "            and \"contradiction\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"entailment\"]\n",
    "        elif (\n",
    "            \"contradiction\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"contradiction\"]\n",
    "        elif (\n",
    "            \"neutral\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"contradiction\" not in value\n",
    "        ):\n",
    "            return mapping[\"neutral\"]\n",
    "\n",
    "        # If the input is ambiguous or invalid, return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def choice_to_binary(value):\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input contains 'choice 1' and does not contain 'choice 2'\n",
    "        if \"choice 1\" in value and \"choice 2\" not in value:\n",
    "            return 0\n",
    "        elif \"choice 2\" in value:\n",
    "            return 1\n",
    "\n",
    "        # If the input does not match any of the conditions, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Task-specific evaluation\n",
    "    if task == \"boolq\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"cb\":\n",
    "        # Convert prediction (entailment/contradiction/neutral) to label (0/1/2)\n",
    "        return entailment_to_label(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"copa\":\n",
    "        # Convert prediction (choice1/choice2) to binary and compare with label (0/1)\n",
    "        return choice_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"multirc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"record\":\n",
    "        # Direct comparison of prediction with the correct entity\n",
    "        processed_answers = [answer.strip().lower() for answer in example[\"answers\"]]\n",
    "        return prediction.strip().lower() in processed_answers\n",
    "\n",
    "    elif task == \"rte\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (1/0)\n",
    "        return yes_no_to_binary(prediction) == (1 - int(example[\"label\"]))\n",
    "\n",
    "    elif task == \"wic\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"wsc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    # Default case: unknown task\n",
    "    return False\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data_rows = []\n",
    "for dataset_label in [\"train\", \"validation\", \"test\"]:\n",
    "    for task, dataset in datasets.items():\n",
    "        test_data = dataset[dataset_label][0 : len(dataset[dataset_label])]\n",
    "\n",
    "        test_data = [\n",
    "            {key: test_data[key][i] for key in test_data}\n",
    "            for i in range(len(test_data[list(test_data.keys())[0]]))\n",
    "        ]\n",
    "\n",
    "        for example in tqdm(test_data, desc=f\"Examples for {task}\", leave=False):\n",
    "            if task == \"boolq\":\n",
    "                prompt = format_boolq_prompt(example)\n",
    "            elif task == \"cb\":\n",
    "                prompt = format_cb_prompt(example)\n",
    "            elif task == \"copa\":\n",
    "                prompt = format_copa_prompt(example)\n",
    "            elif task == \"multirc\":\n",
    "                prompt = format_multirc_prompt(example)\n",
    "            elif task == \"record\":\n",
    "                prompt = format_record_prompt(example)\n",
    "            elif task == \"rte\":\n",
    "                prompt = format_rte_prompt(example)\n",
    "            elif task == \"wic\":\n",
    "                prompt = format_wic_prompt(example)\n",
    "            elif task == \"wsc\":\n",
    "                prompt = format_wsc_prompt(example)\n",
    "\n",
    "            data_rows.append(\n",
    "                {\n",
    "                    \"dataset_label\": dataset_label,\n",
    "                    \"task\": task,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"example\": example,  # Include the raw example for reference if needed\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # options[\"temperature\"] = 0.1\n",
    "            # prediction = generate_text_with_vllm(\n",
    "            #     prompt,\n",
    "            #     options=options,\n",
    "            #     server_url=server_url,\n",
    "            # )\n",
    "\n",
    "            # if dataset_label != \"test\":\n",
    "            #     label = evaluate_prediction_with_conversion(task, example, prediction)\n",
    "            # else:\n",
    "            #     label = None\n",
    "\n",
    "            # result_df = pd.DataFrame(\n",
    "            #     {\n",
    "            #         \"task\": task,\n",
    "            #         \"example\": json.dumps(example),\n",
    "            #         \"prompt\": prompt,\n",
    "            #         \"prediction\": prediction,\n",
    "            #         \"label\": label,\n",
    "            #     },\n",
    "            #     index=[0],\n",
    "            # )\n",
    "            # result_df.to_json(\n",
    "            #     f\"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/superGlue/results_test_{output_name}.jsonl\",\n",
    "            #     orient=\"records\",\n",
    "            #     lines=True,\n",
    "            #     mode=\"a\",\n",
    "            # )\n",
    "\n",
    "super_glue_df = pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset_label', 'task', 'prompt', 'example'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_stats = (\n",
    "    super_glue_df.groupby([\"dataset_label\", \"task\"]).size().reset_index(name=\"count\")\n",
    ")\n",
    "print(grouped_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_glue_df.to_json(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/super_glue_data.jsonl\",\n",
    "    lines=True,\n",
    "    orient=\"records\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the superglue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
