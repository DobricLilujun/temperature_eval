{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ability Classification Probabilities:\n",
      "Causal Reasoning: 17.90%\n",
      "Creativity: 22.17%\n",
      "In-Context Learning: 16.48%\n",
      "Instruction Following: 12.79%\n",
      "Machine Translation: 13.15%\n",
      "Summarization: 17.51%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for model tokenization, sequence classification, tensor computations, and numerical operations\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the name of the pre-trained model to be used for sequence classification\n",
    "model_name = \"Volavion/bert-base-multilingual-uncased-Temperature-CLS\"\n",
    "\n",
    "# Load the tokenizer and model associated with the specified model name\n",
    "# The tokenizer processes text into numerical input for the model, while the model performs sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Retrieve the maximum token length supported by the tokenizer, useful for defining input constraints\n",
    "max_token_length = tokenizer.model_max_length\n",
    "\n",
    "# Define the input text to be analyzed by the model\n",
    "input_text = \"\"\"You are an AI storyteller tasked with crafting an innovative and thought-provoking narrative. Your story should explore the intersection of technology, humanity, and the environment in a near-future world. Use vivid imagery, compelling characters, and unexpected twists to engage the reader.\n",
    "\n",
    "Scenario:\n",
    "In the year 2045, Earth faces a tipping point due to climate change. Amidst this chaos, a young scientist discovers an ancient technology buried deep within the Arctic ice. This technology has the potential to either save the planet or accelerate its demise. The scientist must navigate corporate greed, political intrigue, and personal dilemmas to decide the fate of humanity.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text:\n",
    "# - `add_special_tokens=True` ensures inclusion of special tokens like [CLS] and [SEP]\n",
    "# - `max_length=512` limits input size to prevent exceeding model constraints\n",
    "# - `pad_to_max_length=True` pads shorter inputs to the maximum length\n",
    "# - `return_attention_mask=True` generates an attention mask for the model\n",
    "# - `return_tensors=\"pt\"` outputs PyTorch tensors for model compatibility\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "        input_text,  \n",
    "        add_special_tokens=True,  \n",
    "        max_length=512, \n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  \n",
    "        return_tensors=\"pt\",  # Return pytorch tensors.\n",
    ")\n",
    "\n",
    "# Extract the token IDs and attention mask from the tokenized dictionary\n",
    "input_ids = encoded_dict[\"input_ids\"]\n",
    "attention_mask = encoded_dict[\"attention_mask\"]\n",
    "\n",
    "# Determine the appropriate computation device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device for computation\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode (disables training-specific behaviors like dropout)\n",
    "model.eval()\n",
    "\n",
    "# Transfer input tensors to the same device as the model\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Perform inference without gradient computation to save memory and speed up evaluation\n",
    "with torch.no_grad():\n",
    "    # Obtain the model's output logits (raw predictions)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Convert logits to a NumPy array for easier manipulation\n",
    "logits = outputs.logits.cpu().numpy()\n",
    "\n",
    "# Apply softmax transformation to logits to compute probabilities for each class\n",
    "# Subtracting the max value (logits stability trick) prevents overflow during exponentiation\n",
    "probabilities = np.exp(logits - np.max(logits, axis=1, keepdims=True))  \n",
    "probabilities = probabilities / np.sum(probabilities, axis=1, keepdims=True)\n",
    "\n",
    "# Map class indices to their corresponding abilities\n",
    "ability_mapping = {0: \"Causal Reasoning\", 1: \"Creativity\", 2: \"In-Context Learning\", \n",
    "                   3: \"Instruction Following\", 4: \"Machine Translation\", 5: \"Summarization\"}\n",
    "\n",
    "# Format the probabilities as percentages for each class and construct a readable output string\n",
    "formatted_output = \"\\n\".join(\n",
    "    [f\"{category}: {prob*100:.2f}%\" for prob, category in zip(probabilities[0], ability_mapping.values())]\n",
    ")\n",
    "\n",
    "# Display the classification probabilities for each ability\n",
    "print(\"Ability Classification Probabilities:\")\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
