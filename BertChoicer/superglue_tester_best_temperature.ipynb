{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/causal_env_transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# BERT Selector\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import openai\n",
    "\n",
    "\n",
    "# Specify your cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def format_prompt_gpt(input):\n",
    "    base_instruction = f\"\"\" You will see some text. Based on the content, determine which of the following abilities the text is most aligned with: \n",
    "\n",
    "    Causal Reasoning (CR): Judging and reasoning the causal relationships between events, understanding cause-and-effect chains.\n",
    "    Creativity (CT): Creative thinking, coming up with new and unique solutions or ideas.\n",
    "    In-context Learning (ICL): Learning and understanding based on contextual information, quickly acquiring and applying knowledge from new environments.\n",
    "    Instruction Following (IF): Performing tasks according to given instructions, understanding and executing clear steps or rules.\n",
    "    Machine Translation (MT): Accurately translating between different languages while maintaining grammatical and semantic consistency.\n",
    "    Summarization (SUMM): Summarizing and condensing long texts to extract key information.\n",
    "\n",
    "    Based on the following text, determine which ability it most aligns with. Please indicate the corresponding ability category number in your response. [Only one of the abbreviations: CR, CT, ICL, IF, MT, or SUMM]\n",
    "\n",
    "    Text Input: {input}\n",
    "\n",
    "\"\"\"\n",
    "    return base_instruction\n",
    "\n",
    "\n",
    "def call_openai_api(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def get_category_from_gpt(prompt):\n",
    "    formatted_prompt_gpt = format_prompt_gpt(prompt)\n",
    "    category_list = [\"CR\", \"CT\", \"ICL\", \"IF\", \"MT\", \"SUMM\"]\n",
    "    predicted_category = None\n",
    "\n",
    "    response = call_openai_api(formatted_prompt_gpt)\n",
    "\n",
    "    # Check if response contains any of the valid categories\n",
    "    for cat in category_list:\n",
    "        if cat in response:\n",
    "            predicted_category = cat\n",
    "            break\n",
    "\n",
    "    return predicted_category\n",
    "\n",
    "\n",
    "def find_optimal_temperature_gpt(prompt, model_name, grouped_avg_accuracy):\n",
    "\n",
    "    performance_map = grouped_avg_accuracy[\n",
    "        grouped_avg_accuracy[\"model_name\"] == model_name\n",
    "    ]\n",
    "\n",
    "    best_performance_prediction = -float(\"inf\")\n",
    "    best_temperature = None\n",
    "\n",
    "    predicted_category = get_category_from_gpt(prompt)\n",
    "\n",
    "    if predicted_category is None:\n",
    "        return 1.0, \"None\"\n",
    "    for temperature in np.arange(0.1, 2.0, 0.3):\n",
    "        temperature = round(temperature, 2)\n",
    "        matching_rows = performance_map[\n",
    "            (performance_map[\"temperature\"] == temperature)\n",
    "            & (performance_map[\"category\"] == predicted_category)\n",
    "        ]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            performance_prediction = matching_rows[\"accuracy\"].values[0]\n",
    "\n",
    "            if performance_prediction > best_performance_prediction:\n",
    "                best_performance_prediction = performance_prediction\n",
    "                best_temperature = temperature\n",
    "\n",
    "    return best_temperature, predicted_category\n",
    "\n",
    "\n",
    "def load_model(model_path, tokenizer_path):\n",
    "    \"\"\"\n",
    "    Load the pre-trained model and tokenizer from the specified paths.\n",
    "\n",
    "    Args:\n",
    "    - model_path (str): Path to the saved model.\n",
    "    - tokenizer_path (str): Path to the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    - model: The pre-trained model.\n",
    "    - tokenizer: The pre-trained tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "    model = torch.load(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded and set to evaluation mode.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def evaluate_email(input_text, model, tokenizer, max_padding=512):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    texts = [input_text]\n",
    "    for text in texts:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=max_padding,  # Pad & truncate all sentences.\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict[\"input_ids\"])\n",
    "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor([[0, 1, 2, 3, 4, 5]]).to(torch.int64)\n",
    "\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model.to(device)\n",
    "    # print(\"Model loaded and set to evaluation mode.\")\n",
    "    # model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    b_input_ids = input_ids.to(device)\n",
    "    b_input_mask = attention_masks.to(device)\n",
    "    b_labels = labels.to(device)\n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = output.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "    probabilities = np.exp(\n",
    "        logits - np.max(logits, axis=1, keepdims=True)\n",
    "    )  # Stabilized softmax\n",
    "    probabilities = probabilities / np.sum(probabilities, axis=1, keepdims=True)\n",
    "\n",
    "    # Get the mapping of categories to codes before converting to codes\n",
    "    ability_mapping = {0: \"CR\", 1: \"CT\", 2: \"ICL\", 3: \"IF\", 4: \"MT\", 5: \"SUMM\"}\n",
    "\n",
    "    # print(f\"pred_flat : {probabilities}\")\n",
    "    # print(f\"labels_flat : {label_ids}\")\n",
    "    # print(f\"mapping: {ability_mapping}\")\n",
    "\n",
    "    output = \"\"\n",
    "    prob_dict = {}\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        for code, category in ability_mapping.items():\n",
    "            output += f\"{category}: {prob[code]:.2f} \"\n",
    "            prob_dict[category] = prob[code]\n",
    "\n",
    "    # print(output)\n",
    "    return output, prob_dict\n",
    "\n",
    "\n",
    "def find_optimal_temperature(\n",
    "    prompt, model_name, grouped_avg_accuracy, model, tokenizer\n",
    "):\n",
    "\n",
    "    performance_map = grouped_avg_accuracy[\n",
    "        grouped_avg_accuracy[\"model_name\"] == model_name\n",
    "    ]\n",
    "\n",
    "    best_performance_prediction = -float(\"inf\")\n",
    "    best_temperature = None\n",
    "    _, prediction_dict = evaluate_email(prompt, model, tokenizer, max_padding=512)\n",
    "    predicted_category = max(prediction_dict, key=prediction_dict.get)\n",
    "\n",
    "    for temperature in np.arange(0.1, 2.0, 0.3):\n",
    "        temperature = round(temperature, 2)\n",
    "        matching_rows = performance_map[\n",
    "            (performance_map[\"temperature\"] == temperature)\n",
    "            & (performance_map[\"category\"] == predicted_category)\n",
    "        ]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            performance_prediction = matching_rows[\"accuracy\"].values[0]\n",
    "\n",
    "            if performance_prediction > best_performance_prediction:\n",
    "                best_performance_prediction = performance_prediction\n",
    "                best_temperature = temperature\n",
    "\n",
    "    return best_temperature, predicted_category, prediction_dict\n",
    "\n",
    "\n",
    "# email = \"\"\"Prompt:\n",
    "\n",
    "# # Please summarize the following content, ensuring that key points are covered and expressed concisely:\n",
    "\n",
    "# # Given Content:\n",
    "\n",
    "# # In recent years, the impacts of global climate change have become increasingly evident, especially in the form of frequent extreme weather events. Scientists have found that the rise in greenhouse gas emissions, particularly carbon dioxide and methane, is accelerating global temperature increases. This warming trend has led to the intensification of extreme weather phenomena, including heatwaves, floods, and powerful hurricanes. In response to this challenge, governments and environmental organizations worldwide are actively promoting emission reduction policies and the development of green energy. However, despite global efforts, the rate of increase in greenhouse gas emissions has not significantly slowed, and the pace of global warming has not been effectively controlled. As a result, scientists are calling on countries to intensify their actions and implement stricter climate policies to mitigate climate change and avoid potentially catastrophic consequences in the future.\n",
    "# # \"\"\"\n",
    "\n",
    "# output, prob_dict = evaluate_email(email, model, tokenizer, max_padding=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "folder_path = \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Results_General/evaluation\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            records = [json.loads(line) for line in file if line.strip()]\n",
    "            df = pd.DataFrame(records)\n",
    "            if \"Qwen2.5-1.5B-Instruct\" in filename:\n",
    "                model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "            elif \"Phi-3.5-mini-instruct\" in filename:\n",
    "                model_name = \"Phi-3.5-mini-instruct\"\n",
    "            elif \"Llama-3.2-3B-Instruct\" in filename:\n",
    "                model_name = \"Llama-3.2-3B-Instruct\"\n",
    "            elif \"Qwen2.5-3B-Instruct\" in filename:\n",
    "                model_name = \"Qwen2.5-3B-Instruct\"\n",
    "            elif \"Llama-3.2-1B-Instruct\" in filename:\n",
    "                model_name = \"Llama-3.2-1B-Instruct\"\n",
    "            elif \"Llama-2-7b-chat-hf\" in filename:\n",
    "                model_name = \"Llama-2-7b-chat-hf\"\n",
    "            elif \"Llama-2-13b-chat-hf\" in filename:\n",
    "                model_name = \"Llama-2-13b-chat-hf\"\n",
    "            elif \"Llama-2-70b-chat-hf\" in filename:\n",
    "                model_name = \"Llama-2-70b-chat-hf\"\n",
    "            elif \"Meta-Llama-3-8B-Instruct\" in filename:\n",
    "                model_name = \"Meta-Llama-3-8B-Instruct\"\n",
    "            elif \"Meta-Llama-3-70B-Instruct\" in filename:\n",
    "                model_name = \"Meta-Llama-3-70B-Instruct\"\n",
    "            elif \"Mistral-7B-Instruct-v0.2\" in filename:\n",
    "                model_name = \"Mistral-7B-Instruct-v0.2\"\n",
    "            elif \"Mixtral-8x7B-Instruct-v0.1\" in filename:\n",
    "                model_name = \"Mixtral-8x7B-Instruct-v0.1\"\n",
    "            else:\n",
    "                model_name = \"Unknown\"\n",
    "            df[\"model_name\"] = model_name\n",
    "            if \"accuracy\" in df.columns:\n",
    "                df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "            else:\n",
    "                df[\"accuracy\"] = df.apply(\n",
    "                    lambda row: (\n",
    "                        row[f\"{row['category']}_accuracy\"]\n",
    "                        if f\"{row['category']}_accuracy\" in df.columns\n",
    "                        else None\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "            df[\"temperature\"] = pd.to_numeric(df[\"temperature\"], errors=\"coerce\")\n",
    "            df[\"temperature\"] = df[\"temperature\"].apply(\n",
    "                lambda x: round(x, 2) if pd.notna(x) else x\n",
    "            )\n",
    "            columns_to_keep = [\"model_name\", \"temperature\", \"category\", \"accuracy\"]\n",
    "            df = df[columns_to_keep]\n",
    "            dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_429274/3526203270.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/snt/miniconda3/envs/causal_env_transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df[\"temperature\"] = df[\"temperature\"].astype(float).round(2)\n",
    "\n",
    "grouped_avg_accuracy = (\n",
    "    df.groupby([\"model_name\", \"temperature\", \"category\"])[\"accuracy\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grouped_avg_accuracy[\"temperature\"] = grouped_avg_accuracy[\"temperature\"].round(2)\n",
    "\n",
    "tokenizer_path = \"/home/snt/llm_models/bert-base-multilingual-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "model = torch.load(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/training/bert_model_target_8\"\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Prompt:\n",
    "\n",
    "The year is 2075. Decades of unchecked climate change have transformed Earth into an unpredictable, chaotic environment. Cities lie submerged, deserts stretch endlessly, and hurricanes form with unnatural precision. Despite the warning signs, humanity's response came too late—or did it?\n",
    "\n",
    "Write a short story from the perspective of a climate scientist who stumbles upon an abandoned research facility containing groundbreaking technology. This discovery could reverse the effects of climate change, but using it carries immense risks. As the protagonist grapples with this ethical dilemma, explore their internal conflict, the societal implications of their choice, and how the scars of a warming planet have shaped humanity's resilience.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "best_temperature, predicted_category, prediction_dict = find_optimal_temperature(\n",
    "    prompt=prompt,\n",
    "    model_name=\"Llama-2-7b-chat-hf\",\n",
    "    grouped_avg_accuracy=grouped_avg_accuracy,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(best_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the superglue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_vllm(\n",
    "    prompt,\n",
    "    model_name=\"/home/snt/llm_models/Llama-3.2-1B-Instruct\",\n",
    "    options={},\n",
    "    server_url=\"http://0.0.0.0:8000/v1/chat/completions\",\n",
    "):\n",
    "    \"\"\"Generate text using the specified language model.\"\"\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    }\n",
    "    payload.update(options)\n",
    "    response = requests.post(server_url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "\n",
    "def evaluate_prediction_with_conversion(task, example, prediction):\n",
    "    # Helper functions for conversion\n",
    "    def bool_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"true\"\n",
    "        if value == \"true\":\n",
    "            return 1\n",
    "        elif value == \"false\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"true\" but does not contain \"false\"\n",
    "        elif \"true\" in value and \"false\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"false\" but does not contain \"true\"\n",
    "        elif \"false\" in value and \"true\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"true\" and \"false\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def yes_no_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"yes\"\n",
    "        if value == \"yes\":\n",
    "            return 1\n",
    "        elif value == \"no\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"yes\" but does not contain \"no\"\n",
    "        elif \"yes\" in value and \"no\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"no\" but does not contain \"yes\"\n",
    "        elif \"no\" in value and \"yes\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"yes\" and \"no\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def entailment_to_label(value):\n",
    "        # Define the mapping for entailment, contradiction, and neutral\n",
    "        mapping = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input matches exactly one of the keys in the mapping\n",
    "        if value in mapping:\n",
    "            return mapping[value]\n",
    "\n",
    "        # Check if the input contains one of the keys without ambiguity\n",
    "        elif (\n",
    "            \"entailment\" in value\n",
    "            and \"contradiction\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"entailment\"]\n",
    "        elif (\n",
    "            \"contradiction\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"contradiction\"]\n",
    "        elif (\n",
    "            \"neutral\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"contradiction\" not in value\n",
    "        ):\n",
    "            return mapping[\"neutral\"]\n",
    "\n",
    "        # If the input is ambiguous or invalid, return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def choice_to_binary(value):\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input contains 'choice 1' and does not contain 'choice 2'\n",
    "        if \"choice 1\" in value and \"choice 2\" not in value:\n",
    "            return 0\n",
    "        elif \"choice 2\" in value:\n",
    "            return 1\n",
    "\n",
    "        # If the input does not match any of the conditions, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Task-specific evaluation\n",
    "    if task == \"boolq\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"cb\":\n",
    "        # Convert prediction (entailment/contradiction/neutral) to label (0/1/2)\n",
    "        return entailment_to_label(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"copa\":\n",
    "        # Convert prediction (choice1/choice2) to binary and compare with label (0/1)\n",
    "        return choice_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"multirc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"record\":\n",
    "        # Direct comparison of prediction with the correct entity\n",
    "        processed_answers = [answer.strip().lower() for answer in example[\"answers\"]]\n",
    "        return prediction.strip().lower() in processed_answers\n",
    "\n",
    "    elif task == \"rte\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (1/0)\n",
    "        return yes_no_to_binary(prediction) == (1 - int(example[\"label\"]))\n",
    "\n",
    "    elif task == \"wic\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"wsc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    # Default case: unknown task\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "options = {  # Configuration options for model generation\n",
    "    \"temperature\": 1.0,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"top_p\": 0.9,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "    \"seed\": 47,  # Default\n",
    "    \"n\": 1,\n",
    "}\n",
    "\n",
    "df = pd.read_json(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/super_glue_data.jsonl\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = \"validation\"\n",
    "server_url = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "\n",
    "model_infernece_path = \"/home/snt/llm_models/Llama-3.2-1B-Instruct\"\n",
    "model_name = \"Llama-3.2-1B-Instruct\"\n",
    "output_name = f\"{model_name}_{dataset_label}\"\n",
    "output_path = f\"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/output/sp_bleu_validation_with_temperature_full.jsonl\"\n",
    "\n",
    "df = df[df[\"dataset_label\"] == dataset_label]\n",
    "df = df.groupby(\"task\").head(1000)\n",
    "\n",
    "model_path = \"/home/snt/projects_lujun/temperature_eval/bert_model_target_2\"\n",
    "tokenizer_path = \"/home/snt/llm_models/bert-base-multilingual-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "model = torch.load(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/training/bert_model_target_8\"\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "models = [\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "]\n",
    "\n",
    "with open(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/.vscode/api_key.txt\",\n",
    "    \"r\",\n",
    ") as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Prompts:   0%|          | 0/4175 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Processing Prompts: 100%|██████████| 4175/4175 [2:25:35<00:00,  2.09s/it]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "start_idx = 0\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Prompts\"):\n",
    "    for model_name in models:\n",
    "        prompt = row[\"prompt\"]\n",
    "        optimal_temperature_bert, predicted_category_bert, prediction_dict = (\n",
    "            find_optimal_temperature(\n",
    "                prompt=prompt,\n",
    "                model_name=model_name,\n",
    "                grouped_avg_accuracy=grouped_avg_accuracy,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        optimal_temperature_gpt, predicted_category_gpt = find_optimal_temperature_gpt(\n",
    "            prompt=prompt,\n",
    "            model_name=model_name,\n",
    "            grouped_avg_accuracy=grouped_avg_accuracy,\n",
    "        )\n",
    "\n",
    "        optimal_temperature_gpt = round(optimal_temperature_gpt, 2)\n",
    "        optimal_temperature_bert = round(optimal_temperature_bert, 2)\n",
    "        result_df = pd.DataFrame(\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"temperature_bert\": optimal_temperature_bert,\n",
    "                \"temperature_gpt\": optimal_temperature_gpt,\n",
    "                \"prediction_dict\": json.dumps(str(prediction_dict)),\n",
    "                \"predicted_category\": str(\n",
    "                    [predicted_category_bert, predicted_category_gpt]\n",
    "                ),\n",
    "                \"task\": row[\"task\"],\n",
    "                \"example\": json.dumps(row[\"example\"]),\n",
    "                \"prompt\": prompt,\n",
    "                # \"prediction\": str(\n",
    "                #     [prediction_bert_temperature, prediction_gpt_temperature]\n",
    "                # ),\n",
    "                # \"label\": str([label_bert, label_gpt]),\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "\n",
    "        result_df.to_json(\n",
    "            output_path,\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "            mode=\"a\",\n",
    "        )\n",
    "        start_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\n",
    "    \"/home/snt/projects_lujun/temperature_eval_github/temperature_eval/data/Bert/output/sp_bleu_validation_with_temperature.jsonl\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name                  task   \n",
       "Llama-3.2-1B-Instruct       boolq      200\n",
       "                            cb         207\n",
       "                            copa       200\n",
       "                            multirc    200\n",
       "                            record     200\n",
       "                            rte        200\n",
       "                            wic        200\n",
       "                            wsc        200\n",
       "Meta-Llama-3-8B-Instruct    boolq      200\n",
       "                            cb         208\n",
       "                            copa       200\n",
       "                            multirc    200\n",
       "                            record     200\n",
       "                            rte        200\n",
       "                            wic        200\n",
       "                            wsc        200\n",
       "Mixtral-8x7B-Instruct-v0.1  boolq      200\n",
       "                            cb         208\n",
       "                            copa       200\n",
       "                            multirc    200\n",
       "                            record     200\n",
       "                            rte        200\n",
       "                            wic        200\n",
       "                            wsc        200\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"model_name\", \"task\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temperature_gpt\n",
       "0.4    1866\n",
       "1.0    1603\n",
       "0.7     981\n",
       "0.1     373\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.temperature_gpt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
